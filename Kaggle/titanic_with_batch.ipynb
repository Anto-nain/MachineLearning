{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic : Machine Learning from Disaster\n",
    "The goal is the same, but this time, in order to get the best model we will deal with the missing *age* values by creating a batch of data sets. All these data sets will be trained, and the best model will be selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Importing Libraries\n",
    "* **Keras from Tensorflow** - for Deep Learning\n",
    "* **Pandas** - for Data Manipulation\n",
    "* **Numpy** - for Special Features\n",
    "* **Matplotlib** - for Data Visualization\n",
    "* **Random** - for Data Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librairies successfully imported\n",
      " - Tensorflow version:  2.9.0\n",
      " - Keras version:  2.9.0\n",
      " - Pandas version:  2.0.0\n",
      " - Numpy version:  1.23.5\n",
      " - Matplotlib version:  3.7.1\n",
      " - Random version:  NaN\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "print(\"Librairies successfully imported\")\n",
    "\n",
    "# Clearing the session\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Checking the versions\n",
    "print(\" - Tensorflow version: \", tf.__version__)\n",
    "print(\" - Keras version: \", keras.__version__)\n",
    "print(\" - Pandas version: \", pd.__version__)\n",
    "print(\" - Numpy version: \", np.__version__)\n",
    "print(\" - Matplotlib version: \", plt.matplotlib.__version__)\n",
    "print(\" - Random version: \", \"NaN\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = \"data_titanic/\"\n",
    "\n",
    "# Loading the data\n",
    "raw_train_data = pd.read_csv(data_folder_path + \"train.csv\")\n",
    "raw_test_data = pd.read_csv(data_folder_path + \"test.csv\")\n",
    "\n",
    "print(\"Data successfully loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   \n",
       "0            1         0       3  \\\n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp   \n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1  \\\n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the training data\n",
    "raw_train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Looking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for the training data:\n",
      " -  Age :  177  /  891\n",
      " -  Cabin :  687  /  891\n",
      " -  Embarked :  2  /  891\n",
      "Missing values for the test data:\n",
      " -  Age :  86  /  418\n",
      " -  Fare :  1  /  418\n",
      " -  Cabin :  327  /  418\n"
     ]
    }
   ],
   "source": [
    "# Pandas filter for missing values\n",
    "missing_value_filter_train_data = raw_train_data.isna().sum()[raw_train_data.isna().sum() > 0]\n",
    "missing_value_filter_test_data = raw_test_data.isna().sum()[raw_test_data.isna().sum() > 0]\n",
    "\n",
    "training_data_size = len(raw_train_data)\n",
    "test_data_size = len(raw_test_data)\n",
    "\n",
    "# Displaying the missing values for the training data\n",
    "print(\"Missing values for the training data:\")\n",
    "for column_name, missing_value_number in missing_value_filter_train_data.items():\n",
    "    print(\" - \", column_name, \": \", missing_value_number,' / ', training_data_size)\n",
    "\n",
    "# Displaying the missing values for the test data\n",
    "print(\"Missing values for the test data:\")\n",
    "for column_name, missing_value_number in missing_value_filter_test_data.items():\n",
    "    print(\" - \", column_name, \": \", missing_value_number,' / ', test_data_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Strategy to fix missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the features one by one and see what we can do about the missing values : \n",
    "* **Cabin** : In both data sets most of the values are missing, so we can drop this feature.\n",
    "* **Embarked** : There are only 2 missing values in the training set, so, in order not to polute the data, we can drop these 2 rows.\n",
    "* **Fare** : There is only 1 missing value in the test set, so we can replace it with the mean value of the feature.\n",
    "* **Age** : Is a key feature but is missing a lot in both sets. Simply replacing the missing values by the mean of it might not be the best solution as this omnipresence of the mean value may polute our model. The solution here would be to first look at the representation of ages in the data sets and give random ages to the missing values according to the distribution of the ages in the data sets : if the repartition looks like a Gauss repartition we will give random ages according to a Gauss distribution, if it looks like a uniform repartition we will give random ages according to a uniform distribution, etc.\n",
    "\n",
    "**Warning :** In order to get a \"good\" repartition, we should create a batch of data sets, test them, and finally sellect the best one only.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Fixing the data for Cabin, Embarked and Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare missing values filled by :  33.2\n"
     ]
    }
   ],
   "source": [
    "# Droping the Cabin feature\n",
    "raw_train_data.drop(columns=['Cabin'], inplace=True)\n",
    "raw_test_data.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "# Droping the raws with missing Embarked values\n",
    "raw_train_data.dropna(subset=['Embarked'], inplace=True)\n",
    "\n",
    "# Filling the missing values for the Fare feature\n",
    "mean_Fare_train = raw_train_data['Fare'].mean()\n",
    "mean_Fare_test = raw_test_data['Fare'].mean()\n",
    "filled_value = round((mean_Fare_train*(training_data_size-2) + mean_Fare_test*(test_data_size-1))/(training_data_size+test_data_size-3),1)\n",
    "raw_test_data['Fare'].fillna(filled_value, inplace=True)\n",
    "print(\"Fare missing values filled by : \", filled_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Fixing the data for Age"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the repartition of the ages in the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age for the training data:  29.6\n",
      "Standard deviation for the training data:  14.5\n",
      "Mean age for the test data:  30.3\n",
      "Standard deviation for the test data:  14.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's clean the data for representing the age\n",
    "rep_train = raw_train_data[\"Age\"].dropna()\n",
    "rep_test = raw_test_data[\"Age\"].dropna()\n",
    "\n",
    "# Getting the mean and the standard deviation\n",
    "mean_age_train = round(rep_train.mean(),1)\n",
    "std_age_train = round(rep_train.std(),1)\n",
    "mean_age_test = round(rep_test.mean(),1)\n",
    "std_age_test = round(rep_test.std(),1)\n",
    "\n",
    "print(\"Mean age for the training data: \", mean_age_train)\n",
    "print(\"Standard deviation for the training data: \", std_age_train)\n",
    "print(\"Mean age for the test data: \", mean_age_test)\n",
    "print(\"Standard deviation for the test data: \", std_age_test)\n",
    "\n",
    "# Creating histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.hist(rep_train, bins=14)\n",
    "ax1.set_title(\"Train data\")\n",
    "\n",
    "\n",
    "ax2.hist(rep_test, bins=14)\n",
    "ax2.set_title(\"Test data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's great about the graphs and data mean and standarad deviation is that they all are coherent.\n",
    "\n",
    "For the next step, let's assume those repartitions are Gauss repartitions with a mean of 30 and a standard deviation of 14.4. \n",
    "\n",
    "Now, we only have to create the correct distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the distribution\n",
    "def distrib():\n",
    "    return max(0, rd.gauss( 30, 14.4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the missing data with our distribution. And creat a list of data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>889.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.382452</td>\n",
       "      <td>2.311586</td>\n",
       "      <td>29.842494</td>\n",
       "      <td>0.524184</td>\n",
       "      <td>0.382452</td>\n",
       "      <td>32.096681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>256.998173</td>\n",
       "      <td>0.486260</td>\n",
       "      <td>0.834700</td>\n",
       "      <td>14.452380</td>\n",
       "      <td>1.103705</td>\n",
       "      <td>0.806761</td>\n",
       "      <td>49.697504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>224.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.040805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp   \n",
       "count   889.000000  889.000000  889.000000  889.000000  889.000000  \\\n",
       "mean    446.000000    0.382452    2.311586   29.842494    0.524184   \n",
       "std     256.998173    0.486260    0.834700   14.452380    1.103705   \n",
       "min       1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%     224.000000    0.000000    2.000000   20.040805    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   29.000000    0.000000   \n",
       "75%     668.000000    1.000000    3.000000   39.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  889.000000  889.000000  \n",
       "mean     0.382452   32.096681  \n",
       "std      0.806761   49.697504  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.895800  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data reference for the batch\n",
    "data0 = raw_train_data.copy()\n",
    "\n",
    "\n",
    "# Replace the missing values by the distribution\n",
    "raw_test_data['Age'].fillna(raw_test_data['Age'].apply(lambda x: distrib()), inplace=True)\n",
    "raw_train_data['Age'].fillna(raw_train_data['Age'].apply(lambda x: distrib()), inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "#List of 300 training data\n",
    "list_train_data = []\n",
    "for i in range(batch_size):\n",
    "    data = data0.copy()\n",
    "    list_train_data.append(data)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    data = list_train_data[i]\n",
    "    data['Age'].fillna(data['Age'].apply(lambda x: distrib()), inplace = True)\n",
    "\n",
    "list_train_data[3].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values for the training data:  0\n",
      "Number of missing values for the test data:  0\n",
      "\n",
      "Data successfully cleaned\n"
     ]
    }
   ],
   "source": [
    "# Re-checking the missing values\n",
    "number_of_missing_values_train = raw_train_data.isna().sum()[raw_train_data.isna().sum() > 0].sum()\n",
    "number_of_missing_values_test = raw_test_data.isna().sum()[raw_test_data.isna().sum() > 0].sum()\n",
    "print(\"Number of missing values for the training data: \", number_of_missing_values_train)\n",
    "print(\"Number of missing values for the test data: \", number_of_missing_values_test)\n",
    "\n",
    "# Renaming the data\n",
    "clean_train_data = raw_train_data\n",
    "clean_test_data = raw_test_data\n",
    "print()\n",
    "print(\"Data successfully cleaned\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Data Normalizing and usable datasets creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Data tranformation to categorical values\n",
    "A few modification on the data will be made :\n",
    "* The PassengerId column will be dropped.\n",
    "* The name column will be dropped as it is not relevant for the model.\n",
    "* The Ticket column will be dropped too as it doen't semm to give any information.\n",
    "* The Embarked feature will be tranformed reagarding the chronological steps of the Titanic Journey : Southampton -> Cherbourg -> Queenstown. Thus, we will perform the following transformation :\n",
    "    * S -> 0\n",
    "    * C -> 1\n",
    "    * Q -> 2\n",
    "* The Sex feature will be transformed as followed :\n",
    "    * male -> 0\n",
    "    * female -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the PassengerId, Name and Ticket features\n",
    "clean_test_data.drop(columns=['Name', 'Ticket'], inplace=True)\n",
    "clean_train_data.drop(columns=['PassengerId', 'Name', 'Ticket'], inplace=True)\n",
    "\n",
    "# Replacing the Embarked feature by a numerical feature\n",
    "clean_train_data['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\n",
    "clean_test_data['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\n",
    "\n",
    "# Replacing the gender feature by a numerical feature\n",
    "clean_train_data['Sex'].replace(['male','female'], [0, 1], inplace=True)\n",
    "clean_test_data['Sex'].replace(['male','female'], [0, 1], inplace=True)\n",
    "\n",
    "# Doing so for the list of 300 training data\n",
    "for i in range(batch_size):\n",
    "    list_train_data[i].drop(columns=['PassengerId', 'Name', 'Ticket'], inplace=True)\n",
    "    list_train_data[i]['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\n",
    "    list_train_data[i]['Sex'].replace(['male','female'], [0, 1], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Data Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the train data set\n",
    "for label in clean_train_data.columns:\n",
    "    if label != 'Survived':\n",
    "        mean = clean_train_data[label].mean()\n",
    "        std = clean_train_data[label].std()\n",
    "        clean_train_data[label] = clean_train_data[label].apply(lambda x: (x-mean)/std)\n",
    "\n",
    "# Normalizing the test data set\n",
    "for label in clean_test_data.columns:\n",
    "    if label != 'PassengerId':\n",
    "        mean = clean_test_data[label].mean()\n",
    "        std = clean_test_data[label].std()\n",
    "        clean_test_data[label] = clean_test_data[label].apply(lambda x: (x-mean)/std)\n",
    "        \n",
    "# Normalizing the list of 300 training data\n",
    "for i in range(batch_size):\n",
    "    for label in list_train_data[i].columns:\n",
    "        if label != 'Survived':\n",
    "            mean = list_train_data[i][label].mean()\n",
    "            std = list_train_data[i][label].std()\n",
    "            list_train_data[i][label] = list_train_data[i][label].apply(lambda x: (x-mean)/std)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Data Shuffling and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in ready to be used for the training of the neural network\n",
      "X_train shape:  (711, 7)\n",
      "Y_train shape:  (711,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the data\n",
    "data = clean_train_data.sample(frac=1)\n",
    "\n",
    "# Spliting the data\n",
    "validation_data = data.sample(frac=0.2)\n",
    "train_data = data.drop(validation_data.index)\n",
    "\n",
    "# Creation of the entries and exits of our neural network\n",
    "X_train = train_data.drop(columns=['Survived'])\n",
    "Y_train = train_data['Survived']\n",
    "\n",
    "X_validation = validation_data.drop(columns=['Survived'])\n",
    "Y_validation = validation_data['Survived']\n",
    "\n",
    "X_test = clean_test_data\n",
    "\n",
    "print(\"Data in ready to be used for the training of the neural network\")\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "\n",
    "# Doing so for the list of 300 training data\n",
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "X_validation_list = []\n",
    "Y_validation_list = []\n",
    "for i in range(batch_size):\n",
    "    list_train_data[i] = list_train_data[i].sample(frac=1)\n",
    "    validation_data = list_train_data[i].sample(frac=0.2)\n",
    "    train_data = list_train_data[i].drop(validation_data.index)\n",
    "    X_train_list.append(train_data.drop(columns=['Survived']))\n",
    "    Y_train_list.append(train_data['Survived'])\n",
    "    X_validation_list.append(validation_data.drop(columns=['Survived']))\n",
    "    Y_validation_list.append(validation_data['Survived'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                256       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "shape = X_train.shape[1] # = 7\n",
    "\n",
    "# Creation of the model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss      = 'binary_crossentropy',\n",
    "              metrics   = ['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : to select the bet model, we will choose the one that has the best accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.6199 - accuracy: 0.6444 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.82022, saving model to prediction_titanic\\best_model_v2.h5\n",
      "72/72 [==============================] - 1s 4ms/step - loss: 0.6065 - accuracy: 0.6709 - val_loss: 0.5222 - val_accuracy: 0.8202\n",
      "Epoch 2/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.4984 - accuracy: 0.7933\n",
      "Epoch 2: val_accuracy improved from 0.82022 to 0.83146, saving model to prediction_titanic\\best_model_v2.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7904 - val_loss: 0.4624 - val_accuracy: 0.8315\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.4722 - accuracy: 0.8000\n",
      "Epoch 3: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.8129 - val_loss: 0.4317 - val_accuracy: 0.8202\n",
      "Epoch 4/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.4484 - accuracy: 0.8149\n",
      "Epoch 4: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.8143 - val_loss: 0.4178 - val_accuracy: 0.8258\n",
      "Epoch 5/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.4342 - accuracy: 0.8167\n",
      "Epoch 5: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8143 - val_loss: 0.4081 - val_accuracy: 0.8258\n",
      "Epoch 6/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.4198 - accuracy: 0.8250\n",
      "Epoch 6: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8158 - val_loss: 0.4054 - val_accuracy: 0.8202\n",
      "Epoch 7/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.4284 - accuracy: 0.8215\n",
      "Epoch 7: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8242 - val_loss: 0.4009 - val_accuracy: 0.8202\n",
      "Epoch 8/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.3823 - accuracy: 0.8528\n",
      "Epoch 8: val_accuracy did not improve from 0.83146\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8242 - val_loss: 0.3972 - val_accuracy: 0.8315\n",
      "Epoch 9/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.4157 - accuracy: 0.8357\n",
      "Epoch 9: val_accuracy improved from 0.83146 to 0.83708, saving model to prediction_titanic\\best_model_v2.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8256 - val_loss: 0.3944 - val_accuracy: 0.8371\n",
      "Epoch 10/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.4294 - accuracy: 0.8222\n",
      "Epoch 10: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8270 - val_loss: 0.3931 - val_accuracy: 0.8371\n",
      "Epoch 11/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.4224 - accuracy: 0.8244\n",
      "Epoch 11: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8326 - val_loss: 0.3928 - val_accuracy: 0.8371\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.8298\n",
      "Epoch 12: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8298 - val_loss: 0.3930 - val_accuracy: 0.8258\n",
      "Epoch 13/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3919 - accuracy: 0.8350\n",
      "Epoch 13: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8312 - val_loss: 0.3904 - val_accuracy: 0.8315\n",
      "Epoch 14/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.4177 - accuracy: 0.8366\n",
      "Epoch 14: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8312 - val_loss: 0.3901 - val_accuracy: 0.8315\n",
      "Epoch 15/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.4171 - accuracy: 0.8262\n",
      "Epoch 15: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8312 - val_loss: 0.3868 - val_accuracy: 0.8371\n",
      "Epoch 16/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.4171 - accuracy: 0.8289\n",
      "Epoch 16: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.8340 - val_loss: 0.3891 - val_accuracy: 0.8315\n",
      "Epoch 17/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3988 - accuracy: 0.8273\n",
      "Epoch 17: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8284 - val_loss: 0.3876 - val_accuracy: 0.8371\n",
      "Epoch 18/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.3954 - accuracy: 0.8400\n",
      "Epoch 18: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8354 - val_loss: 0.3861 - val_accuracy: 0.8258\n",
      "Epoch 19/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.3591 - accuracy: 0.8444\n",
      "Epoch 19: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8298 - val_loss: 0.3897 - val_accuracy: 0.8258\n",
      "Epoch 20/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3925 - accuracy: 0.8300\n",
      "Epoch 20: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8312 - val_loss: 0.3928 - val_accuracy: 0.8371\n",
      "Epoch 21/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.4105 - accuracy: 0.8302\n",
      "Epoch 21: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8411 - val_loss: 0.3916 - val_accuracy: 0.8371\n",
      "Epoch 22/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.4065 - accuracy: 0.8356\n",
      "Epoch 22: val_accuracy improved from 0.83708 to 0.84270, saving model to prediction_titanic\\best_model_v2.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8425 - val_loss: 0.3941 - val_accuracy: 0.8427\n",
      "Epoch 23/100\n",
      "46/72 [==================>...........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8457\n",
      "Epoch 23: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8425 - val_loss: 0.3914 - val_accuracy: 0.8371\n",
      "Epoch 24/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.4110 - accuracy: 0.8310\n",
      "Epoch 24: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8481 - val_loss: 0.3912 - val_accuracy: 0.8315\n",
      "Epoch 25/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3813 - accuracy: 0.8439\n",
      "Epoch 25: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8411 - val_loss: 0.3917 - val_accuracy: 0.8427\n",
      "Epoch 26/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3535 - accuracy: 0.8659\n",
      "Epoch 26: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8439 - val_loss: 0.3920 - val_accuracy: 0.8427\n",
      "Epoch 27/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3964 - accuracy: 0.8452\n",
      "Epoch 27: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8481 - val_loss: 0.3933 - val_accuracy: 0.8427\n",
      "Epoch 28/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3978 - accuracy: 0.8300\n",
      "Epoch 28: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8453 - val_loss: 0.3933 - val_accuracy: 0.8427\n",
      "Epoch 29/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3775 - accuracy: 0.8568\n",
      "Epoch 29: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8481 - val_loss: 0.3943 - val_accuracy: 0.8427\n",
      "Epoch 30/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3843 - accuracy: 0.8400\n",
      "Epoch 30: val_accuracy improved from 0.84270 to 0.84831, saving model to prediction_titanic\\best_model_v2.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8425 - val_loss: 0.3928 - val_accuracy: 0.8483\n",
      "Epoch 31/100\n",
      "54/72 [=====================>........] - ETA: 0s - loss: 0.3848 - accuracy: 0.8426\n",
      "Epoch 31: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8453 - val_loss: 0.3941 - val_accuracy: 0.8427\n",
      "Epoch 32/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3663 - accuracy: 0.8605\n",
      "Epoch 32: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8453 - val_loss: 0.3937 - val_accuracy: 0.8483\n",
      "Epoch 33/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3533 - accuracy: 0.8400\n",
      "Epoch 33: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8383 - val_loss: 0.3976 - val_accuracy: 0.8483\n",
      "Epoch 34/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3948 - accuracy: 0.8366\n",
      "Epoch 34: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8439 - val_loss: 0.3953 - val_accuracy: 0.8483\n",
      "Epoch 35/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3750 - accuracy: 0.8457\n",
      "Epoch 35: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8453 - val_loss: 0.3959 - val_accuracy: 0.8427\n",
      "Epoch 36/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3654 - accuracy: 0.8488\n",
      "Epoch 36: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8439 - val_loss: 0.3970 - val_accuracy: 0.8483\n",
      "Epoch 37/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.3702 - accuracy: 0.8540\n",
      "Epoch 37: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8523 - val_loss: 0.4000 - val_accuracy: 0.8371\n",
      "Epoch 38/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3740 - accuracy: 0.8419\n",
      "Epoch 38: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8453 - val_loss: 0.3985 - val_accuracy: 0.8483\n",
      "Epoch 39/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3627 - accuracy: 0.8545\n",
      "Epoch 39: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8467 - val_loss: 0.4003 - val_accuracy: 0.8371\n",
      "Epoch 40/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3718 - accuracy: 0.8350\n",
      "Epoch 40: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8368 - val_loss: 0.4054 - val_accuracy: 0.8483\n",
      "Epoch 41/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.3548 - accuracy: 0.8533\n",
      "Epoch 41: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8495 - val_loss: 0.3990 - val_accuracy: 0.8371\n",
      "Epoch 42/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3638 - accuracy: 0.8625\n",
      "Epoch 42: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8537 - val_loss: 0.4001 - val_accuracy: 0.8315\n",
      "Epoch 43/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3462 - accuracy: 0.8538\n",
      "Epoch 43: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.8467 - val_loss: 0.4072 - val_accuracy: 0.8427\n",
      "Epoch 44/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3546 - accuracy: 0.8585\n",
      "Epoch 44: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8509 - val_loss: 0.4045 - val_accuracy: 0.8483\n",
      "Epoch 45/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3696 - accuracy: 0.8524\n",
      "Epoch 45: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8579 - val_loss: 0.4104 - val_accuracy: 0.8371\n",
      "Epoch 46/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3388 - accuracy: 0.8756\n",
      "Epoch 46: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8579 - val_loss: 0.4063 - val_accuracy: 0.8315\n",
      "Epoch 47/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3805 - accuracy: 0.8476\n",
      "Epoch 47: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8636 - val_loss: 0.4108 - val_accuracy: 0.8371\n",
      "Epoch 48/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3496 - accuracy: 0.8550\n",
      "Epoch 48: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8509 - val_loss: 0.4094 - val_accuracy: 0.8371\n",
      "Epoch 49/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3459 - accuracy: 0.8561\n",
      "Epoch 49: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8636 - val_loss: 0.4103 - val_accuracy: 0.8371\n",
      "Epoch 50/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3526 - accuracy: 0.8667\n",
      "Epoch 50: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8608 - val_loss: 0.4118 - val_accuracy: 0.8315\n",
      "Epoch 51/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3865 - accuracy: 0.8381\n",
      "Epoch 51: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3515 - accuracy: 0.8565 - val_loss: 0.4209 - val_accuracy: 0.8371\n",
      "Epoch 52/100\n",
      "46/72 [==================>...........] - ETA: 0s - loss: 0.3505 - accuracy: 0.8609\n",
      "Epoch 52: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8594 - val_loss: 0.4111 - val_accuracy: 0.8315\n",
      "Epoch 53/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3474 - accuracy: 0.8651\n",
      "Epoch 53: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8636 - val_loss: 0.4105 - val_accuracy: 0.8371\n",
      "Epoch 54/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3591 - accuracy: 0.8667\n",
      "Epoch 54: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8678 - val_loss: 0.4088 - val_accuracy: 0.8371\n",
      "Epoch 55/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3824 - accuracy: 0.8439\n",
      "Epoch 55: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8608 - val_loss: 0.4174 - val_accuracy: 0.8427\n",
      "Epoch 56/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.3487 - accuracy: 0.8644\n",
      "Epoch 56: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8650 - val_loss: 0.4166 - val_accuracy: 0.8371\n",
      "Epoch 57/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3147 - accuracy: 0.8786\n",
      "Epoch 57: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8678 - val_loss: 0.4142 - val_accuracy: 0.8371\n",
      "Epoch 58/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2930 - accuracy: 0.8927\n",
      "Epoch 58: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8636 - val_loss: 0.4188 - val_accuracy: 0.8315\n",
      "Epoch 59/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3275 - accuracy: 0.8650\n",
      "Epoch 59: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8692 - val_loss: 0.4212 - val_accuracy: 0.8315\n",
      "Epoch 60/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3364 - accuracy: 0.8595\n",
      "Epoch 60: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8650 - val_loss: 0.4266 - val_accuracy: 0.8371\n",
      "Epoch 61/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.3455 - accuracy: 0.8635\n",
      "Epoch 61: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8636 - val_loss: 0.4285 - val_accuracy: 0.8258\n",
      "Epoch 62/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3087 - accuracy: 0.8725\n",
      "Epoch 62: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8636 - val_loss: 0.4215 - val_accuracy: 0.8202\n",
      "Epoch 63/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3411 - accuracy: 0.8690\n",
      "Epoch 63: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8650 - val_loss: 0.4198 - val_accuracy: 0.8315\n",
      "Epoch 64/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3457 - accuracy: 0.8700\n",
      "Epoch 64: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8678 - val_loss: 0.4251 - val_accuracy: 0.8258\n",
      "Epoch 65/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3315 - accuracy: 0.8605\n",
      "Epoch 65: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3417 - accuracy: 0.8622 - val_loss: 0.4185 - val_accuracy: 0.8146\n",
      "Epoch 66/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3533 - accuracy: 0.8590\n",
      "Epoch 66: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8650 - val_loss: 0.4301 - val_accuracy: 0.8371\n",
      "Epoch 67/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3583 - accuracy: 0.8561\n",
      "Epoch 67: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8622 - val_loss: 0.4250 - val_accuracy: 0.8315\n",
      "Epoch 68/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3445 - accuracy: 0.8641\n",
      "Epoch 68: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8678 - val_loss: 0.4380 - val_accuracy: 0.8371\n",
      "Epoch 69/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3381 - accuracy: 0.8659\n",
      "Epoch 69: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8692 - val_loss: 0.4216 - val_accuracy: 0.8258\n",
      "Epoch 70/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3540 - accuracy: 0.8476\n",
      "Epoch 70: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8622 - val_loss: 0.4348 - val_accuracy: 0.8258\n",
      "Epoch 71/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3375 - accuracy: 0.8600\n",
      "Epoch 71: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8678 - val_loss: 0.4284 - val_accuracy: 0.8371\n",
      "Epoch 72/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3452 - accuracy: 0.8610\n",
      "Epoch 72: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8692 - val_loss: 0.4362 - val_accuracy: 0.8315\n",
      "Epoch 73/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3172 - accuracy: 0.8721\n",
      "Epoch 73: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.8664 - val_loss: 0.4222 - val_accuracy: 0.8146\n",
      "Epoch 74/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2930 - accuracy: 0.8825\n",
      "Epoch 74: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8678 - val_loss: 0.4355 - val_accuracy: 0.8202\n",
      "Epoch 75/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3443 - accuracy: 0.8476\n",
      "Epoch 75: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8650 - val_loss: 0.4265 - val_accuracy: 0.8371\n",
      "Epoch 76/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.3412 - accuracy: 0.8649\n",
      "Epoch 76: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8664 - val_loss: 0.4326 - val_accuracy: 0.8371\n",
      "Epoch 77/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3303 - accuracy: 0.8561\n",
      "Epoch 77: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8678 - val_loss: 0.4392 - val_accuracy: 0.8315\n",
      "Epoch 78/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3164 - accuracy: 0.8714\n",
      "Epoch 78: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8608 - val_loss: 0.4292 - val_accuracy: 0.8258\n",
      "Epoch 79/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3455 - accuracy: 0.8707\n",
      "Epoch 79: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8706 - val_loss: 0.4374 - val_accuracy: 0.8202\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8636\n",
      "Epoch 80: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8636 - val_loss: 0.4308 - val_accuracy: 0.8146\n",
      "Epoch 81/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3342 - accuracy: 0.8667\n",
      "Epoch 81: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8720 - val_loss: 0.4385 - val_accuracy: 0.8258\n",
      "Epoch 82/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.3310 - accuracy: 0.8646\n",
      "Epoch 82: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8650 - val_loss: 0.4300 - val_accuracy: 0.8371\n",
      "Epoch 83/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8671\n",
      "Epoch 83: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8650 - val_loss: 0.4358 - val_accuracy: 0.8315\n",
      "Epoch 84/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3220 - accuracy: 0.8727\n",
      "Epoch 84: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8692 - val_loss: 0.4443 - val_accuracy: 0.8202\n",
      "Epoch 85/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3328 - accuracy: 0.8610\n",
      "Epoch 85: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8692 - val_loss: 0.4462 - val_accuracy: 0.8315\n",
      "Epoch 86/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3160 - accuracy: 0.8810\n",
      "Epoch 86: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8692 - val_loss: 0.4427 - val_accuracy: 0.8315\n",
      "Epoch 87/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3251 - accuracy: 0.8675\n",
      "Epoch 87: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8636 - val_loss: 0.4422 - val_accuracy: 0.8315\n",
      "Epoch 88/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2811 - accuracy: 0.8868\n",
      "Epoch 88: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8650 - val_loss: 0.4393 - val_accuracy: 0.8258\n",
      "Epoch 89/100\n",
      "35/72 [=============>................] - ETA: 0s - loss: 0.2860 - accuracy: 0.8829\n",
      "Epoch 89: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8734 - val_loss: 0.4499 - val_accuracy: 0.8315\n",
      "Epoch 90/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3520 - accuracy: 0.8548\n",
      "Epoch 90: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8734 - val_loss: 0.4405 - val_accuracy: 0.8258\n",
      "Epoch 91/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3351 - accuracy: 0.8605\n",
      "Epoch 91: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8706 - val_loss: 0.4421 - val_accuracy: 0.8427\n",
      "Epoch 92/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3184 - accuracy: 0.8744\n",
      "Epoch 92: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8734 - val_loss: 0.4435 - val_accuracy: 0.8315\n",
      "Epoch 93/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3298 - accuracy: 0.8636\n",
      "Epoch 93: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8720 - val_loss: 0.4434 - val_accuracy: 0.8315\n",
      "Epoch 94/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3617 - accuracy: 0.8512\n",
      "Epoch 94: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8706 - val_loss: 0.4546 - val_accuracy: 0.8202\n",
      "Epoch 95/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3123 - accuracy: 0.8659\n",
      "Epoch 95: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8664 - val_loss: 0.4638 - val_accuracy: 0.8202\n",
      "Epoch 96/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3188 - accuracy: 0.8727\n",
      "Epoch 96: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3240 - accuracy: 0.8664 - val_loss: 0.4428 - val_accuracy: 0.8315\n",
      "Epoch 97/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3140 - accuracy: 0.8650\n",
      "Epoch 97: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8678 - val_loss: 0.4484 - val_accuracy: 0.8315\n",
      "Epoch 98/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2929 - accuracy: 0.8953\n",
      "Epoch 98: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8678 - val_loss: 0.4503 - val_accuracy: 0.8146\n",
      "Epoch 99/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3226 - accuracy: 0.8690\n",
      "Epoch 99: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8678 - val_loss: 0.4463 - val_accuracy: 0.8146\n",
      "Epoch 100/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3166 - accuracy: 0.8786\n",
      "Epoch 100: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8762 - val_loss: 0.4576 - val_accuracy: 0.8315\n",
      "Data set number:  0\n",
      "Epoch 1/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3690 - accuracy: 0.8425\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87640, saving model to prediction_titanic\\best_model_batch_0.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3632 - accuracy: 0.8467 - val_loss: 0.3633 - val_accuracy: 0.8764\n",
      "Epoch 2/100\n",
      "34/72 [=============>................] - ETA: 0s - loss: 0.3546 - accuracy: 0.8441\n",
      "Epoch 2: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8481 - val_loss: 0.3618 - val_accuracy: 0.8596\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3125 - accuracy: 0.8762\n",
      "Epoch 3: val_accuracy improved from 0.87640 to 0.88764, saving model to prediction_titanic\\best_model_batch_0.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8495 - val_loss: 0.3610 - val_accuracy: 0.8876\n",
      "Epoch 4/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3662 - accuracy: 0.8463\n",
      "Epoch 4: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8537 - val_loss: 0.3638 - val_accuracy: 0.8652\n",
      "Epoch 5/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3557 - accuracy: 0.8571\n",
      "Epoch 5: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8579 - val_loss: 0.3593 - val_accuracy: 0.8708\n",
      "Epoch 6/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.3417 - accuracy: 0.8562\n",
      "Epoch 6: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8594 - val_loss: 0.3570 - val_accuracy: 0.8708\n",
      "Epoch 7/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3273 - accuracy: 0.8674\n",
      "Epoch 7: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8579 - val_loss: 0.3571 - val_accuracy: 0.8820\n",
      "Epoch 8/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3449 - accuracy: 0.8610\n",
      "Epoch 8: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8551 - val_loss: 0.3648 - val_accuracy: 0.8596\n",
      "Epoch 9/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3299 - accuracy: 0.8553\n",
      "Epoch 9: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8523 - val_loss: 0.3671 - val_accuracy: 0.8596\n",
      "Epoch 10/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3095 - accuracy: 0.8714\n",
      "Epoch 10: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8537 - val_loss: 0.3647 - val_accuracy: 0.8596\n",
      "Epoch 11/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3312 - accuracy: 0.8475\n",
      "Epoch 11: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8537 - val_loss: 0.3666 - val_accuracy: 0.8539\n",
      "Epoch 12/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3458 - accuracy: 0.8476\n",
      "Epoch 12: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8579 - val_loss: 0.3620 - val_accuracy: 0.8539\n",
      "Epoch 13/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3212 - accuracy: 0.8682\n",
      "Epoch 13: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8622 - val_loss: 0.3639 - val_accuracy: 0.8596\n",
      "Epoch 14/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3124 - accuracy: 0.8737\n",
      "Epoch 14: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8636 - val_loss: 0.3643 - val_accuracy: 0.8596\n",
      "Epoch 15/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8543\n",
      "Epoch 15: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8551 - val_loss: 0.3689 - val_accuracy: 0.8596\n",
      "Epoch 16/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3064 - accuracy: 0.8628\n",
      "Epoch 16: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8565 - val_loss: 0.3650 - val_accuracy: 0.8596\n",
      "Epoch 17/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3391 - accuracy: 0.8585\n",
      "Epoch 17: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8594 - val_loss: 0.3722 - val_accuracy: 0.8708\n",
      "Epoch 18/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3489 - accuracy: 0.8568\n",
      "Epoch 18: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8537 - val_loss: 0.3744 - val_accuracy: 0.8539\n",
      "Epoch 19/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8549\n",
      "Epoch 19: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8537 - val_loss: 0.3748 - val_accuracy: 0.8483\n",
      "Epoch 20/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3045 - accuracy: 0.8711\n",
      "Epoch 20: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8551 - val_loss: 0.3726 - val_accuracy: 0.8539\n",
      "Epoch 21/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3128 - accuracy: 0.8684\n",
      "Epoch 21: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8650 - val_loss: 0.3736 - val_accuracy: 0.8539\n",
      "Epoch 22/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3193 - accuracy: 0.8632\n",
      "Epoch 22: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3194 - accuracy: 0.8678 - val_loss: 0.3723 - val_accuracy: 0.8652\n",
      "Epoch 23/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3197 - accuracy: 0.8737\n",
      "Epoch 23: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8678 - val_loss: 0.3729 - val_accuracy: 0.8483\n",
      "Epoch 24/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3446 - accuracy: 0.8487\n",
      "Epoch 24: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8650 - val_loss: 0.3694 - val_accuracy: 0.8483\n",
      "Epoch 25/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8614\n",
      "Epoch 25: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8636 - val_loss: 0.3802 - val_accuracy: 0.8539\n",
      "Epoch 26/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8657\n",
      "Epoch 26: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8664 - val_loss: 0.3847 - val_accuracy: 0.8596\n",
      "Epoch 27/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3115 - accuracy: 0.8667\n",
      "Epoch 27: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8664 - val_loss: 0.3732 - val_accuracy: 0.8483\n",
      "Epoch 28/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3094 - accuracy: 0.8732\n",
      "Epoch 28: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8636 - val_loss: 0.3754 - val_accuracy: 0.8539\n",
      "Epoch 29/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3144 - accuracy: 0.8735\n",
      "Epoch 29: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8692 - val_loss: 0.3787 - val_accuracy: 0.8539\n",
      "Epoch 30/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3164 - accuracy: 0.8698\n",
      "Epoch 30: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8678 - val_loss: 0.3801 - val_accuracy: 0.8483\n",
      "Epoch 31/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3350 - accuracy: 0.8488\n",
      "Epoch 31: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8678 - val_loss: 0.3795 - val_accuracy: 0.8483\n",
      "Epoch 32/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3104 - accuracy: 0.8667\n",
      "Epoch 32: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8678 - val_loss: 0.3801 - val_accuracy: 0.8483\n",
      "Epoch 33/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3086 - accuracy: 0.8810\n",
      "Epoch 33: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8706 - val_loss: 0.3838 - val_accuracy: 0.8539\n",
      "Epoch 34/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3210 - accuracy: 0.8700\n",
      "Epoch 34: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8636 - val_loss: 0.3798 - val_accuracy: 0.8483\n",
      "Epoch 35/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3066 - accuracy: 0.8700\n",
      "Epoch 35: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8692 - val_loss: 0.3792 - val_accuracy: 0.8483\n",
      "Epoch 36/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3019 - accuracy: 0.8756\n",
      "Epoch 36: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8734 - val_loss: 0.3835 - val_accuracy: 0.8539\n",
      "Epoch 37/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3145 - accuracy: 0.8750\n",
      "Epoch 37: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8678 - val_loss: 0.3814 - val_accuracy: 0.8371\n",
      "Epoch 38/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3389 - accuracy: 0.8595\n",
      "Epoch 38: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8734 - val_loss: 0.3925 - val_accuracy: 0.8427\n",
      "Epoch 39/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3300 - accuracy: 0.8512\n",
      "Epoch 39: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8664 - val_loss: 0.3824 - val_accuracy: 0.8483\n",
      "Epoch 40/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2998 - accuracy: 0.8714\n",
      "Epoch 40: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8678 - val_loss: 0.3834 - val_accuracy: 0.8371\n",
      "Epoch 41/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2968 - accuracy: 0.8732\n",
      "Epoch 41: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8706 - val_loss: 0.3843 - val_accuracy: 0.8483\n",
      "Epoch 42/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3044 - accuracy: 0.8725\n",
      "Epoch 42: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8720 - val_loss: 0.3861 - val_accuracy: 0.8483\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3260 - accuracy: 0.8667\n",
      "Epoch 43: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8748 - val_loss: 0.3888 - val_accuracy: 0.8539\n",
      "Epoch 44/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3182 - accuracy: 0.8628\n",
      "Epoch 44: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8692 - val_loss: 0.3849 - val_accuracy: 0.8371\n",
      "Epoch 45/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3138 - accuracy: 0.8675\n",
      "Epoch 45: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8636 - val_loss: 0.4033 - val_accuracy: 0.8483\n",
      "Epoch 46/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3148 - accuracy: 0.8643\n",
      "Epoch 46: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8636 - val_loss: 0.3856 - val_accuracy: 0.8483\n",
      "Epoch 47/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3276 - accuracy: 0.8659\n",
      "Epoch 47: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8692 - val_loss: 0.3985 - val_accuracy: 0.8427\n",
      "Epoch 48/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2561 - accuracy: 0.8953\n",
      "Epoch 48: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8636 - val_loss: 0.3836 - val_accuracy: 0.8539\n",
      "Epoch 49/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3243 - accuracy: 0.8762\n",
      "Epoch 49: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8776 - val_loss: 0.3957 - val_accuracy: 0.8371\n",
      "Epoch 50/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3176 - accuracy: 0.8769\n",
      "Epoch 50: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8720 - val_loss: 0.3913 - val_accuracy: 0.8371\n",
      "Epoch 51/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2906 - accuracy: 0.8800\n",
      "Epoch 51: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8636 - val_loss: 0.3939 - val_accuracy: 0.8427\n",
      "Epoch 52/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3201 - accuracy: 0.8698\n",
      "Epoch 52: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8762 - val_loss: 0.3989 - val_accuracy: 0.8371\n",
      "Epoch 53/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2971 - accuracy: 0.8786\n",
      "Epoch 53: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8734 - val_loss: 0.3921 - val_accuracy: 0.8371\n",
      "Epoch 54/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3213 - accuracy: 0.8744\n",
      "Epoch 54: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8734 - val_loss: 0.3900 - val_accuracy: 0.8427\n",
      "Epoch 55/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3043 - accuracy: 0.8902\n",
      "Epoch 55: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8776 - val_loss: 0.3966 - val_accuracy: 0.8483\n",
      "Epoch 56/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2927 - accuracy: 0.8776\n",
      "Epoch 56: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.8692 - val_loss: 0.3955 - val_accuracy: 0.8258\n",
      "Epoch 57/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3089 - accuracy: 0.8791\n",
      "Epoch 57: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8833 - val_loss: 0.3930 - val_accuracy: 0.8483\n",
      "Epoch 58/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2986 - accuracy: 0.8725\n",
      "Epoch 58: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8706 - val_loss: 0.3917 - val_accuracy: 0.8427\n",
      "Epoch 59/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.8841\n",
      "Epoch 59: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8833 - val_loss: 0.4002 - val_accuracy: 0.8427\n",
      "Epoch 60/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2950 - accuracy: 0.8878\n",
      "Epoch 60: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8734 - val_loss: 0.3904 - val_accuracy: 0.8371\n",
      "Epoch 61/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2997 - accuracy: 0.8690\n",
      "Epoch 61: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8762 - val_loss: 0.3995 - val_accuracy: 0.8371\n",
      "Epoch 62/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2948 - accuracy: 0.8816\n",
      "Epoch 62: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8762 - val_loss: 0.4154 - val_accuracy: 0.8427\n",
      "Epoch 63/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2714 - accuracy: 0.8930\n",
      "Epoch 63: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8734 - val_loss: 0.3898 - val_accuracy: 0.8483\n",
      "Epoch 64/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2974 - accuracy: 0.8765\n",
      "Epoch 64: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3004 - accuracy: 0.8734 - val_loss: 0.4074 - val_accuracy: 0.8202\n",
      "Epoch 65/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3145 - accuracy: 0.8595\n",
      "Epoch 65: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8720 - val_loss: 0.4000 - val_accuracy: 0.8371\n",
      "Epoch 66/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2704 - accuracy: 0.9000\n",
      "Epoch 66: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8762 - val_loss: 0.3959 - val_accuracy: 0.8315\n",
      "Epoch 67/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2971 - accuracy: 0.8877\n",
      "Epoch 67: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8805 - val_loss: 0.3964 - val_accuracy: 0.8539\n",
      "Epoch 68/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3062 - accuracy: 0.8667\n",
      "Epoch 68: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8762 - val_loss: 0.4007 - val_accuracy: 0.8483\n",
      "Epoch 69/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3000 - accuracy: 0.8857\n",
      "Epoch 69: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8790 - val_loss: 0.3957 - val_accuracy: 0.8596\n",
      "Epoch 70/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2866 - accuracy: 0.8725\n",
      "Epoch 70: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8748 - val_loss: 0.3933 - val_accuracy: 0.8427\n",
      "Epoch 71/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3289 - accuracy: 0.8439\n",
      "Epoch 71: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8706 - val_loss: 0.4017 - val_accuracy: 0.8483\n",
      "Epoch 72/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3072 - accuracy: 0.8780\n",
      "Epoch 72: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8776 - val_loss: 0.3988 - val_accuracy: 0.8483\n",
      "Epoch 73/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2905 - accuracy: 0.8762\n",
      "Epoch 73: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2938 - accuracy: 0.8790 - val_loss: 0.4006 - val_accuracy: 0.8483\n",
      "Epoch 74/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3010 - accuracy: 0.8780\n",
      "Epoch 74: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8819 - val_loss: 0.3981 - val_accuracy: 0.8539\n",
      "Epoch 75/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3099 - accuracy: 0.8667\n",
      "Epoch 75: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8805 - val_loss: 0.4015 - val_accuracy: 0.8483\n",
      "Epoch 76/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3049 - accuracy: 0.8786\n",
      "Epoch 76: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8847 - val_loss: 0.3995 - val_accuracy: 0.8483\n",
      "Epoch 77/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2988 - accuracy: 0.8864\n",
      "Epoch 77: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.8833 - val_loss: 0.4065 - val_accuracy: 0.8315\n",
      "Epoch 78/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2889 - accuracy: 0.8750\n",
      "Epoch 78: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.8734 - val_loss: 0.3910 - val_accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2611 - accuracy: 0.8878\n",
      "Epoch 79: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2937 - accuracy: 0.8762 - val_loss: 0.3995 - val_accuracy: 0.8427\n",
      "Epoch 80/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2862 - accuracy: 0.8902\n",
      "Epoch 80: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8776 - val_loss: 0.4063 - val_accuracy: 0.8539\n",
      "Epoch 81/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3255 - accuracy: 0.8786\n",
      "Epoch 81: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8847 - val_loss: 0.4066 - val_accuracy: 0.8539\n",
      "Epoch 82/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2826 - accuracy: 0.8860\n",
      "Epoch 82: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2930 - accuracy: 0.8748 - val_loss: 0.3963 - val_accuracy: 0.8483\n",
      "Epoch 83/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2765 - accuracy: 0.8907\n",
      "Epoch 83: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8650 - val_loss: 0.4022 - val_accuracy: 0.8427\n",
      "Epoch 84/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3043 - accuracy: 0.8636\n",
      "Epoch 84: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2904 - accuracy: 0.8819 - val_loss: 0.4098 - val_accuracy: 0.8427\n",
      "Epoch 85/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2792 - accuracy: 0.8839\n",
      "Epoch 85: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8734 - val_loss: 0.4070 - val_accuracy: 0.8371\n",
      "Epoch 86/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2425 - accuracy: 0.9119\n",
      "Epoch 86: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8805 - val_loss: 0.4046 - val_accuracy: 0.8483\n",
      "Epoch 87/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2841 - accuracy: 0.8837\n",
      "Epoch 87: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8748 - val_loss: 0.4084 - val_accuracy: 0.8202\n",
      "Epoch 88/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.3142 - accuracy: 0.8474\n",
      "Epoch 88: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8678 - val_loss: 0.4194 - val_accuracy: 0.8596\n",
      "Epoch 89/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3264 - accuracy: 0.8767\n",
      "Epoch 89: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8847 - val_loss: 0.4095 - val_accuracy: 0.8652\n",
      "Epoch 90/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2772 - accuracy: 0.8930\n",
      "Epoch 90: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8819 - val_loss: 0.4118 - val_accuracy: 0.8315\n",
      "Epoch 91/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2938 - accuracy: 0.8837\n",
      "Epoch 91: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8805 - val_loss: 0.4098 - val_accuracy: 0.8483\n",
      "Epoch 92/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2710 - accuracy: 0.8833\n",
      "Epoch 92: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8805 - val_loss: 0.4025 - val_accuracy: 0.8483\n",
      "Epoch 93/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2546 - accuracy: 0.9047\n",
      "Epoch 93: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2883 - accuracy: 0.8819 - val_loss: 0.4011 - val_accuracy: 0.8596\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3178 - accuracy: 0.8690\n",
      "Epoch 94: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8833 - val_loss: 0.4136 - val_accuracy: 0.8539\n",
      "Epoch 95/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3041 - accuracy: 0.8692\n",
      "Epoch 95: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8790 - val_loss: 0.4088 - val_accuracy: 0.8427\n",
      "Epoch 96/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2823 - accuracy: 0.8895\n",
      "Epoch 96: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8819 - val_loss: 0.4122 - val_accuracy: 0.8315\n",
      "Epoch 97/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2905 - accuracy: 0.8786\n",
      "Epoch 97: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.8790 - val_loss: 0.4044 - val_accuracy: 0.8427\n",
      "Epoch 98/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2561 - accuracy: 0.9000\n",
      "Epoch 98: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8776 - val_loss: 0.4127 - val_accuracy: 0.8427\n",
      "Epoch 99/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3009 - accuracy: 0.8619\n",
      "Epoch 99: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2846 - accuracy: 0.8790 - val_loss: 0.4036 - val_accuracy: 0.8427\n",
      "Epoch 100/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2766 - accuracy: 0.8767\n",
      "Epoch 100: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8833 - val_loss: 0.4054 - val_accuracy: 0.8539\n",
      "Data set number:  1\n",
      "Epoch 1/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3303 - accuracy: 0.8698\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83708, saving model to prediction_titanic\\best_model_batch_1.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8650 - val_loss: 0.3609 - val_accuracy: 0.8371\n",
      "Epoch 2/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.3324 - accuracy: 0.8595\n",
      "Epoch 2: val_accuracy improved from 0.83708 to 0.84270, saving model to prediction_titanic\\best_model_batch_1.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.8622 - val_loss: 0.3606 - val_accuracy: 0.8427\n",
      "Epoch 3/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3640 - accuracy: 0.8634\n",
      "Epoch 3: val_accuracy did not improve from 0.84270\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8790 - val_loss: 0.3781 - val_accuracy: 0.8427\n",
      "Epoch 4/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3000 - accuracy: 0.8674\n",
      "Epoch 4: val_accuracy improved from 0.84270 to 0.84831, saving model to prediction_titanic\\best_model_batch_1.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8622 - val_loss: 0.3728 - val_accuracy: 0.8483\n",
      "Epoch 5/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3033 - accuracy: 0.8625\n",
      "Epoch 5: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8664 - val_loss: 0.3750 - val_accuracy: 0.8483\n",
      "Epoch 6/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2993 - accuracy: 0.8690\n",
      "Epoch 6: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8594 - val_loss: 0.3749 - val_accuracy: 0.8483\n",
      "Epoch 7/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2706 - accuracy: 0.8930\n",
      "Epoch 7: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.8776 - val_loss: 0.3802 - val_accuracy: 0.8483\n",
      "Epoch 8/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3019 - accuracy: 0.8667\n",
      "Epoch 8: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8720 - val_loss: 0.3832 - val_accuracy: 0.8427\n",
      "Epoch 9/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3000 - accuracy: 0.8786\n",
      "Epoch 9: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8762 - val_loss: 0.3825 - val_accuracy: 0.8427\n",
      "Epoch 10/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2522 - accuracy: 0.9048\n",
      "Epoch 10: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8776 - val_loss: 0.3937 - val_accuracy: 0.8483\n",
      "Epoch 11/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3019 - accuracy: 0.8814\n",
      "Epoch 11: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8748 - val_loss: 0.3897 - val_accuracy: 0.8371\n",
      "Epoch 12/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3025 - accuracy: 0.8841\n",
      "Epoch 12: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8762 - val_loss: 0.3919 - val_accuracy: 0.8427\n",
      "Epoch 13/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3029 - accuracy: 0.8800\n",
      "Epoch 13: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8805 - val_loss: 0.3993 - val_accuracy: 0.8427\n",
      "Epoch 14/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3130 - accuracy: 0.8732\n",
      "Epoch 14: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8776 - val_loss: 0.3894 - val_accuracy: 0.8483\n",
      "Epoch 15/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2907 - accuracy: 0.8773\n",
      "Epoch 15: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8706 - val_loss: 0.3912 - val_accuracy: 0.8427\n",
      "Epoch 16/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.2978 - accuracy: 0.8687\n",
      "Epoch 16: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.8748 - val_loss: 0.3932 - val_accuracy: 0.8371\n",
      "Epoch 17/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2504 - accuracy: 0.9083\n",
      "Epoch 17: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2996 - accuracy: 0.8720 - val_loss: 0.3957 - val_accuracy: 0.8427\n",
      "Epoch 18/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3034 - accuracy: 0.8581\n",
      "Epoch 18: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8664 - val_loss: 0.3954 - val_accuracy: 0.8427\n",
      "Epoch 19/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2581 - accuracy: 0.8932\n",
      "Epoch 19: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8762 - val_loss: 0.3968 - val_accuracy: 0.8427\n",
      "Epoch 20/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2844 - accuracy: 0.8905\n",
      "Epoch 20: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8819 - val_loss: 0.3980 - val_accuracy: 0.8371\n",
      "Epoch 21/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.3054 - accuracy: 0.8727\n",
      "Epoch 21: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8776 - val_loss: 0.3966 - val_accuracy: 0.8427\n",
      "Epoch 22/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2915 - accuracy: 0.8810\n",
      "Epoch 22: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8776 - val_loss: 0.4068 - val_accuracy: 0.8371\n",
      "Epoch 23/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2711 - accuracy: 0.9000\n",
      "Epoch 23: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8805 - val_loss: 0.4026 - val_accuracy: 0.8371\n",
      "Epoch 24/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2932 - accuracy: 0.8881\n",
      "Epoch 24: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8819 - val_loss: 0.4074 - val_accuracy: 0.8371\n",
      "Epoch 25/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2732 - accuracy: 0.8878\n",
      "Epoch 25: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8805 - val_loss: 0.4032 - val_accuracy: 0.8427\n",
      "Epoch 26/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3246 - accuracy: 0.8667\n",
      "Epoch 26: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8819 - val_loss: 0.4087 - val_accuracy: 0.8371\n",
      "Epoch 27/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2845 - accuracy: 0.8905\n",
      "Epoch 27: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.8819 - val_loss: 0.4038 - val_accuracy: 0.8371\n",
      "Epoch 28/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2838 - accuracy: 0.8930\n",
      "Epoch 28: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8790 - val_loss: 0.4090 - val_accuracy: 0.8427\n",
      "Epoch 29/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2887 - accuracy: 0.8643\n",
      "Epoch 29: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2895 - accuracy: 0.8748 - val_loss: 0.4019 - val_accuracy: 0.8427\n",
      "Epoch 30/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2896 - accuracy: 0.8698\n",
      "Epoch 30: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8748 - val_loss: 0.4083 - val_accuracy: 0.8315\n",
      "Epoch 31/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.2928 - accuracy: 0.8762\n",
      "Epoch 31: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2890 - accuracy: 0.8805 - val_loss: 0.4064 - val_accuracy: 0.8427\n",
      "Epoch 32/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2938 - accuracy: 0.8764\n",
      "Epoch 32: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8762 - val_loss: 0.4059 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.8734\n",
      "Epoch 33: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8734 - val_loss: 0.4063 - val_accuracy: 0.8427\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.8805\n",
      "Epoch 34: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2875 - accuracy: 0.8805 - val_loss: 0.4040 - val_accuracy: 0.8427\n",
      "Epoch 35/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2881 - accuracy: 0.8861\n",
      "Epoch 35: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8847 - val_loss: 0.4113 - val_accuracy: 0.8315\n",
      "Epoch 36/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3115 - accuracy: 0.8628\n",
      "Epoch 36: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8790 - val_loss: 0.4087 - val_accuracy: 0.8427\n",
      "Epoch 37/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2823 - accuracy: 0.8714\n",
      "Epoch 37: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8790 - val_loss: 0.4122 - val_accuracy: 0.8315\n",
      "Epoch 38/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2886 - accuracy: 0.8762\n",
      "Epoch 38: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8875 - val_loss: 0.4083 - val_accuracy: 0.8315\n",
      "Epoch 39/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2834 - accuracy: 0.8806\n",
      "Epoch 39: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8847 - val_loss: 0.4083 - val_accuracy: 0.8315\n",
      "Epoch 40/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2601 - accuracy: 0.8953\n",
      "Epoch 40: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.8776 - val_loss: 0.4151 - val_accuracy: 0.8315\n",
      "Epoch 41/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2911 - accuracy: 0.8714\n",
      "Epoch 41: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8734 - val_loss: 0.4123 - val_accuracy: 0.8371\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2961 - accuracy: 0.8690\n",
      "Epoch 42: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8720 - val_loss: 0.4081 - val_accuracy: 0.8371\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2872 - accuracy: 0.8786\n",
      "Epoch 43: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8805 - val_loss: 0.4089 - val_accuracy: 0.8427\n",
      "Epoch 44/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8775\n",
      "Epoch 44: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8776 - val_loss: 0.4165 - val_accuracy: 0.8258\n",
      "Epoch 45/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3241 - accuracy: 0.8535\n",
      "Epoch 45: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.8805 - val_loss: 0.4214 - val_accuracy: 0.8371\n",
      "Epoch 46/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2978 - accuracy: 0.8814\n",
      "Epoch 46: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8847 - val_loss: 0.4188 - val_accuracy: 0.8258\n",
      "Epoch 47/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3156 - accuracy: 0.8581\n",
      "Epoch 47: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8776 - val_loss: 0.4257 - val_accuracy: 0.8315\n",
      "Epoch 48/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3071 - accuracy: 0.8714\n",
      "Epoch 48: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8805 - val_loss: 0.4193 - val_accuracy: 0.8483\n",
      "Epoch 49/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2858 - accuracy: 0.8721\n",
      "Epoch 49: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.8819 - val_loss: 0.4171 - val_accuracy: 0.8315\n",
      "Epoch 50/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2567 - accuracy: 0.8923\n",
      "Epoch 50: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8776 - val_loss: 0.4147 - val_accuracy: 0.8371\n",
      "Epoch 51/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2758 - accuracy: 0.8860\n",
      "Epoch 51: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.8861 - val_loss: 0.4197 - val_accuracy: 0.8315\n",
      "Epoch 52/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2560 - accuracy: 0.8929\n",
      "Epoch 52: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.8790 - val_loss: 0.4124 - val_accuracy: 0.8371\n",
      "Epoch 53/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2829 - accuracy: 0.8902\n",
      "Epoch 53: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.8861 - val_loss: 0.4175 - val_accuracy: 0.8258\n",
      "Epoch 54/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2940 - accuracy: 0.8762\n",
      "Epoch 54: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2809 - accuracy: 0.8875 - val_loss: 0.4251 - val_accuracy: 0.8315\n",
      "Epoch 55/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2756 - accuracy: 0.8881\n",
      "Epoch 55: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8819 - val_loss: 0.4170 - val_accuracy: 0.8315\n",
      "Epoch 56/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2666 - accuracy: 0.8953\n",
      "Epoch 56: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.8861 - val_loss: 0.4140 - val_accuracy: 0.8427\n",
      "Epoch 57/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2916 - accuracy: 0.8829\n",
      "Epoch 57: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8861 - val_loss: 0.4175 - val_accuracy: 0.8258\n",
      "Epoch 58/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2252 - accuracy: 0.9243\n",
      "Epoch 58: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8861 - val_loss: 0.4189 - val_accuracy: 0.8315\n",
      "Epoch 59/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2850 - accuracy: 0.8881\n",
      "Epoch 59: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8847 - val_loss: 0.4298 - val_accuracy: 0.8146\n",
      "Epoch 60/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2923 - accuracy: 0.8750\n",
      "Epoch 60: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8861 - val_loss: 0.4274 - val_accuracy: 0.8202\n",
      "Epoch 61/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2645 - accuracy: 0.8881\n",
      "Epoch 61: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8833 - val_loss: 0.4259 - val_accuracy: 0.8315\n",
      "Epoch 62/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2706 - accuracy: 0.8846\n",
      "Epoch 62: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2828 - accuracy: 0.8748 - val_loss: 0.4214 - val_accuracy: 0.8258\n",
      "Epoch 63/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2672 - accuracy: 0.8857\n",
      "Epoch 63: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.8847 - val_loss: 0.4242 - val_accuracy: 0.8258\n",
      "Epoch 64/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2747 - accuracy: 0.8854\n",
      "Epoch 64: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8861 - val_loss: 0.4195 - val_accuracy: 0.8258\n",
      "Epoch 65/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2370 - accuracy: 0.9047\n",
      "Epoch 65: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2733 - accuracy: 0.8917 - val_loss: 0.4374 - val_accuracy: 0.8258\n",
      "Epoch 66/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2855 - accuracy: 0.8857\n",
      "Epoch 66: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2801 - accuracy: 0.8805 - val_loss: 0.4437 - val_accuracy: 0.8202\n",
      "Epoch 67/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2933 - accuracy: 0.8860\n",
      "Epoch 67: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.8889 - val_loss: 0.4309 - val_accuracy: 0.8258\n",
      "Epoch 68/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3044 - accuracy: 0.8786\n",
      "Epoch 68: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.8833 - val_loss: 0.4265 - val_accuracy: 0.8315\n",
      "Epoch 69/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3033 - accuracy: 0.8786\n",
      "Epoch 69: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8776 - val_loss: 0.4267 - val_accuracy: 0.8483\n",
      "Epoch 70/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2837 - accuracy: 0.8837\n",
      "Epoch 70: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8875 - val_loss: 0.4314 - val_accuracy: 0.8258\n",
      "Epoch 71/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2642 - accuracy: 0.8966\n",
      "Epoch 71: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2758 - accuracy: 0.8889 - val_loss: 0.4298 - val_accuracy: 0.8315\n",
      "Epoch 72/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2640 - accuracy: 0.8975\n",
      "Epoch 72: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.8875 - val_loss: 0.4286 - val_accuracy: 0.8315\n",
      "Epoch 73/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2790 - accuracy: 0.8757\n",
      "Epoch 73: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.8734 - val_loss: 0.4297 - val_accuracy: 0.8371\n",
      "Epoch 74/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2683 - accuracy: 0.8905\n",
      "Epoch 74: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.8875 - val_loss: 0.4290 - val_accuracy: 0.8258\n",
      "Epoch 75/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2634 - accuracy: 0.8930\n",
      "Epoch 75: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8917 - val_loss: 0.4332 - val_accuracy: 0.8090\n",
      "Epoch 76/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2845 - accuracy: 0.8860\n",
      "Epoch 76: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.8875 - val_loss: 0.4385 - val_accuracy: 0.8202\n",
      "Epoch 77/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2704 - accuracy: 0.8905\n",
      "Epoch 77: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8889 - val_loss: 0.4264 - val_accuracy: 0.8315\n",
      "Epoch 78/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2766 - accuracy: 0.8884\n",
      "Epoch 78: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8875 - val_loss: 0.4335 - val_accuracy: 0.8371\n",
      "Epoch 79/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2536 - accuracy: 0.9150\n",
      "Epoch 79: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.8917 - val_loss: 0.4321 - val_accuracy: 0.8258\n",
      "Epoch 80/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2806 - accuracy: 0.8846\n",
      "Epoch 80: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8861 - val_loss: 0.4362 - val_accuracy: 0.8202\n",
      "Epoch 81/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2659 - accuracy: 0.8837\n",
      "Epoch 81: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8819 - val_loss: 0.4343 - val_accuracy: 0.8258\n",
      "Epoch 82/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2590 - accuracy: 0.8929\n",
      "Epoch 82: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8861 - val_loss: 0.4352 - val_accuracy: 0.8202\n",
      "Epoch 83/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2675 - accuracy: 0.8929\n",
      "Epoch 83: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8819 - val_loss: 0.4353 - val_accuracy: 0.8258\n",
      "Epoch 84/100\n",
      "34/72 [=============>................] - ETA: 0s - loss: 0.2896 - accuracy: 0.8824\n",
      "Epoch 84: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.8945 - val_loss: 0.4355 - val_accuracy: 0.8258\n",
      "Epoch 85/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2717 - accuracy: 0.8884\n",
      "Epoch 85: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8903 - val_loss: 0.4417 - val_accuracy: 0.8034\n",
      "Epoch 86/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2708 - accuracy: 0.8884\n",
      "Epoch 86: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8875 - val_loss: 0.4360 - val_accuracy: 0.8258\n",
      "Epoch 87/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2985 - accuracy: 0.8791\n",
      "Epoch 87: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.8847 - val_loss: 0.4321 - val_accuracy: 0.8258\n",
      "Epoch 88/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2799 - accuracy: 0.8842\n",
      "Epoch 88: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8833 - val_loss: 0.4409 - val_accuracy: 0.8202\n",
      "Epoch 89/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2441 - accuracy: 0.8976\n",
      "Epoch 89: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.8805 - val_loss: 0.4356 - val_accuracy: 0.8202\n",
      "Epoch 90/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2870 - accuracy: 0.8833\n",
      "Epoch 90: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8903 - val_loss: 0.4350 - val_accuracy: 0.8202\n",
      "Epoch 91/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2682 - accuracy: 0.8905\n",
      "Epoch 91: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8945 - val_loss: 0.4421 - val_accuracy: 0.8090\n",
      "Epoch 92/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2813 - accuracy: 0.8762\n",
      "Epoch 92: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8776 - val_loss: 0.4381 - val_accuracy: 0.8202\n",
      "Epoch 93/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2834 - accuracy: 0.8930\n",
      "Epoch 93: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8931 - val_loss: 0.4348 - val_accuracy: 0.8146\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2706 - accuracy: 0.8881\n",
      "Epoch 94: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8903 - val_loss: 0.4425 - val_accuracy: 0.8315\n",
      "Epoch 95/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2817 - accuracy: 0.8833\n",
      "Epoch 95: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8875 - val_loss: 0.4343 - val_accuracy: 0.8202\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8847\n",
      "Epoch 96: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8847 - val_loss: 0.4415 - val_accuracy: 0.8146\n",
      "Epoch 97/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2671 - accuracy: 0.8907\n",
      "Epoch 97: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2688 - accuracy: 0.8903 - val_loss: 0.4292 - val_accuracy: 0.8258\n",
      "Epoch 98/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2507 - accuracy: 0.8905\n",
      "Epoch 98: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.8819 - val_loss: 0.4424 - val_accuracy: 0.7978\n",
      "Epoch 99/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2548 - accuracy: 0.9023\n",
      "Epoch 99: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8861 - val_loss: 0.4330 - val_accuracy: 0.8315\n",
      "Epoch 100/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2530 - accuracy: 0.9070\n",
      "Epoch 100: val_accuracy did not improve from 0.84831\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8931 - val_loss: 0.4487 - val_accuracy: 0.8090\n",
      "Data set number:  2\n",
      "Epoch 1/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3293 - accuracy: 0.8674\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83708, saving model to prediction_titanic\\best_model_batch_2.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8678 - val_loss: 0.3453 - val_accuracy: 0.8371\n",
      "Epoch 2/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3297 - accuracy: 0.8692\n",
      "Epoch 2: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8790 - val_loss: 0.3569 - val_accuracy: 0.8258\n",
      "Epoch 3/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2712 - accuracy: 0.8921\n",
      "Epoch 3: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8819 - val_loss: 0.3569 - val_accuracy: 0.8315\n",
      "Epoch 4/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3103 - accuracy: 0.8698\n",
      "Epoch 4: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.8678 - val_loss: 0.3725 - val_accuracy: 0.8258\n",
      "Epoch 5/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2734 - accuracy: 0.8930\n",
      "Epoch 5: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8805 - val_loss: 0.3713 - val_accuracy: 0.8146\n",
      "Epoch 6/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3081 - accuracy: 0.8714\n",
      "Epoch 6: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8748 - val_loss: 0.3736 - val_accuracy: 0.8258\n",
      "Epoch 7/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2744 - accuracy: 0.8961\n",
      "Epoch 7: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8903 - val_loss: 0.3726 - val_accuracy: 0.8258\n",
      "Epoch 8/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2927 - accuracy: 0.8764\n",
      "Epoch 8: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8805 - val_loss: 0.3713 - val_accuracy: 0.8090\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.8861\n",
      "Epoch 9: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8861 - val_loss: 0.3696 - val_accuracy: 0.8146\n",
      "Epoch 10/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.2774 - accuracy: 0.8894\n",
      "Epoch 10: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.8861 - val_loss: 0.3895 - val_accuracy: 0.8090\n",
      "Epoch 11/100\n",
      "35/72 [=============>................] - ETA: 0s - loss: 0.2759 - accuracy: 0.9029\n",
      "Epoch 11: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.8917 - val_loss: 0.3774 - val_accuracy: 0.8034\n",
      "Epoch 12/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2721 - accuracy: 0.9053\n",
      "Epoch 12: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8903 - val_loss: 0.3809 - val_accuracy: 0.8090\n",
      "Epoch 13/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2759 - accuracy: 0.8927\n",
      "Epoch 13: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8861 - val_loss: 0.3822 - val_accuracy: 0.8034\n",
      "Epoch 14/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2745 - accuracy: 0.8929\n",
      "Epoch 14: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.8833 - val_loss: 0.3867 - val_accuracy: 0.8146\n",
      "Epoch 15/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2592 - accuracy: 0.9000\n",
      "Epoch 15: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8903 - val_loss: 0.3918 - val_accuracy: 0.8090\n",
      "Epoch 16/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2799 - accuracy: 0.8896\n",
      "Epoch 16: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8903 - val_loss: 0.3874 - val_accuracy: 0.8090\n",
      "Epoch 17/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2488 - accuracy: 0.9095\n",
      "Epoch 17: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8903 - val_loss: 0.3956 - val_accuracy: 0.8090\n",
      "Epoch 18/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2376 - accuracy: 0.9116\n",
      "Epoch 18: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8931 - val_loss: 0.4107 - val_accuracy: 0.8090\n",
      "Epoch 19/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2949 - accuracy: 0.8795\n",
      "Epoch 19: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2789 - accuracy: 0.8875 - val_loss: 0.4050 - val_accuracy: 0.8090\n",
      "Epoch 20/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2590 - accuracy: 0.8884\n",
      "Epoch 20: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.8875 - val_loss: 0.4133 - val_accuracy: 0.8034\n",
      "Epoch 21/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2909 - accuracy: 0.8659\n",
      "Epoch 21: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8819 - val_loss: 0.4197 - val_accuracy: 0.8090\n",
      "Epoch 22/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2732 - accuracy: 0.8907\n",
      "Epoch 22: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8945 - val_loss: 0.4102 - val_accuracy: 0.8090\n",
      "Epoch 23/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2721 - accuracy: 0.8837\n",
      "Epoch 23: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8889 - val_loss: 0.4068 - val_accuracy: 0.8146\n",
      "Epoch 24/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2808 - accuracy: 0.8854\n",
      "Epoch 24: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.8973 - val_loss: 0.4088 - val_accuracy: 0.8146\n",
      "Epoch 25/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2922 - accuracy: 0.8975\n",
      "Epoch 25: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8959 - val_loss: 0.4168 - val_accuracy: 0.8146\n",
      "Epoch 26/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2748 - accuracy: 0.8872\n",
      "Epoch 26: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8875 - val_loss: 0.3982 - val_accuracy: 0.8146\n",
      "Epoch 27/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.2678 - accuracy: 0.8938\n",
      "Epoch 27: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8889 - val_loss: 0.4148 - val_accuracy: 0.8090\n",
      "Epoch 28/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2782 - accuracy: 0.8905\n",
      "Epoch 28: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8945 - val_loss: 0.4001 - val_accuracy: 0.8090\n",
      "Epoch 29/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2573 - accuracy: 0.9000\n",
      "Epoch 29: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8945 - val_loss: 0.4053 - val_accuracy: 0.7978\n",
      "Epoch 30/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2783 - accuracy: 0.8905\n",
      "Epoch 30: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8903 - val_loss: 0.4229 - val_accuracy: 0.8202\n",
      "Epoch 31/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2734 - accuracy: 0.9024\n",
      "Epoch 31: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8973 - val_loss: 0.4151 - val_accuracy: 0.7978\n",
      "Epoch 32/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2715 - accuracy: 0.8952\n",
      "Epoch 32: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8903 - val_loss: 0.4234 - val_accuracy: 0.8090\n",
      "Epoch 33/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2920 - accuracy: 0.8881\n",
      "Epoch 33: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.9001 - val_loss: 0.4325 - val_accuracy: 0.8090\n",
      "Epoch 34/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2546 - accuracy: 0.9068\n",
      "Epoch 34: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.8959 - val_loss: 0.4173 - val_accuracy: 0.7978\n",
      "Epoch 35/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2451 - accuracy: 0.9073\n",
      "Epoch 35: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.9001 - val_loss: 0.4321 - val_accuracy: 0.8146\n",
      "Epoch 36/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2813 - accuracy: 0.8930\n",
      "Epoch 36: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.9015 - val_loss: 0.4147 - val_accuracy: 0.8090\n",
      "Epoch 37/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2762 - accuracy: 0.8952\n",
      "Epoch 37: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8889 - val_loss: 0.4216 - val_accuracy: 0.8090\n",
      "Epoch 38/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2643 - accuracy: 0.8925\n",
      "Epoch 38: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8945 - val_loss: 0.4257 - val_accuracy: 0.8146\n",
      "Epoch 39/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2701 - accuracy: 0.8966\n",
      "Epoch 39: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8959 - val_loss: 0.4280 - val_accuracy: 0.8090\n",
      "Epoch 40/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2794 - accuracy: 0.8902\n",
      "Epoch 40: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.8973 - val_loss: 0.4297 - val_accuracy: 0.8146\n",
      "Epoch 41/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2587 - accuracy: 0.8907\n",
      "Epoch 41: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8945 - val_loss: 0.4330 - val_accuracy: 0.8034\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2521 - accuracy: 0.9024\n",
      "Epoch 42: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8959 - val_loss: 0.4252 - val_accuracy: 0.8090\n",
      "Epoch 43/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2740 - accuracy: 0.8953\n",
      "Epoch 43: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8959 - val_loss: 0.4169 - val_accuracy: 0.8090\n",
      "Epoch 44/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2753 - accuracy: 0.8953\n",
      "Epoch 44: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8973 - val_loss: 0.4284 - val_accuracy: 0.7865\n",
      "Epoch 45/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2503 - accuracy: 0.9000\n",
      "Epoch 45: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8987 - val_loss: 0.4418 - val_accuracy: 0.8090\n",
      "Epoch 46/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2426 - accuracy: 0.9070\n",
      "Epoch 46: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9001 - val_loss: 0.4528 - val_accuracy: 0.7978\n",
      "Epoch 47/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2750 - accuracy: 0.8930\n",
      "Epoch 47: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8987 - val_loss: 0.4410 - val_accuracy: 0.8090\n",
      "Epoch 48/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2645 - accuracy: 0.9000\n",
      "Epoch 48: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.9001 - val_loss: 0.4209 - val_accuracy: 0.8090\n",
      "Epoch 49/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2574 - accuracy: 0.9070\n",
      "Epoch 49: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2621 - accuracy: 0.9030 - val_loss: 0.4384 - val_accuracy: 0.7978\n",
      "Epoch 50/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2626 - accuracy: 0.8930\n",
      "Epoch 50: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8917 - val_loss: 0.4348 - val_accuracy: 0.8090\n",
      "Epoch 51/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2797 - accuracy: 0.8821\n",
      "Epoch 51: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2610 - accuracy: 0.8945 - val_loss: 0.4373 - val_accuracy: 0.8146\n",
      "Epoch 52/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2825 - accuracy: 0.8902\n",
      "Epoch 52: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8973 - val_loss: 0.4452 - val_accuracy: 0.8090\n",
      "Epoch 53/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2449 - accuracy: 0.9073\n",
      "Epoch 53: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.9001 - val_loss: 0.4365 - val_accuracy: 0.8090\n",
      "Epoch 54/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2574 - accuracy: 0.9047\n",
      "Epoch 54: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8959 - val_loss: 0.4351 - val_accuracy: 0.8090\n",
      "Epoch 55/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2911 - accuracy: 0.8932\n",
      "Epoch 55: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.9015 - val_loss: 0.4442 - val_accuracy: 0.7978\n",
      "Epoch 56/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3062 - accuracy: 0.8775\n",
      "Epoch 56: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8987 - val_loss: 0.4484 - val_accuracy: 0.8034\n",
      "Epoch 57/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2561 - accuracy: 0.9073\n",
      "Epoch 57: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.9015 - val_loss: 0.4423 - val_accuracy: 0.7978\n",
      "Epoch 58/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2711 - accuracy: 0.8976\n",
      "Epoch 58: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.9001 - val_loss: 0.4420 - val_accuracy: 0.8090\n",
      "Epoch 59/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2475 - accuracy: 0.9023\n",
      "Epoch 59: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.8987 - val_loss: 0.4521 - val_accuracy: 0.8034\n",
      "Epoch 60/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.8958\n",
      "Epoch 60: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8945 - val_loss: 0.4477 - val_accuracy: 0.8090\n",
      "Epoch 61/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2758 - accuracy: 0.8955\n",
      "Epoch 61: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8931 - val_loss: 0.4535 - val_accuracy: 0.7865\n",
      "Epoch 62/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2406 - accuracy: 0.9116\n",
      "Epoch 62: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8987 - val_loss: 0.4675 - val_accuracy: 0.8034\n",
      "Epoch 63/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2594 - accuracy: 0.9091\n",
      "Epoch 63: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8959 - val_loss: 0.4543 - val_accuracy: 0.8146\n",
      "Epoch 64/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2502 - accuracy: 0.8974\n",
      "Epoch 64: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2584 - accuracy: 0.9001 - val_loss: 0.4617 - val_accuracy: 0.8034\n",
      "Epoch 65/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2679 - accuracy: 0.9024\n",
      "Epoch 65: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9044 - val_loss: 0.4567 - val_accuracy: 0.8090\n",
      "Epoch 66/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2664 - accuracy: 0.8976\n",
      "Epoch 66: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8973 - val_loss: 0.4510 - val_accuracy: 0.8034\n",
      "Epoch 67/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2727 - accuracy: 0.8881\n",
      "Epoch 67: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.8959 - val_loss: 0.4661 - val_accuracy: 0.8146\n",
      "Epoch 68/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2405 - accuracy: 0.9122\n",
      "Epoch 68: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.9030 - val_loss: 0.4671 - val_accuracy: 0.7978\n",
      "Epoch 69/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2712 - accuracy: 0.8884\n",
      "Epoch 69: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8973 - val_loss: 0.4529 - val_accuracy: 0.8090\n",
      "Epoch 70/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2574 - accuracy: 0.9023\n",
      "Epoch 70: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8903 - val_loss: 0.4581 - val_accuracy: 0.8034\n",
      "Epoch 71/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2551 - accuracy: 0.8985\n",
      "Epoch 71: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8973 - val_loss: 0.4596 - val_accuracy: 0.8034\n",
      "Epoch 72/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2514 - accuracy: 0.9025\n",
      "Epoch 72: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.8987 - val_loss: 0.4551 - val_accuracy: 0.7978\n",
      "Epoch 73/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2311 - accuracy: 0.9070\n",
      "Epoch 73: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.9030 - val_loss: 0.4521 - val_accuracy: 0.8034\n",
      "Epoch 74/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2812 - accuracy: 0.8905\n",
      "Epoch 74: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8959 - val_loss: 0.4560 - val_accuracy: 0.8034\n",
      "Epoch 75/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2730 - accuracy: 0.9000\n",
      "Epoch 75: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8973 - val_loss: 0.4580 - val_accuracy: 0.8090\n",
      "Epoch 76/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2613 - accuracy: 0.9023\n",
      "Epoch 76: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9072 - val_loss: 0.4636 - val_accuracy: 0.8034\n",
      "Epoch 77/100\n",
      "45/72 [=================>............] - ETA: 0s - loss: 0.2660 - accuracy: 0.8867\n",
      "Epoch 77: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.8931 - val_loss: 0.4638 - val_accuracy: 0.7978\n",
      "Epoch 78/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2578 - accuracy: 0.8953\n",
      "Epoch 78: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8973 - val_loss: 0.4814 - val_accuracy: 0.7978\n",
      "Epoch 79/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2541 - accuracy: 0.9100\n",
      "Epoch 79: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8987 - val_loss: 0.4658 - val_accuracy: 0.8034\n",
      "Epoch 80/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2692 - accuracy: 0.8919\n",
      "Epoch 80: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.8987 - val_loss: 0.4763 - val_accuracy: 0.8090\n",
      "Epoch 81/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.9014\n",
      "Epoch 81: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.9015 - val_loss: 0.4759 - val_accuracy: 0.7921\n",
      "Epoch 82/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2333 - accuracy: 0.9150\n",
      "Epoch 82: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.9058 - val_loss: 0.4627 - val_accuracy: 0.8034\n",
      "Epoch 83/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2468 - accuracy: 0.9093\n",
      "Epoch 83: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.8987 - val_loss: 0.4772 - val_accuracy: 0.8090\n",
      "Epoch 84/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.9014\n",
      "Epoch 84: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9001 - val_loss: 0.4777 - val_accuracy: 0.8090\n",
      "Epoch 85/100\n",
      "61/72 [========================>.....] - ETA: 0s - loss: 0.2635 - accuracy: 0.8934\n",
      "Epoch 85: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.8973 - val_loss: 0.4802 - val_accuracy: 0.8202\n",
      "Epoch 86/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2453 - accuracy: 0.9058\n",
      "Epoch 86: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.9030 - val_loss: 0.4896 - val_accuracy: 0.8034\n",
      "Epoch 87/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2548 - accuracy: 0.9000\n",
      "Epoch 87: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9015 - val_loss: 0.4912 - val_accuracy: 0.8034\n",
      "Epoch 88/100\n",
      "35/72 [=============>................] - ETA: 0s - loss: 0.2393 - accuracy: 0.9086\n",
      "Epoch 88: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9015 - val_loss: 0.4697 - val_accuracy: 0.8034\n",
      "Epoch 89/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2278 - accuracy: 0.9186\n",
      "Epoch 89: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.9072 - val_loss: 0.4953 - val_accuracy: 0.8090\n",
      "Epoch 90/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2495 - accuracy: 0.9083\n",
      "Epoch 90: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9044 - val_loss: 0.4807 - val_accuracy: 0.8034\n",
      "Epoch 91/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2436 - accuracy: 0.9047\n",
      "Epoch 91: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9015 - val_loss: 0.4817 - val_accuracy: 0.7978\n",
      "Epoch 92/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2681 - accuracy: 0.8977\n",
      "Epoch 92: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2547 - accuracy: 0.9015 - val_loss: 0.4676 - val_accuracy: 0.8090\n",
      "Epoch 93/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2508 - accuracy: 0.9091\n",
      "Epoch 93: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.9044 - val_loss: 0.4845 - val_accuracy: 0.8090\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2775 - accuracy: 0.8762\n",
      "Epoch 94: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8917 - val_loss: 0.4850 - val_accuracy: 0.7978\n",
      "Epoch 95/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2413 - accuracy: 0.9071\n",
      "Epoch 95: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.9030 - val_loss: 0.4975 - val_accuracy: 0.8034\n",
      "Epoch 96/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2566 - accuracy: 0.9122\n",
      "Epoch 96: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9015 - val_loss: 0.4846 - val_accuracy: 0.8034\n",
      "Epoch 97/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2494 - accuracy: 0.9000\n",
      "Epoch 97: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9001 - val_loss: 0.4957 - val_accuracy: 0.7978\n",
      "Epoch 98/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.2483 - accuracy: 0.9000\n",
      "Epoch 98: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8987 - val_loss: 0.4911 - val_accuracy: 0.7978\n",
      "Epoch 99/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2456 - accuracy: 0.9000\n",
      "Epoch 99: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.9030 - val_loss: 0.4765 - val_accuracy: 0.7921\n",
      "Epoch 100/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2620 - accuracy: 0.9023\n",
      "Epoch 100: val_accuracy did not improve from 0.83708\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9072 - val_loss: 0.4959 - val_accuracy: 0.8090\n",
      "Data set number:  3\n",
      "Epoch 1/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2925 - accuracy: 0.8786\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87079, saving model to prediction_titanic\\best_model_batch_3.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8692 - val_loss: 0.3436 - val_accuracy: 0.8708\n",
      "Epoch 2/100\n",
      "34/72 [=============>................] - ETA: 0s - loss: 0.2778 - accuracy: 0.8882\n",
      "Epoch 2: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2926 - accuracy: 0.8819 - val_loss: 0.3509 - val_accuracy: 0.8652\n",
      "Epoch 3/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2965 - accuracy: 0.8750\n",
      "Epoch 3: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8847 - val_loss: 0.3484 - val_accuracy: 0.8652\n",
      "Epoch 4/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2854 - accuracy: 0.8930\n",
      "Epoch 4: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8861 - val_loss: 0.3558 - val_accuracy: 0.8708\n",
      "Epoch 5/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2737 - accuracy: 0.8850\n",
      "Epoch 5: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.8833 - val_loss: 0.3690 - val_accuracy: 0.8652\n",
      "Epoch 6/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2346 - accuracy: 0.9140\n",
      "Epoch 6: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.8861 - val_loss: 0.3614 - val_accuracy: 0.8539\n",
      "Epoch 7/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2837 - accuracy: 0.8794\n",
      "Epoch 7: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8819 - val_loss: 0.3698 - val_accuracy: 0.8539\n",
      "Epoch 8/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2456 - accuracy: 0.8953\n",
      "Epoch 8: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8875 - val_loss: 0.3993 - val_accuracy: 0.8483\n",
      "Epoch 9/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2756 - accuracy: 0.8950\n",
      "Epoch 9: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8875 - val_loss: 0.3794 - val_accuracy: 0.8483\n",
      "Epoch 10/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2820 - accuracy: 0.8907\n",
      "Epoch 10: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8959 - val_loss: 0.3960 - val_accuracy: 0.8483\n",
      "Epoch 11/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2902 - accuracy: 0.8818\n",
      "Epoch 11: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8903 - val_loss: 0.3742 - val_accuracy: 0.8539\n",
      "Epoch 12/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2603 - accuracy: 0.8878\n",
      "Epoch 12: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8875 - val_loss: 0.3765 - val_accuracy: 0.8539\n",
      "Epoch 13/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3240 - accuracy: 0.8571\n",
      "Epoch 13: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8805 - val_loss: 0.3792 - val_accuracy: 0.8596\n",
      "Epoch 14/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2818 - accuracy: 0.8860\n",
      "Epoch 14: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8861 - val_loss: 0.4040 - val_accuracy: 0.8427\n",
      "Epoch 15/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2693 - accuracy: 0.8896\n",
      "Epoch 15: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8889 - val_loss: 0.3908 - val_accuracy: 0.8539\n",
      "Epoch 16/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2911 - accuracy: 0.8641\n",
      "Epoch 16: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.8875 - val_loss: 0.3930 - val_accuracy: 0.8483\n",
      "Epoch 17/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2780 - accuracy: 0.8814\n",
      "Epoch 17: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8833 - val_loss: 0.3842 - val_accuracy: 0.8371\n",
      "Epoch 18/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2711 - accuracy: 0.8810\n",
      "Epoch 18: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.8790 - val_loss: 0.3949 - val_accuracy: 0.8483\n",
      "Epoch 19/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2615 - accuracy: 0.8929\n",
      "Epoch 19: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8875 - val_loss: 0.3819 - val_accuracy: 0.8427\n",
      "Epoch 20/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2843 - accuracy: 0.8805\n",
      "Epoch 20: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.8889 - val_loss: 0.3964 - val_accuracy: 0.8427\n",
      "Epoch 21/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3022 - accuracy: 0.8674\n",
      "Epoch 21: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.8819 - val_loss: 0.3837 - val_accuracy: 0.8427\n",
      "Epoch 22/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2478 - accuracy: 0.9071\n",
      "Epoch 22: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8959 - val_loss: 0.3890 - val_accuracy: 0.8596\n",
      "Epoch 23/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2654 - accuracy: 0.8836\n",
      "Epoch 23: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.8931 - val_loss: 0.4106 - val_accuracy: 0.8315\n",
      "Epoch 24/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2506 - accuracy: 0.9051\n",
      "Epoch 24: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8973 - val_loss: 0.3708 - val_accuracy: 0.8371\n",
      "Epoch 25/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2548 - accuracy: 0.8927\n",
      "Epoch 25: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.8945 - val_loss: 0.4017 - val_accuracy: 0.8427\n",
      "Epoch 26/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2840 - accuracy: 0.8905\n",
      "Epoch 26: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2591 - accuracy: 0.8973 - val_loss: 0.4024 - val_accuracy: 0.8483\n",
      "Epoch 27/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2271 - accuracy: 0.9053\n",
      "Epoch 27: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8889 - val_loss: 0.4089 - val_accuracy: 0.8371\n",
      "Epoch 28/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2660 - accuracy: 0.8683\n",
      "Epoch 28: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.8875 - val_loss: 0.3877 - val_accuracy: 0.8371\n",
      "Epoch 29/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2650 - accuracy: 0.8833\n",
      "Epoch 29: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.8833 - val_loss: 0.4096 - val_accuracy: 0.8427\n",
      "Epoch 30/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2625 - accuracy: 0.8927\n",
      "Epoch 30: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8847 - val_loss: 0.4070 - val_accuracy: 0.8539\n",
      "Epoch 31/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2499 - accuracy: 0.8976\n",
      "Epoch 31: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8917 - val_loss: 0.3971 - val_accuracy: 0.8427\n",
      "Epoch 32/100\n",
      "60/72 [========================>.....] - ETA: 0s - loss: 0.2521 - accuracy: 0.8950\n",
      "Epoch 32: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8917 - val_loss: 0.4016 - val_accuracy: 0.8427\n",
      "Epoch 33/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2520 - accuracy: 0.8977\n",
      "Epoch 33: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.8861 - val_loss: 0.3968 - val_accuracy: 0.8371\n",
      "Epoch 34/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2498 - accuracy: 0.8949\n",
      "Epoch 34: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.8917 - val_loss: 0.4001 - val_accuracy: 0.8371\n",
      "Epoch 35/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2387 - accuracy: 0.8952\n",
      "Epoch 35: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.8861 - val_loss: 0.4118 - val_accuracy: 0.8427\n",
      "Epoch 36/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2200 - accuracy: 0.9100\n",
      "Epoch 36: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.8917 - val_loss: 0.4089 - val_accuracy: 0.8315\n",
      "Epoch 37/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2563 - accuracy: 0.8951\n",
      "Epoch 37: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8931 - val_loss: 0.4109 - val_accuracy: 0.8427\n",
      "Epoch 38/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2514 - accuracy: 0.8868\n",
      "Epoch 38: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.8861 - val_loss: 0.3996 - val_accuracy: 0.8315\n",
      "Epoch 39/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2366 - accuracy: 0.9000\n",
      "Epoch 39: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8931 - val_loss: 0.3932 - val_accuracy: 0.8371\n",
      "Epoch 40/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2491 - accuracy: 0.8952\n",
      "Epoch 40: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.8931 - val_loss: 0.3964 - val_accuracy: 0.8427\n",
      "Epoch 41/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2490 - accuracy: 0.8927\n",
      "Epoch 41: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.8875 - val_loss: 0.3963 - val_accuracy: 0.8539\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2295 - accuracy: 0.9071\n",
      "Epoch 42: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.8903 - val_loss: 0.4142 - val_accuracy: 0.8596\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2663 - accuracy: 0.8786\n",
      "Epoch 43: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8875 - val_loss: 0.4115 - val_accuracy: 0.8483\n",
      "Epoch 44/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2430 - accuracy: 0.9047\n",
      "Epoch 44: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9001 - val_loss: 0.4120 - val_accuracy: 0.8427\n",
      "Epoch 45/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2243 - accuracy: 0.9053\n",
      "Epoch 45: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.8903 - val_loss: 0.4252 - val_accuracy: 0.8258\n",
      "Epoch 46/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2600 - accuracy: 0.8786\n",
      "Epoch 46: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8861 - val_loss: 0.4116 - val_accuracy: 0.8483\n",
      "Epoch 47/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2623 - accuracy: 0.8923\n",
      "Epoch 47: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8931 - val_loss: 0.4167 - val_accuracy: 0.8427\n",
      "Epoch 48/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2407 - accuracy: 0.8977\n",
      "Epoch 48: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8931 - val_loss: 0.4083 - val_accuracy: 0.8371\n",
      "Epoch 49/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2540 - accuracy: 0.8855\n",
      "Epoch 49: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.8889 - val_loss: 0.4217 - val_accuracy: 0.8371\n",
      "Epoch 50/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2899 - accuracy: 0.8841\n",
      "Epoch 50: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2606 - accuracy: 0.8917 - val_loss: 0.4230 - val_accuracy: 0.8483\n",
      "Epoch 51/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2357 - accuracy: 0.8929\n",
      "Epoch 51: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.8833 - val_loss: 0.4181 - val_accuracy: 0.8371\n",
      "Epoch 52/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2496 - accuracy: 0.8968\n",
      "Epoch 52: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.8931 - val_loss: 0.4056 - val_accuracy: 0.8371\n",
      "Epoch 53/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2375 - accuracy: 0.8976\n",
      "Epoch 53: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.8847 - val_loss: 0.4352 - val_accuracy: 0.8315\n",
      "Epoch 54/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2571 - accuracy: 0.8953\n",
      "Epoch 54: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2462 - accuracy: 0.9015 - val_loss: 0.4099 - val_accuracy: 0.8371\n",
      "Epoch 55/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2395 - accuracy: 0.9050\n",
      "Epoch 55: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.8945 - val_loss: 0.4255 - val_accuracy: 0.8315\n",
      "Epoch 56/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2448 - accuracy: 0.8905\n",
      "Epoch 56: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.8875 - val_loss: 0.4197 - val_accuracy: 0.8427\n",
      "Epoch 57/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2506 - accuracy: 0.8942\n",
      "Epoch 57: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.8959 - val_loss: 0.4179 - val_accuracy: 0.8483\n",
      "Epoch 58/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2642 - accuracy: 0.8762\n",
      "Epoch 58: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.8889 - val_loss: 0.4376 - val_accuracy: 0.8539\n",
      "Epoch 59/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2459 - accuracy: 0.8980\n",
      "Epoch 59: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8959 - val_loss: 0.4361 - val_accuracy: 0.8315\n",
      "Epoch 60/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2453 - accuracy: 0.9027\n",
      "Epoch 60: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.8959 - val_loss: 0.4453 - val_accuracy: 0.8371\n",
      "Epoch 61/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.8943\n",
      "Epoch 61: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.8931 - val_loss: 0.4108 - val_accuracy: 0.8371\n",
      "Epoch 62/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2432 - accuracy: 0.8970\n",
      "Epoch 62: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8973 - val_loss: 0.4183 - val_accuracy: 0.8371\n",
      "Epoch 63/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2439 - accuracy: 0.9000\n",
      "Epoch 63: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8931 - val_loss: 0.4441 - val_accuracy: 0.8539\n",
      "Epoch 64/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2406 - accuracy: 0.9000\n",
      "Epoch 64: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.8903 - val_loss: 0.4230 - val_accuracy: 0.8371\n",
      "Epoch 65/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2602 - accuracy: 0.8927\n",
      "Epoch 65: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.8945 - val_loss: 0.4426 - val_accuracy: 0.8483\n",
      "Epoch 66/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2562 - accuracy: 0.8857\n",
      "Epoch 66: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.8903 - val_loss: 0.4348 - val_accuracy: 0.8539\n",
      "Epoch 67/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2646 - accuracy: 0.9024\n",
      "Epoch 67: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9001 - val_loss: 0.4423 - val_accuracy: 0.8539\n",
      "Epoch 68/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2263 - accuracy: 0.9068\n",
      "Epoch 68: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.8931 - val_loss: 0.4428 - val_accuracy: 0.8315\n",
      "Epoch 69/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2285 - accuracy: 0.9000\n",
      "Epoch 69: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.8959 - val_loss: 0.4307 - val_accuracy: 0.8258\n",
      "Epoch 70/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2549 - accuracy: 0.8953\n",
      "Epoch 70: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.8959 - val_loss: 0.4357 - val_accuracy: 0.8427\n",
      "Epoch 71/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2524 - accuracy: 0.8975\n",
      "Epoch 71: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.8917 - val_loss: 0.4527 - val_accuracy: 0.8483\n",
      "Epoch 72/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2556 - accuracy: 0.8884\n",
      "Epoch 72: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8917 - val_loss: 0.4434 - val_accuracy: 0.8371\n",
      "Epoch 73/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.8930\n",
      "Epoch 73: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2437 - accuracy: 0.8931 - val_loss: 0.4559 - val_accuracy: 0.8315\n",
      "Epoch 74/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2399 - accuracy: 0.9000\n",
      "Epoch 74: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8917 - val_loss: 0.4286 - val_accuracy: 0.8371\n",
      "Epoch 75/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2523 - accuracy: 0.8952\n",
      "Epoch 75: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.8945 - val_loss: 0.4386 - val_accuracy: 0.8371\n",
      "Epoch 76/100\n",
      "54/72 [=====================>........] - ETA: 0s - loss: 0.2624 - accuracy: 0.8870\n",
      "Epoch 76: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8973 - val_loss: 0.4449 - val_accuracy: 0.8483\n",
      "Epoch 77/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2563 - accuracy: 0.8900\n",
      "Epoch 77: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.8973 - val_loss: 0.4555 - val_accuracy: 0.8371\n",
      "Epoch 78/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2546 - accuracy: 0.8902\n",
      "Epoch 78: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8945 - val_loss: 0.4443 - val_accuracy: 0.8258\n",
      "Epoch 79/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2386 - accuracy: 0.9000\n",
      "Epoch 79: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.8931 - val_loss: 0.4508 - val_accuracy: 0.8483\n",
      "Epoch 80/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2395 - accuracy: 0.8976\n",
      "Epoch 80: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.8973 - val_loss: 0.4383 - val_accuracy: 0.8371\n",
      "Epoch 81/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2551 - accuracy: 0.8884\n",
      "Epoch 81: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2468 - accuracy: 0.8917 - val_loss: 0.4439 - val_accuracy: 0.8371\n",
      "Epoch 82/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2422 - accuracy: 0.8952\n",
      "Epoch 82: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9001 - val_loss: 0.4603 - val_accuracy: 0.8427\n",
      "Epoch 83/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2398 - accuracy: 0.8948\n",
      "Epoch 83: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.8917 - val_loss: 0.4441 - val_accuracy: 0.8258\n",
      "Epoch 84/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2398 - accuracy: 0.8953\n",
      "Epoch 84: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.8959 - val_loss: 0.4345 - val_accuracy: 0.8427\n",
      "Epoch 85/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2341 - accuracy: 0.9000\n",
      "Epoch 85: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8945 - val_loss: 0.4628 - val_accuracy: 0.8483\n",
      "Epoch 86/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2438 - accuracy: 0.9000\n",
      "Epoch 86: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8987 - val_loss: 0.4431 - val_accuracy: 0.8427\n",
      "Epoch 87/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2424 - accuracy: 0.9024\n",
      "Epoch 87: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.8959 - val_loss: 0.4496 - val_accuracy: 0.8427\n",
      "Epoch 88/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2328 - accuracy: 0.9023\n",
      "Epoch 88: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8973 - val_loss: 0.4457 - val_accuracy: 0.8371\n",
      "Epoch 89/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2704 - accuracy: 0.8725\n",
      "Epoch 89: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8987 - val_loss: 0.4592 - val_accuracy: 0.8483\n",
      "Epoch 90/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2316 - accuracy: 0.9034\n",
      "Epoch 90: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9015 - val_loss: 0.4546 - val_accuracy: 0.8315\n",
      "Epoch 91/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2555 - accuracy: 0.8833\n",
      "Epoch 91: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.8959 - val_loss: 0.4612 - val_accuracy: 0.8427\n",
      "Epoch 92/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2176 - accuracy: 0.9103\n",
      "Epoch 92: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9015 - val_loss: 0.4624 - val_accuracy: 0.8371\n",
      "Epoch 93/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2360 - accuracy: 0.9068\n",
      "Epoch 93: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.8931 - val_loss: 0.4482 - val_accuracy: 0.8371\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2478 - accuracy: 0.8929\n",
      "Epoch 94: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.8945 - val_loss: 0.4545 - val_accuracy: 0.8427\n",
      "Epoch 95/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2303 - accuracy: 0.8976\n",
      "Epoch 95: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.8903 - val_loss: 0.4616 - val_accuracy: 0.8483\n",
      "Epoch 96/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2568 - accuracy: 0.8791\n",
      "Epoch 96: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.8973 - val_loss: 0.4638 - val_accuracy: 0.8427\n",
      "Epoch 97/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2715 - accuracy: 0.8744\n",
      "Epoch 97: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.8875 - val_loss: 0.4619 - val_accuracy: 0.8371\n",
      "Epoch 98/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2620 - accuracy: 0.8810\n",
      "Epoch 98: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8987 - val_loss: 0.4714 - val_accuracy: 0.8315\n",
      "Epoch 99/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2592 - accuracy: 0.8892\n",
      "Epoch 99: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8945 - val_loss: 0.4522 - val_accuracy: 0.8371\n",
      "Epoch 100/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2473 - accuracy: 0.9000\n",
      "Epoch 100: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9001 - val_loss: 0.4501 - val_accuracy: 0.8427\n",
      "Data set number:  4\n",
      "Epoch 1/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.3520 - accuracy: 0.8683\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88764, saving model to prediction_titanic\\best_model_batch_4.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8692 - val_loss: 0.2952 - val_accuracy: 0.8876\n",
      "Epoch 2/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3149 - accuracy: 0.8800\n",
      "Epoch 2: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8748 - val_loss: 0.3030 - val_accuracy: 0.8820\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3198 - accuracy: 0.8643\n",
      "Epoch 3: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8692 - val_loss: 0.3065 - val_accuracy: 0.8876\n",
      "Epoch 4/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2945 - accuracy: 0.8857\n",
      "Epoch 4: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3044 - accuracy: 0.8748 - val_loss: 0.3149 - val_accuracy: 0.8876\n",
      "Epoch 5/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.3172 - accuracy: 0.8764\n",
      "Epoch 5: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3034 - accuracy: 0.8734 - val_loss: 0.3262 - val_accuracy: 0.8708\n",
      "Epoch 6/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2782 - accuracy: 0.8854\n",
      "Epoch 6: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8734 - val_loss: 0.3061 - val_accuracy: 0.8820\n",
      "Epoch 7/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.3002 - accuracy: 0.8700\n",
      "Epoch 7: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8748 - val_loss: 0.3077 - val_accuracy: 0.8820\n",
      "Epoch 8/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2921 - accuracy: 0.8716\n",
      "Epoch 8: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8692 - val_loss: 0.3157 - val_accuracy: 0.8820\n",
      "Epoch 9/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.3169 - accuracy: 0.8641\n",
      "Epoch 9: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.8790 - val_loss: 0.3208 - val_accuracy: 0.8876\n",
      "Epoch 10/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2754 - accuracy: 0.8829\n",
      "Epoch 10: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8734 - val_loss: 0.3244 - val_accuracy: 0.8652\n",
      "Epoch 11/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.3005 - accuracy: 0.8582\n",
      "Epoch 11: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8664 - val_loss: 0.3442 - val_accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2971 - accuracy: 0.8658\n",
      "Epoch 12: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8748 - val_loss: 0.3304 - val_accuracy: 0.8596\n",
      "Epoch 13/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2617 - accuracy: 0.8775\n",
      "Epoch 13: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.8734 - val_loss: 0.3231 - val_accuracy: 0.8708\n",
      "Epoch 14/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2668 - accuracy: 0.8929\n",
      "Epoch 14: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8861 - val_loss: 0.3338 - val_accuracy: 0.8820\n",
      "Epoch 15/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2746 - accuracy: 0.8732\n",
      "Epoch 15: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8790 - val_loss: 0.3430 - val_accuracy: 0.8596\n",
      "Epoch 16/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2966 - accuracy: 0.8738\n",
      "Epoch 16: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2768 - accuracy: 0.8847 - val_loss: 0.3338 - val_accuracy: 0.8652\n",
      "Epoch 17/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2608 - accuracy: 0.8786\n",
      "Epoch 17: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8776 - val_loss: 0.3391 - val_accuracy: 0.8764\n",
      "Epoch 18/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2761 - accuracy: 0.8780\n",
      "Epoch 18: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8776 - val_loss: 0.3334 - val_accuracy: 0.8539\n",
      "Epoch 19/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.8757\n",
      "Epoch 19: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2708 - accuracy: 0.8776 - val_loss: 0.3462 - val_accuracy: 0.8764\n",
      "Epoch 20/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2496 - accuracy: 0.8881\n",
      "Epoch 20: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8805 - val_loss: 0.3491 - val_accuracy: 0.8652\n",
      "Epoch 21/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2642 - accuracy: 0.8833\n",
      "Epoch 21: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8861 - val_loss: 0.3438 - val_accuracy: 0.8764\n",
      "Epoch 22/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2912 - accuracy: 0.8667\n",
      "Epoch 22: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8805 - val_loss: 0.3526 - val_accuracy: 0.8708\n",
      "Epoch 23/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2511 - accuracy: 0.8857\n",
      "Epoch 23: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8847 - val_loss: 0.3506 - val_accuracy: 0.8820\n",
      "Epoch 24/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2401 - accuracy: 0.8921\n",
      "Epoch 24: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8833 - val_loss: 0.3527 - val_accuracy: 0.8539\n",
      "Epoch 25/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2652 - accuracy: 0.8875\n",
      "Epoch 25: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8819 - val_loss: 0.3504 - val_accuracy: 0.8315\n",
      "Epoch 26/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2575 - accuracy: 0.8786\n",
      "Epoch 26: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8706 - val_loss: 0.3747 - val_accuracy: 0.8315\n",
      "Epoch 27/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2576 - accuracy: 0.8902\n",
      "Epoch 27: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8833 - val_loss: 0.3551 - val_accuracy: 0.8596\n",
      "Epoch 28/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.2856 - accuracy: 0.8519\n",
      "Epoch 28: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8720 - val_loss: 0.3558 - val_accuracy: 0.8483\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.8819\n",
      "Epoch 29: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8819 - val_loss: 0.3695 - val_accuracy: 0.8596\n",
      "Epoch 30/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2629 - accuracy: 0.8737\n",
      "Epoch 30: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8805 - val_loss: 0.3505 - val_accuracy: 0.8708\n",
      "Epoch 31/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2225 - accuracy: 0.9024\n",
      "Epoch 31: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8790 - val_loss: 0.3602 - val_accuracy: 0.8539\n",
      "Epoch 32/100\n",
      "54/72 [=====================>........] - ETA: 0s - loss: 0.2753 - accuracy: 0.8796\n",
      "Epoch 32: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8847 - val_loss: 0.3507 - val_accuracy: 0.8596\n",
      "Epoch 33/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2629 - accuracy: 0.8910\n",
      "Epoch 33: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8931 - val_loss: 0.3614 - val_accuracy: 0.8483\n",
      "Epoch 34/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2603 - accuracy: 0.8866\n",
      "Epoch 34: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2622 - accuracy: 0.8861 - val_loss: 0.3717 - val_accuracy: 0.8427\n",
      "Epoch 35/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2484 - accuracy: 0.9018\n",
      "Epoch 35: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.8903 - val_loss: 0.3573 - val_accuracy: 0.8596\n",
      "Epoch 36/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2872 - accuracy: 0.8738\n",
      "Epoch 36: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8903 - val_loss: 0.3631 - val_accuracy: 0.8483\n",
      "Epoch 37/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2436 - accuracy: 0.8860\n",
      "Epoch 37: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2597 - accuracy: 0.8805 - val_loss: 0.3640 - val_accuracy: 0.8539\n",
      "Epoch 38/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2666 - accuracy: 0.8738\n",
      "Epoch 38: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2566 - accuracy: 0.8847 - val_loss: 0.3688 - val_accuracy: 0.8539\n",
      "Epoch 39/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2692 - accuracy: 0.8905\n",
      "Epoch 39: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8931 - val_loss: 0.3635 - val_accuracy: 0.8596\n",
      "Epoch 40/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2722 - accuracy: 0.8881\n",
      "Epoch 40: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.8805 - val_loss: 0.3618 - val_accuracy: 0.8820\n",
      "Epoch 41/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2632 - accuracy: 0.8786\n",
      "Epoch 41: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.8861 - val_loss: 0.3709 - val_accuracy: 0.8596\n",
      "Epoch 42/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2658 - accuracy: 0.8873\n",
      "Epoch 42: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8861 - val_loss: 0.3657 - val_accuracy: 0.8539\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2528 - accuracy: 0.8929\n",
      "Epoch 43: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.8889 - val_loss: 0.3633 - val_accuracy: 0.8427\n",
      "Epoch 44/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2629 - accuracy: 0.8881\n",
      "Epoch 44: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2568 - accuracy: 0.8931 - val_loss: 0.3773 - val_accuracy: 0.8427\n",
      "Epoch 45/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2444 - accuracy: 0.9047\n",
      "Epoch 45: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2582 - accuracy: 0.8959 - val_loss: 0.3758 - val_accuracy: 0.8539\n",
      "Epoch 46/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2489 - accuracy: 0.8951\n",
      "Epoch 46: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.8819 - val_loss: 0.3720 - val_accuracy: 0.8315\n",
      "Epoch 47/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2697 - accuracy: 0.8833\n",
      "Epoch 47: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.8875 - val_loss: 0.3773 - val_accuracy: 0.8539\n",
      "Epoch 48/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2484 - accuracy: 0.9000\n",
      "Epoch 48: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8903 - val_loss: 0.3790 - val_accuracy: 0.8427\n",
      "Epoch 49/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2473 - accuracy: 0.8905\n",
      "Epoch 49: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.8833 - val_loss: 0.3710 - val_accuracy: 0.8539\n",
      "Epoch 50/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.8771\n",
      "Epoch 50: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.8776 - val_loss: 0.3664 - val_accuracy: 0.8483\n",
      "Epoch 51/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2487 - accuracy: 0.8857\n",
      "Epoch 51: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8861 - val_loss: 0.3729 - val_accuracy: 0.8652\n",
      "Epoch 52/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2625 - accuracy: 0.8895\n",
      "Epoch 52: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2591 - accuracy: 0.8889 - val_loss: 0.3761 - val_accuracy: 0.8652\n",
      "Epoch 53/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2527 - accuracy: 0.8925\n",
      "Epoch 53: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8903 - val_loss: 0.3737 - val_accuracy: 0.8539\n",
      "Epoch 54/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2044 - accuracy: 0.9175\n",
      "Epoch 54: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.8819 - val_loss: 0.3832 - val_accuracy: 0.8315\n",
      "Epoch 55/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2597 - accuracy: 0.8857\n",
      "Epoch 55: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.8917 - val_loss: 0.3708 - val_accuracy: 0.8708\n",
      "Epoch 56/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2373 - accuracy: 0.9071\n",
      "Epoch 56: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8875 - val_loss: 0.3797 - val_accuracy: 0.8539\n",
      "Epoch 57/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2561 - accuracy: 0.8976\n",
      "Epoch 57: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.8889 - val_loss: 0.3693 - val_accuracy: 0.8596\n",
      "Epoch 58/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2496 - accuracy: 0.8897\n",
      "Epoch 58: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8917 - val_loss: 0.3762 - val_accuracy: 0.8371\n",
      "Epoch 59/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2558 - accuracy: 0.8805\n",
      "Epoch 59: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8861 - val_loss: 0.3801 - val_accuracy: 0.8596\n",
      "Epoch 60/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2672 - accuracy: 0.8767\n",
      "Epoch 60: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.8819 - val_loss: 0.3715 - val_accuracy: 0.8483\n",
      "Epoch 61/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2693 - accuracy: 0.8707\n",
      "Epoch 61: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.8819 - val_loss: 0.3771 - val_accuracy: 0.8483\n",
      "Epoch 62/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.2455 - accuracy: 0.8846\n",
      "Epoch 62: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.8861 - val_loss: 0.3847 - val_accuracy: 0.8539\n",
      "Epoch 63/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2405 - accuracy: 0.8929\n",
      "Epoch 63: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.8945 - val_loss: 0.3752 - val_accuracy: 0.8315\n",
      "Epoch 64/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2693 - accuracy: 0.8854\n",
      "Epoch 64: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8889 - val_loss: 0.3641 - val_accuracy: 0.8483\n",
      "Epoch 65/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2725 - accuracy: 0.8763\n",
      "Epoch 65: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.8889 - val_loss: 0.3849 - val_accuracy: 0.8596\n",
      "Epoch 66/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2279 - accuracy: 0.9000\n",
      "Epoch 66: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.8875 - val_loss: 0.3816 - val_accuracy: 0.8708\n",
      "Epoch 67/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2211 - accuracy: 0.9023\n",
      "Epoch 67: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.8889 - val_loss: 0.3942 - val_accuracy: 0.8315\n",
      "Epoch 68/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.2567 - accuracy: 0.8904\n",
      "Epoch 68: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.8889 - val_loss: 0.3846 - val_accuracy: 0.8427\n",
      "Epoch 69/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2641 - accuracy: 0.8690\n",
      "Epoch 69: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.8847 - val_loss: 0.3790 - val_accuracy: 0.8371\n",
      "Epoch 70/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2392 - accuracy: 0.8977\n",
      "Epoch 70: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.8875 - val_loss: 0.3959 - val_accuracy: 0.8371\n",
      "Epoch 71/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2336 - accuracy: 0.8833\n",
      "Epoch 71: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8861 - val_loss: 0.3942 - val_accuracy: 0.8483\n",
      "Epoch 72/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2607 - accuracy: 0.8881\n",
      "Epoch 72: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8861 - val_loss: 0.3846 - val_accuracy: 0.8596\n",
      "Epoch 73/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2779 - accuracy: 0.8756\n",
      "Epoch 73: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.8945 - val_loss: 0.3766 - val_accuracy: 0.8371\n",
      "Epoch 74/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2512 - accuracy: 0.8837\n",
      "Epoch 74: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.8903 - val_loss: 0.3843 - val_accuracy: 0.8427\n",
      "Epoch 75/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2665 - accuracy: 0.8800\n",
      "Epoch 75: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8903 - val_loss: 0.3830 - val_accuracy: 0.8539\n",
      "Epoch 76/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2733 - accuracy: 0.8829\n",
      "Epoch 76: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.8945 - val_loss: 0.3838 - val_accuracy: 0.8764\n",
      "Epoch 77/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2738 - accuracy: 0.8643\n",
      "Epoch 77: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.8805 - val_loss: 0.3786 - val_accuracy: 0.8483\n",
      "Epoch 78/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2615 - accuracy: 0.8833\n",
      "Epoch 78: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.8917 - val_loss: 0.3892 - val_accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2502 - accuracy: 0.9000\n",
      "Epoch 79: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8931 - val_loss: 0.3867 - val_accuracy: 0.8202\n",
      "Epoch 80/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2514 - accuracy: 0.8829\n",
      "Epoch 80: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8847 - val_loss: 0.3917 - val_accuracy: 0.8539\n",
      "Epoch 81/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2490 - accuracy: 0.9000\n",
      "Epoch 81: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.9001 - val_loss: 0.4002 - val_accuracy: 0.8371\n",
      "Epoch 82/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2724 - accuracy: 0.8732\n",
      "Epoch 82: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.8931 - val_loss: 0.3957 - val_accuracy: 0.8315\n",
      "Epoch 83/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2391 - accuracy: 0.8932\n",
      "Epoch 83: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.8931 - val_loss: 0.3826 - val_accuracy: 0.8483\n",
      "Epoch 84/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2483 - accuracy: 0.8833\n",
      "Epoch 84: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.8861 - val_loss: 0.3897 - val_accuracy: 0.8315\n",
      "Epoch 85/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2422 - accuracy: 0.8953\n",
      "Epoch 85: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.8959 - val_loss: 0.3916 - val_accuracy: 0.8202\n",
      "Epoch 86/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2297 - accuracy: 0.8975\n",
      "Epoch 86: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8945 - val_loss: 0.3902 - val_accuracy: 0.8427\n",
      "Epoch 87/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2551 - accuracy: 0.8854\n",
      "Epoch 87: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.8903 - val_loss: 0.3827 - val_accuracy: 0.8483\n",
      "Epoch 88/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2481 - accuracy: 0.8930\n",
      "Epoch 88: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8833 - val_loss: 0.4023 - val_accuracy: 0.8315\n",
      "Epoch 89/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.2414 - accuracy: 0.9032\n",
      "Epoch 89: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.8945 - val_loss: 0.3939 - val_accuracy: 0.8315\n",
      "Epoch 90/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2344 - accuracy: 0.9024\n",
      "Epoch 90: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.8987 - val_loss: 0.3965 - val_accuracy: 0.8371\n",
      "Epoch 91/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2236 - accuracy: 0.9077\n",
      "Epoch 91: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8917 - val_loss: 0.3966 - val_accuracy: 0.8258\n",
      "Epoch 92/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2552 - accuracy: 0.8930\n",
      "Epoch 92: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.8973 - val_loss: 0.3927 - val_accuracy: 0.8483\n",
      "Epoch 93/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2089 - accuracy: 0.9095\n",
      "Epoch 93: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8917 - val_loss: 0.3958 - val_accuracy: 0.8483\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.8819\n",
      "Epoch 94: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2463 - accuracy: 0.8819 - val_loss: 0.3982 - val_accuracy: 0.8371\n",
      "Epoch 95/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2475 - accuracy: 0.8952\n",
      "Epoch 95: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8945 - val_loss: 0.3918 - val_accuracy: 0.8483\n",
      "Epoch 96/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2725 - accuracy: 0.8707\n",
      "Epoch 96: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.8903 - val_loss: 0.4095 - val_accuracy: 0.8539\n",
      "Epoch 97/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2486 - accuracy: 0.8821\n",
      "Epoch 97: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2448 - accuracy: 0.8847 - val_loss: 0.3999 - val_accuracy: 0.8371\n",
      "Epoch 98/100\n",
      "61/72 [========================>.....] - ETA: 0s - loss: 0.2573 - accuracy: 0.8770\n",
      "Epoch 98: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8847 - val_loss: 0.3974 - val_accuracy: 0.8371\n",
      "Epoch 99/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2562 - accuracy: 0.9000\n",
      "Epoch 99: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9015 - val_loss: 0.4001 - val_accuracy: 0.8483\n",
      "Epoch 100/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2507 - accuracy: 0.8878\n",
      "Epoch 100: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.8861 - val_loss: 0.3964 - val_accuracy: 0.8202\n",
      "Data set number:  5\n",
      "Epoch 1/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3127 - accuracy: 0.8548\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88764, saving model to prediction_titanic\\best_model_batch_5.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.8678 - val_loss: 0.2721 - val_accuracy: 0.8876\n",
      "Epoch 2/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.2975 - accuracy: 0.8727\n",
      "Epoch 2: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8790 - val_loss: 0.2679 - val_accuracy: 0.8876\n",
      "Epoch 3/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2852 - accuracy: 0.8698\n",
      "Epoch 3: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8790 - val_loss: 0.2741 - val_accuracy: 0.8820\n",
      "Epoch 4/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2870 - accuracy: 0.8881\n",
      "Epoch 4: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8889 - val_loss: 0.2696 - val_accuracy: 0.8820\n",
      "Epoch 5/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.2567 - accuracy: 0.9038\n",
      "Epoch 5: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8903 - val_loss: 0.2725 - val_accuracy: 0.8820\n",
      "Epoch 6/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2676 - accuracy: 0.8804\n",
      "Epoch 6: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.8833 - val_loss: 0.2841 - val_accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2780 - accuracy: 0.8821\n",
      "Epoch 7: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8833 - val_loss: 0.2894 - val_accuracy: 0.8764\n",
      "Epoch 8/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2715 - accuracy: 0.8884\n",
      "Epoch 8: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.8875 - val_loss: 0.2801 - val_accuracy: 0.8652\n",
      "Epoch 9/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2660 - accuracy: 0.8940\n",
      "Epoch 9: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.8903 - val_loss: 0.2851 - val_accuracy: 0.8652\n",
      "Epoch 10/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2654 - accuracy: 0.9023\n",
      "Epoch 10: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8931 - val_loss: 0.2830 - val_accuracy: 0.8708\n",
      "Epoch 11/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2648 - accuracy: 0.8952\n",
      "Epoch 11: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8917 - val_loss: 0.2859 - val_accuracy: 0.8427\n",
      "Epoch 12/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2763 - accuracy: 0.8780\n",
      "Epoch 12: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.8833 - val_loss: 0.3023 - val_accuracy: 0.8539\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.8889\n",
      "Epoch 13: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8889 - val_loss: 0.2901 - val_accuracy: 0.8427\n",
      "Epoch 14/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2771 - accuracy: 0.8805\n",
      "Epoch 14: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8903 - val_loss: 0.2993 - val_accuracy: 0.8539\n",
      "Epoch 15/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2575 - accuracy: 0.8952\n",
      "Epoch 15: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.8931 - val_loss: 0.2900 - val_accuracy: 0.8652\n",
      "Epoch 16/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2746 - accuracy: 0.8837\n",
      "Epoch 16: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2595 - accuracy: 0.8917 - val_loss: 0.2900 - val_accuracy: 0.8652\n",
      "Epoch 17/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2389 - accuracy: 0.9024\n",
      "Epoch 17: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.8875 - val_loss: 0.3054 - val_accuracy: 0.8315\n",
      "Epoch 18/100\n",
      "50/72 [===================>..........] - ETA: 0s - loss: 0.2498 - accuracy: 0.8980\n",
      "Epoch 18: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.8903 - val_loss: 0.3108 - val_accuracy: 0.8315\n",
      "Epoch 19/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2613 - accuracy: 0.8953\n",
      "Epoch 19: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8917 - val_loss: 0.2999 - val_accuracy: 0.8427\n",
      "Epoch 20/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2617 - accuracy: 0.8872\n",
      "Epoch 20: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.8875 - val_loss: 0.3171 - val_accuracy: 0.8427\n",
      "Epoch 21/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2652 - accuracy: 0.8929\n",
      "Epoch 21: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8931 - val_loss: 0.2965 - val_accuracy: 0.8539\n",
      "Epoch 22/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2833 - accuracy: 0.8854\n",
      "Epoch 22: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.8917 - val_loss: 0.3065 - val_accuracy: 0.8427\n",
      "Epoch 23/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2332 - accuracy: 0.9023\n",
      "Epoch 23: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8959 - val_loss: 0.3217 - val_accuracy: 0.8202\n",
      "Epoch 24/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2614 - accuracy: 0.8952\n",
      "Epoch 24: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.8931 - val_loss: 0.3140 - val_accuracy: 0.8371\n",
      "Epoch 25/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2568 - accuracy: 0.8929\n",
      "Epoch 25: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.8903 - val_loss: 0.3071 - val_accuracy: 0.8371\n",
      "Epoch 26/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2443 - accuracy: 0.8898\n",
      "Epoch 26: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8903 - val_loss: 0.3022 - val_accuracy: 0.8539\n",
      "Epoch 27/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2701 - accuracy: 0.8881\n",
      "Epoch 27: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.8917 - val_loss: 0.3194 - val_accuracy: 0.8371\n",
      "Epoch 28/100\n",
      "32/72 [============>.................] - ETA: 0s - loss: 0.2319 - accuracy: 0.9094\n",
      "Epoch 28: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.8945 - val_loss: 0.3113 - val_accuracy: 0.8427\n",
      "Epoch 29/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2590 - accuracy: 0.8952\n",
      "Epoch 29: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8917 - val_loss: 0.3074 - val_accuracy: 0.8539\n",
      "Epoch 30/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2478 - accuracy: 0.9023\n",
      "Epoch 30: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.8917 - val_loss: 0.3147 - val_accuracy: 0.8483\n",
      "Epoch 31/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2421 - accuracy: 0.8951\n",
      "Epoch 31: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2547 - accuracy: 0.8903 - val_loss: 0.3141 - val_accuracy: 0.8539\n",
      "Epoch 32/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2822 - accuracy: 0.8692\n",
      "Epoch 32: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8847 - val_loss: 0.3187 - val_accuracy: 0.8315\n",
      "Epoch 33/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2639 - accuracy: 0.8829\n",
      "Epoch 33: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2582 - accuracy: 0.8847 - val_loss: 0.3167 - val_accuracy: 0.8427\n",
      "Epoch 34/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2483 - accuracy: 0.8951\n",
      "Epoch 34: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.8889 - val_loss: 0.3197 - val_accuracy: 0.8427\n",
      "Epoch 35/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2500 - accuracy: 0.8952\n",
      "Epoch 35: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.8945 - val_loss: 0.3264 - val_accuracy: 0.8483\n",
      "Epoch 36/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2607 - accuracy: 0.8791\n",
      "Epoch 36: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2543 - accuracy: 0.8861 - val_loss: 0.3147 - val_accuracy: 0.8539\n",
      "Epoch 37/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2542 - accuracy: 0.8875\n",
      "Epoch 37: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8889 - val_loss: 0.3174 - val_accuracy: 0.8483\n",
      "Epoch 38/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2532 - accuracy: 0.8791\n",
      "Epoch 38: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.8889 - val_loss: 0.3367 - val_accuracy: 0.8371\n",
      "Epoch 39/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2470 - accuracy: 0.8951\n",
      "Epoch 39: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8931 - val_loss: 0.3328 - val_accuracy: 0.8427\n",
      "Epoch 40/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2574 - accuracy: 0.8930\n",
      "Epoch 40: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.8959 - val_loss: 0.3242 - val_accuracy: 0.8539\n",
      "Epoch 41/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2874 - accuracy: 0.8634\n",
      "Epoch 41: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.8889 - val_loss: 0.3312 - val_accuracy: 0.8539\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2479 - accuracy: 0.8976\n",
      "Epoch 42: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.8959 - val_loss: 0.3417 - val_accuracy: 0.8315\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2246 - accuracy: 0.9190\n",
      "Epoch 43: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9030 - val_loss: 0.3234 - val_accuracy: 0.8652\n",
      "Epoch 44/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2503 - accuracy: 0.8878\n",
      "Epoch 44: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8903 - val_loss: 0.3475 - val_accuracy: 0.8315\n",
      "Epoch 45/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2405 - accuracy: 0.9070\n",
      "Epoch 45: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8959 - val_loss: 0.3415 - val_accuracy: 0.8371\n",
      "Epoch 46/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2589 - accuracy: 0.8897\n",
      "Epoch 46: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2575 - accuracy: 0.8861 - val_loss: 0.3374 - val_accuracy: 0.8596\n",
      "Epoch 47/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2214 - accuracy: 0.9079\n",
      "Epoch 47: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.8889 - val_loss: 0.3356 - val_accuracy: 0.8483\n",
      "Epoch 48/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2369 - accuracy: 0.8976\n",
      "Epoch 48: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2463 - accuracy: 0.8931 - val_loss: 0.3356 - val_accuracy: 0.8539\n",
      "Epoch 49/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2611 - accuracy: 0.8786\n",
      "Epoch 49: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.8847 - val_loss: 0.3432 - val_accuracy: 0.8427\n",
      "Epoch 50/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2172 - accuracy: 0.9071\n",
      "Epoch 50: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.8945 - val_loss: 0.3449 - val_accuracy: 0.8371\n",
      "Epoch 51/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2498 - accuracy: 0.8902\n",
      "Epoch 51: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.8875 - val_loss: 0.3531 - val_accuracy: 0.8315\n",
      "Epoch 52/100\n",
      "61/72 [========================>.....] - ETA: 0s - loss: 0.2449 - accuracy: 0.9016\n",
      "Epoch 52: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8945 - val_loss: 0.3494 - val_accuracy: 0.8427\n",
      "Epoch 53/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2539 - accuracy: 0.8952\n",
      "Epoch 53: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.8973 - val_loss: 0.3530 - val_accuracy: 0.8371\n",
      "Epoch 54/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2484 - accuracy: 0.8929\n",
      "Epoch 54: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.8959 - val_loss: 0.3496 - val_accuracy: 0.8539\n",
      "Epoch 55/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2673 - accuracy: 0.8850\n",
      "Epoch 55: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2451 - accuracy: 0.8959 - val_loss: 0.3489 - val_accuracy: 0.8539\n",
      "Epoch 56/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2639 - accuracy: 0.8881\n",
      "Epoch 56: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.8903 - val_loss: 0.3593 - val_accuracy: 0.8315\n",
      "Epoch 57/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2626 - accuracy: 0.8829\n",
      "Epoch 57: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.8847 - val_loss: 0.3701 - val_accuracy: 0.8258\n",
      "Epoch 58/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2595 - accuracy: 0.8953\n",
      "Epoch 58: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9001 - val_loss: 0.3538 - val_accuracy: 0.8539\n",
      "Epoch 59/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2374 - accuracy: 0.9047\n",
      "Epoch 59: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.8931 - val_loss: 0.3648 - val_accuracy: 0.8315\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.9015\n",
      "Epoch 60: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.9015 - val_loss: 0.3653 - val_accuracy: 0.8596\n",
      "Epoch 61/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2287 - accuracy: 0.9024\n",
      "Epoch 61: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.8917 - val_loss: 0.3665 - val_accuracy: 0.8427\n",
      "Epoch 62/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2882 - accuracy: 0.8698\n",
      "Epoch 62: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.8931 - val_loss: 0.3608 - val_accuracy: 0.8483\n",
      "Epoch 63/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2736 - accuracy: 0.8854\n",
      "Epoch 63: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.8931 - val_loss: 0.3704 - val_accuracy: 0.8315\n",
      "Epoch 64/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2592 - accuracy: 0.8929\n",
      "Epoch 64: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8959 - val_loss: 0.3621 - val_accuracy: 0.8483\n",
      "Epoch 65/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2644 - accuracy: 0.8952\n",
      "Epoch 65: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9001 - val_loss: 0.3810 - val_accuracy: 0.8596\n",
      "Epoch 66/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2210 - accuracy: 0.9150\n",
      "Epoch 66: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.9001 - val_loss: 0.3885 - val_accuracy: 0.8371\n",
      "Epoch 67/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2620 - accuracy: 0.8952\n",
      "Epoch 67: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.9030 - val_loss: 0.3679 - val_accuracy: 0.8483\n",
      "Epoch 68/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2398 - accuracy: 0.8921\n",
      "Epoch 68: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.8917 - val_loss: 0.3728 - val_accuracy: 0.8427\n",
      "Epoch 69/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2431 - accuracy: 0.8971\n",
      "Epoch 69: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.8973 - val_loss: 0.3838 - val_accuracy: 0.8427\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.8945\n",
      "Epoch 70: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.8945 - val_loss: 0.3828 - val_accuracy: 0.8427\n",
      "Epoch 71/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2462 - accuracy: 0.8953\n",
      "Epoch 71: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.8889 - val_loss: 0.3948 - val_accuracy: 0.8258\n",
      "Epoch 72/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2304 - accuracy: 0.9146\n",
      "Epoch 72: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.9015 - val_loss: 0.3768 - val_accuracy: 0.8539\n",
      "Epoch 73/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2650 - accuracy: 0.8789\n",
      "Epoch 73: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.8945 - val_loss: 0.3842 - val_accuracy: 0.8258\n",
      "Epoch 74/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2284 - accuracy: 0.9049\n",
      "Epoch 74: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.8959 - val_loss: 0.3825 - val_accuracy: 0.8371\n",
      "Epoch 75/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2502 - accuracy: 0.8977\n",
      "Epoch 75: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.8987 - val_loss: 0.3884 - val_accuracy: 0.8315\n",
      "Epoch 76/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2137 - accuracy: 0.9095\n",
      "Epoch 76: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.8959 - val_loss: 0.3910 - val_accuracy: 0.8371\n",
      "Epoch 77/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2418 - accuracy: 0.8902\n",
      "Epoch 77: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.8945 - val_loss: 0.3863 - val_accuracy: 0.8371\n",
      "Epoch 78/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2534 - accuracy: 0.8912\n",
      "Epoch 78: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8903 - val_loss: 0.3886 - val_accuracy: 0.8202\n",
      "Epoch 79/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2268 - accuracy: 0.9050\n",
      "Epoch 79: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2337 - accuracy: 0.9030 - val_loss: 0.3814 - val_accuracy: 0.8483\n",
      "Epoch 80/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.8944\n",
      "Epoch 80: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.8945 - val_loss: 0.3812 - val_accuracy: 0.8371\n",
      "Epoch 81/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2337 - accuracy: 0.8964\n",
      "Epoch 81: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.8945 - val_loss: 0.3847 - val_accuracy: 0.8371\n",
      "Epoch 82/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.8986\n",
      "Epoch 82: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8987 - val_loss: 0.3879 - val_accuracy: 0.8371\n",
      "Epoch 83/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2427 - accuracy: 0.8955\n",
      "Epoch 83: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2387 - accuracy: 0.8959 - val_loss: 0.3941 - val_accuracy: 0.8315\n",
      "Epoch 84/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.2433 - accuracy: 0.8894\n",
      "Epoch 84: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.8917 - val_loss: 0.3920 - val_accuracy: 0.8315\n",
      "Epoch 85/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9057\n",
      "Epoch 85: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2419 - accuracy: 0.9030 - val_loss: 0.3860 - val_accuracy: 0.8371\n",
      "Epoch 86/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2471 - accuracy: 0.9000\n",
      "Epoch 86: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.8959 - val_loss: 0.3884 - val_accuracy: 0.8427\n",
      "Epoch 87/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2321 - accuracy: 0.9190\n",
      "Epoch 87: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.8973 - val_loss: 0.3877 - val_accuracy: 0.8315\n",
      "Epoch 88/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2392 - accuracy: 0.8833\n",
      "Epoch 88: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2445 - accuracy: 0.8875 - val_loss: 0.3804 - val_accuracy: 0.8371\n",
      "Epoch 89/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2148 - accuracy: 0.8976\n",
      "Epoch 89: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8959 - val_loss: 0.4121 - val_accuracy: 0.8146\n",
      "Epoch 90/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2383 - accuracy: 0.8976\n",
      "Epoch 90: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8973 - val_loss: 0.3812 - val_accuracy: 0.8371\n",
      "Epoch 91/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.3002 - accuracy: 0.8714\n",
      "Epoch 91: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8819 - val_loss: 0.3791 - val_accuracy: 0.8427\n",
      "Epoch 92/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2479 - accuracy: 0.9048\n",
      "Epoch 92: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8987 - val_loss: 0.3922 - val_accuracy: 0.8539\n",
      "Epoch 93/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2045 - accuracy: 0.9250\n",
      "Epoch 93: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.8973 - val_loss: 0.4059 - val_accuracy: 0.8371\n",
      "Epoch 94/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2398 - accuracy: 0.8929\n",
      "Epoch 94: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.8945 - val_loss: 0.3953 - val_accuracy: 0.8371\n",
      "Epoch 95/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2420 - accuracy: 0.9171\n",
      "Epoch 95: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9030 - val_loss: 0.3886 - val_accuracy: 0.8427\n",
      "Epoch 96/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2245 - accuracy: 0.9070\n",
      "Epoch 96: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.8973 - val_loss: 0.4140 - val_accuracy: 0.8202\n",
      "Epoch 97/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2170 - accuracy: 0.9116\n",
      "Epoch 97: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2378 - accuracy: 0.8931 - val_loss: 0.3998 - val_accuracy: 0.8427\n",
      "Epoch 98/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2325 - accuracy: 0.8952\n",
      "Epoch 98: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.8959 - val_loss: 0.4032 - val_accuracy: 0.8427\n",
      "Epoch 99/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1994 - accuracy: 0.9349\n",
      "Epoch 99: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.9072 - val_loss: 0.4086 - val_accuracy: 0.8371\n",
      "Epoch 100/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2379 - accuracy: 0.8953\n",
      "Epoch 100: val_accuracy did not improve from 0.88764\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9001 - val_loss: 0.4075 - val_accuracy: 0.8427\n",
      "Data set number:  6\n",
      "Epoch 1/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2612 - accuracy: 0.8976\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86517, saving model to prediction_titanic\\best_model_batch_6.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8847 - val_loss: 0.3339 - val_accuracy: 0.8652\n",
      "Epoch 2/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2993 - accuracy: 0.8804\n",
      "Epoch 2: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8917 - val_loss: 0.3453 - val_accuracy: 0.8539\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2660 - accuracy: 0.8952\n",
      "Epoch 3: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8917 - val_loss: 0.3505 - val_accuracy: 0.8371\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.8945\n",
      "Epoch 4: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.8945 - val_loss: 0.3526 - val_accuracy: 0.8371\n",
      "Epoch 5/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2648 - accuracy: 0.8814\n",
      "Epoch 5: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8847 - val_loss: 0.3686 - val_accuracy: 0.8315\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.8917\n",
      "Epoch 6: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.8917 - val_loss: 0.3595 - val_accuracy: 0.8315\n",
      "Epoch 7/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2047 - accuracy: 0.9154\n",
      "Epoch 7: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8917 - val_loss: 0.3562 - val_accuracy: 0.8427\n",
      "Epoch 8/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2575 - accuracy: 0.8884\n",
      "Epoch 8: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.8959 - val_loss: 0.3737 - val_accuracy: 0.8315\n",
      "Epoch 9/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2242 - accuracy: 0.9000\n",
      "Epoch 9: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.8917 - val_loss: 0.3854 - val_accuracy: 0.8202\n",
      "Epoch 10/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2522 - accuracy: 0.8942\n",
      "Epoch 10: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.8959 - val_loss: 0.3715 - val_accuracy: 0.8371\n",
      "Epoch 11/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2359 - accuracy: 0.8857\n",
      "Epoch 11: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8903 - val_loss: 0.3866 - val_accuracy: 0.8315\n",
      "Epoch 12/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2220 - accuracy: 0.9122\n",
      "Epoch 12: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9044 - val_loss: 0.3773 - val_accuracy: 0.8315\n",
      "Epoch 13/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2349 - accuracy: 0.9071\n",
      "Epoch 13: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2367 - accuracy: 0.9001 - val_loss: 0.4129 - val_accuracy: 0.8090\n",
      "Epoch 14/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2408 - accuracy: 0.9026\n",
      "Epoch 14: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9030 - val_loss: 0.3745 - val_accuracy: 0.8315\n",
      "Epoch 15/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2134 - accuracy: 0.9048\n",
      "Epoch 15: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8987 - val_loss: 0.3898 - val_accuracy: 0.8146\n",
      "Epoch 16/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2427 - accuracy: 0.9000\n",
      "Epoch 16: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2387 - accuracy: 0.8945 - val_loss: 0.4035 - val_accuracy: 0.8202\n",
      "Epoch 17/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2174 - accuracy: 0.9195\n",
      "Epoch 17: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.9001 - val_loss: 0.3839 - val_accuracy: 0.8371\n",
      "Epoch 18/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2659 - accuracy: 0.8738\n",
      "Epoch 18: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.8987 - val_loss: 0.3908 - val_accuracy: 0.8258\n",
      "Epoch 19/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9043\n",
      "Epoch 19: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9044 - val_loss: 0.3939 - val_accuracy: 0.8146\n",
      "Epoch 20/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2484 - accuracy: 0.8925\n",
      "Epoch 20: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9030 - val_loss: 0.3932 - val_accuracy: 0.8258\n",
      "Epoch 21/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2240 - accuracy: 0.8977\n",
      "Epoch 21: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2316 - accuracy: 0.9001 - val_loss: 0.3926 - val_accuracy: 0.8483\n",
      "Epoch 22/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2221 - accuracy: 0.9024\n",
      "Epoch 22: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.8987 - val_loss: 0.4079 - val_accuracy: 0.8146\n",
      "Epoch 23/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2143 - accuracy: 0.9122\n",
      "Epoch 23: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9030 - val_loss: 0.4009 - val_accuracy: 0.8090\n",
      "Epoch 24/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2364 - accuracy: 0.8929\n",
      "Epoch 24: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9015 - val_loss: 0.3973 - val_accuracy: 0.8315\n",
      "Epoch 25/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2334 - accuracy: 0.9070\n",
      "Epoch 25: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.9072 - val_loss: 0.4015 - val_accuracy: 0.8090\n",
      "Epoch 26/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2252 - accuracy: 0.9171\n",
      "Epoch 26: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9114 - val_loss: 0.4031 - val_accuracy: 0.8258\n",
      "Epoch 27/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.2260 - accuracy: 0.9047\n",
      "Epoch 27: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.8987 - val_loss: 0.4034 - val_accuracy: 0.8315\n",
      "Epoch 28/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2400 - accuracy: 0.8925\n",
      "Epoch 28: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9058 - val_loss: 0.4042 - val_accuracy: 0.8202\n",
      "Epoch 29/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2154 - accuracy: 0.9143\n",
      "Epoch 29: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9100 - val_loss: 0.4073 - val_accuracy: 0.8371\n",
      "Epoch 30/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2272 - accuracy: 0.9171\n",
      "Epoch 30: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2288 - accuracy: 0.9128 - val_loss: 0.4076 - val_accuracy: 0.8371\n",
      "Epoch 31/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2383 - accuracy: 0.8976\n",
      "Epoch 31: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9015 - val_loss: 0.4094 - val_accuracy: 0.8146\n",
      "Epoch 32/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2074 - accuracy: 0.9256\n",
      "Epoch 32: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9058 - val_loss: 0.4086 - val_accuracy: 0.8258\n",
      "Epoch 33/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2210 - accuracy: 0.9132\n",
      "Epoch 33: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2227 - accuracy: 0.9128 - val_loss: 0.4304 - val_accuracy: 0.8090\n",
      "Epoch 34/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2218 - accuracy: 0.9233\n",
      "Epoch 34: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9156 - val_loss: 0.4149 - val_accuracy: 0.8034\n",
      "Epoch 35/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2066 - accuracy: 0.9154\n",
      "Epoch 35: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2213 - accuracy: 0.9086 - val_loss: 0.4219 - val_accuracy: 0.8090\n",
      "Epoch 36/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2204 - accuracy: 0.9095\n",
      "Epoch 36: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9142 - val_loss: 0.4295 - val_accuracy: 0.8090\n",
      "Epoch 37/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2329 - accuracy: 0.8975\n",
      "Epoch 37: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9015 - val_loss: 0.4213 - val_accuracy: 0.8258\n",
      "Epoch 38/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2163 - accuracy: 0.9186\n",
      "Epoch 38: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9100 - val_loss: 0.4196 - val_accuracy: 0.8090\n",
      "Epoch 39/100\n",
      "53/72 [=====================>........] - ETA: 0s - loss: 0.2253 - accuracy: 0.9132\n",
      "Epoch 39: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9128 - val_loss: 0.4299 - val_accuracy: 0.8146\n",
      "Epoch 40/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1987 - accuracy: 0.9268\n",
      "Epoch 40: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.9114 - val_loss: 0.4220 - val_accuracy: 0.8146\n",
      "Epoch 41/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2152 - accuracy: 0.9163\n",
      "Epoch 41: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9100 - val_loss: 0.4315 - val_accuracy: 0.7978\n",
      "Epoch 42/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2259 - accuracy: 0.9075\n",
      "Epoch 42: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9058 - val_loss: 0.4401 - val_accuracy: 0.7921\n",
      "Epoch 43/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2143 - accuracy: 0.9190\n",
      "Epoch 43: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9114 - val_loss: 0.4321 - val_accuracy: 0.8258\n",
      "Epoch 44/100\n",
      "35/72 [=============>................] - ETA: 0s - loss: 0.2117 - accuracy: 0.9057\n",
      "Epoch 44: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9030 - val_loss: 0.4456 - val_accuracy: 0.8034\n",
      "Epoch 45/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9024\n",
      "Epoch 45: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9100 - val_loss: 0.4372 - val_accuracy: 0.8090\n",
      "Epoch 46/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2075 - accuracy: 0.9220\n",
      "Epoch 46: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9128 - val_loss: 0.4465 - val_accuracy: 0.8034\n",
      "Epoch 47/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2281 - accuracy: 0.9024\n",
      "Epoch 47: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9086 - val_loss: 0.4361 - val_accuracy: 0.8258\n",
      "Epoch 48/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2212 - accuracy: 0.9081\n",
      "Epoch 48: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9086 - val_loss: 0.4389 - val_accuracy: 0.8315\n",
      "Epoch 49/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2146 - accuracy: 0.9136\n",
      "Epoch 49: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9114 - val_loss: 0.4355 - val_accuracy: 0.8146\n",
      "Epoch 50/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2188 - accuracy: 0.9143\n",
      "Epoch 50: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9114 - val_loss: 0.4624 - val_accuracy: 0.7921\n",
      "Epoch 51/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2148 - accuracy: 0.9119\n",
      "Epoch 51: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9044 - val_loss: 0.4378 - val_accuracy: 0.8202\n",
      "Epoch 52/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2060 - accuracy: 0.9093\n",
      "Epoch 52: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9072 - val_loss: 0.4464 - val_accuracy: 0.8034\n",
      "Epoch 53/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.2140 - accuracy: 0.9154\n",
      "Epoch 53: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9114 - val_loss: 0.4499 - val_accuracy: 0.7978\n",
      "Epoch 54/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2460 - accuracy: 0.8927\n",
      "Epoch 54: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9072 - val_loss: 0.4499 - val_accuracy: 0.8034\n",
      "Epoch 55/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.2259 - accuracy: 0.9047\n",
      "Epoch 55: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9100 - val_loss: 0.4661 - val_accuracy: 0.8034\n",
      "Epoch 56/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2139 - accuracy: 0.9162\n",
      "Epoch 56: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9128 - val_loss: 0.4736 - val_accuracy: 0.7809\n",
      "Epoch 57/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2144 - accuracy: 0.9162\n",
      "Epoch 57: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9128 - val_loss: 0.4545 - val_accuracy: 0.8146\n",
      "Epoch 58/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2133 - accuracy: 0.9184\n",
      "Epoch 58: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.9184 - val_loss: 0.4632 - val_accuracy: 0.8146\n",
      "Epoch 59/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2051 - accuracy: 0.9093\n",
      "Epoch 59: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2135 - accuracy: 0.9086 - val_loss: 0.4618 - val_accuracy: 0.8034\n",
      "Epoch 60/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2298 - accuracy: 0.8929\n",
      "Epoch 60: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9058 - val_loss: 0.4629 - val_accuracy: 0.8090\n",
      "Epoch 61/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1986 - accuracy: 0.9233\n",
      "Epoch 61: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9156 - val_loss: 0.4656 - val_accuracy: 0.8034\n",
      "Epoch 62/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2242 - accuracy: 0.9073\n",
      "Epoch 62: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9100 - val_loss: 0.4649 - val_accuracy: 0.8034\n",
      "Epoch 63/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2200 - accuracy: 0.9073\n",
      "Epoch 63: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9170 - val_loss: 0.4880 - val_accuracy: 0.8258\n",
      "Epoch 64/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2362 - accuracy: 0.8929\n",
      "Epoch 64: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9001 - val_loss: 0.4706 - val_accuracy: 0.7978\n",
      "Epoch 65/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2101 - accuracy: 0.9179\n",
      "Epoch 65: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9170 - val_loss: 0.4697 - val_accuracy: 0.8090\n",
      "Epoch 66/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2191 - accuracy: 0.9098\n",
      "Epoch 66: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9086 - val_loss: 0.4823 - val_accuracy: 0.7921\n",
      "Epoch 67/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2168 - accuracy: 0.9122\n",
      "Epoch 67: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9142 - val_loss: 0.4649 - val_accuracy: 0.8034\n",
      "Epoch 68/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2142 - accuracy: 0.9119\n",
      "Epoch 68: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2109 - accuracy: 0.9128 - val_loss: 0.4677 - val_accuracy: 0.7865\n",
      "Epoch 69/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2321 - accuracy: 0.9047\n",
      "Epoch 69: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2179 - accuracy: 0.9114 - val_loss: 0.4658 - val_accuracy: 0.8202\n",
      "Epoch 70/100\n",
      "54/72 [=====================>........] - ETA: 0s - loss: 0.2176 - accuracy: 0.9111\n",
      "Epoch 70: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9100 - val_loss: 0.4893 - val_accuracy: 0.8146\n",
      "Epoch 71/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2009 - accuracy: 0.9143\n",
      "Epoch 71: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9100 - val_loss: 0.4811 - val_accuracy: 0.8090\n",
      "Epoch 72/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2392 - accuracy: 0.9143\n",
      "Epoch 72: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9198 - val_loss: 0.4683 - val_accuracy: 0.7978\n",
      "Epoch 73/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2190 - accuracy: 0.9116\n",
      "Epoch 73: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2129 - accuracy: 0.9142 - val_loss: 0.4690 - val_accuracy: 0.8034\n",
      "Epoch 74/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2305 - accuracy: 0.9025\n",
      "Epoch 74: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9086 - val_loss: 0.4837 - val_accuracy: 0.8146\n",
      "Epoch 75/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2227 - accuracy: 0.9146\n",
      "Epoch 75: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9142 - val_loss: 0.4693 - val_accuracy: 0.8034\n",
      "Epoch 76/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2127 - accuracy: 0.9163\n",
      "Epoch 76: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9156 - val_loss: 0.4839 - val_accuracy: 0.8034\n",
      "Epoch 77/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2096 - accuracy: 0.9143\n",
      "Epoch 77: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9142 - val_loss: 0.4730 - val_accuracy: 0.8146\n",
      "Epoch 78/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2142 - accuracy: 0.9058\n",
      "Epoch 78: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9072 - val_loss: 0.4790 - val_accuracy: 0.7921\n",
      "Epoch 79/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2005 - accuracy: 0.9310\n",
      "Epoch 79: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9142 - val_loss: 0.4817 - val_accuracy: 0.7978\n",
      "Epoch 80/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2275 - accuracy: 0.9047\n",
      "Epoch 80: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9114 - val_loss: 0.4894 - val_accuracy: 0.7978\n",
      "Epoch 81/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2010 - accuracy: 0.9209\n",
      "Epoch 81: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2148 - accuracy: 0.9114 - val_loss: 0.4902 - val_accuracy: 0.7921\n",
      "Epoch 82/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1953 - accuracy: 0.9310\n",
      "Epoch 82: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9184 - val_loss: 0.4880 - val_accuracy: 0.8202\n",
      "Epoch 83/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2146 - accuracy: 0.9116\n",
      "Epoch 83: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9114 - val_loss: 0.5029 - val_accuracy: 0.7978\n",
      "Epoch 84/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2091 - accuracy: 0.9140\n",
      "Epoch 84: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9100 - val_loss: 0.4875 - val_accuracy: 0.8034\n",
      "Epoch 85/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2102 - accuracy: 0.9146\n",
      "Epoch 85: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9156 - val_loss: 0.4941 - val_accuracy: 0.7978\n",
      "Epoch 86/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.1836 - accuracy: 0.9289\n",
      "Epoch 86: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9156 - val_loss: 0.4868 - val_accuracy: 0.8146\n",
      "Epoch 87/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1925 - accuracy: 0.9146\n",
      "Epoch 87: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9058 - val_loss: 0.4986 - val_accuracy: 0.7978\n",
      "Epoch 88/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1807 - accuracy: 0.9372\n",
      "Epoch 88: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9170 - val_loss: 0.5298 - val_accuracy: 0.7921\n",
      "Epoch 89/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2204 - accuracy: 0.9116\n",
      "Epoch 89: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2155 - accuracy: 0.9100 - val_loss: 0.4914 - val_accuracy: 0.8146\n",
      "Epoch 90/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2055 - accuracy: 0.9214\n",
      "Epoch 90: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2118 - accuracy: 0.9212 - val_loss: 0.4871 - val_accuracy: 0.8034\n",
      "Epoch 91/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2067 - accuracy: 0.9158\n",
      "Epoch 91: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9156 - val_loss: 0.4968 - val_accuracy: 0.8034\n",
      "Epoch 92/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1905 - accuracy: 0.9357\n",
      "Epoch 92: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9170 - val_loss: 0.4903 - val_accuracy: 0.8034\n",
      "Epoch 93/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2066 - accuracy: 0.9250\n",
      "Epoch 93: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9156 - val_loss: 0.5000 - val_accuracy: 0.8034\n",
      "Epoch 94/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.1971 - accuracy: 0.9350\n",
      "Epoch 94: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9170 - val_loss: 0.4959 - val_accuracy: 0.8090\n",
      "Epoch 95/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2059 - accuracy: 0.9195\n",
      "Epoch 95: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9114 - val_loss: 0.5168 - val_accuracy: 0.8034\n",
      "Epoch 96/100\n",
      "64/72 [=========================>....] - ETA: 0s - loss: 0.2112 - accuracy: 0.9109\n",
      "Epoch 96: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9114 - val_loss: 0.4963 - val_accuracy: 0.8258\n",
      "Epoch 97/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2243 - accuracy: 0.9075\n",
      "Epoch 97: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9100 - val_loss: 0.5010 - val_accuracy: 0.8034\n",
      "Epoch 98/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1987 - accuracy: 0.9171\n",
      "Epoch 98: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9156 - val_loss: 0.5137 - val_accuracy: 0.7978\n",
      "Epoch 99/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1969 - accuracy: 0.9186\n",
      "Epoch 99: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9128 - val_loss: 0.4991 - val_accuracy: 0.8090\n",
      "Epoch 100/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2418 - accuracy: 0.9000\n",
      "Epoch 100: val_accuracy did not improve from 0.86517\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9184 - val_loss: 0.5220 - val_accuracy: 0.7978\n",
      "Data set number:  7\n",
      "Epoch 1/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3123 - accuracy: 0.8930\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87079, saving model to prediction_titanic\\best_model_batch_7.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8776 - val_loss: 0.3330 - val_accuracy: 0.8708\n",
      "Epoch 2/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2905 - accuracy: 0.8870\n",
      "Epoch 2: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8861 - val_loss: 0.3480 - val_accuracy: 0.8652\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2807 - accuracy: 0.8929\n",
      "Epoch 3: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8903 - val_loss: 0.3525 - val_accuracy: 0.8596\n",
      "Epoch 4/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2729 - accuracy: 0.9000\n",
      "Epoch 4: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.8917 - val_loss: 0.3354 - val_accuracy: 0.8539\n",
      "Epoch 5/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2330 - accuracy: 0.9162\n",
      "Epoch 5: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.8945 - val_loss: 0.3475 - val_accuracy: 0.8708\n",
      "Epoch 6/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2629 - accuracy: 0.8881\n",
      "Epoch 6: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8945 - val_loss: 0.3563 - val_accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2503 - accuracy: 0.9095\n",
      "Epoch 7: val_accuracy did not improve from 0.87079\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.8945 - val_loss: 0.3558 - val_accuracy: 0.8652\n",
      "Epoch 8/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2404 - accuracy: 0.8976\n",
      "Epoch 8: val_accuracy improved from 0.87079 to 0.87640, saving model to prediction_titanic\\best_model_batch_7.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.8847 - val_loss: 0.3469 - val_accuracy: 0.8764\n",
      "Epoch 9/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2307 - accuracy: 0.8975\n",
      "Epoch 9: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8903 - val_loss: 0.3503 - val_accuracy: 0.8427\n",
      "Epoch 10/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2621 - accuracy: 0.8763\n",
      "Epoch 10: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8833 - val_loss: 0.3693 - val_accuracy: 0.8315\n",
      "Epoch 11/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.2584 - accuracy: 0.8969\n",
      "Epoch 11: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.8959 - val_loss: 0.3570 - val_accuracy: 0.8652\n",
      "Epoch 12/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2272 - accuracy: 0.9024\n",
      "Epoch 12: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.8917 - val_loss: 0.3396 - val_accuracy: 0.8652\n",
      "Epoch 13/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2450 - accuracy: 0.8878\n",
      "Epoch 13: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2427 - accuracy: 0.8959 - val_loss: 0.3693 - val_accuracy: 0.8539\n",
      "Epoch 14/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2302 - accuracy: 0.9024\n",
      "Epoch 14: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8959 - val_loss: 0.3557 - val_accuracy: 0.8539\n",
      "Epoch 15/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2131 - accuracy: 0.9100\n",
      "Epoch 15: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.8987 - val_loss: 0.3472 - val_accuracy: 0.8596\n",
      "Epoch 16/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2591 - accuracy: 0.8929\n",
      "Epoch 16: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.8973 - val_loss: 0.3533 - val_accuracy: 0.8596\n",
      "Epoch 17/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2480 - accuracy: 0.8951\n",
      "Epoch 17: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.8917 - val_loss: 0.3590 - val_accuracy: 0.8539\n",
      "Epoch 18/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2454 - accuracy: 0.8897\n",
      "Epoch 18: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.8875 - val_loss: 0.3609 - val_accuracy: 0.8652\n",
      "Epoch 19/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2269 - accuracy: 0.9000\n",
      "Epoch 19: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2399 - accuracy: 0.8959 - val_loss: 0.3778 - val_accuracy: 0.8652\n",
      "Epoch 20/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2388 - accuracy: 0.8952\n",
      "Epoch 20: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.8917 - val_loss: 0.3471 - val_accuracy: 0.8708\n",
      "Epoch 21/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2461 - accuracy: 0.8930\n",
      "Epoch 21: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2386 - accuracy: 0.8987 - val_loss: 0.3654 - val_accuracy: 0.8539\n",
      "Epoch 22/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2317 - accuracy: 0.9024\n",
      "Epoch 22: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2425 - accuracy: 0.8945 - val_loss: 0.3708 - val_accuracy: 0.8652\n",
      "Epoch 23/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.2402 - accuracy: 0.9032\n",
      "Epoch 23: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9015 - val_loss: 0.3746 - val_accuracy: 0.8708\n",
      "Epoch 24/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2428 - accuracy: 0.9070\n",
      "Epoch 24: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.8973 - val_loss: 0.3679 - val_accuracy: 0.8371\n",
      "Epoch 25/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2255 - accuracy: 0.8923\n",
      "Epoch 25: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8903 - val_loss: 0.3696 - val_accuracy: 0.8596\n",
      "Epoch 26/100\n",
      "53/72 [=====================>........] - ETA: 0s - loss: 0.2345 - accuracy: 0.8962\n",
      "Epoch 26: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8959 - val_loss: 0.3722 - val_accuracy: 0.8596\n",
      "Epoch 27/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2448 - accuracy: 0.8985\n",
      "Epoch 27: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.8973 - val_loss: 0.3575 - val_accuracy: 0.8427\n",
      "Epoch 28/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2461 - accuracy: 0.8974\n",
      "Epoch 28: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9015 - val_loss: 0.3898 - val_accuracy: 0.8596\n",
      "Epoch 29/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2390 - accuracy: 0.9000\n",
      "Epoch 29: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9001 - val_loss: 0.3636 - val_accuracy: 0.8483\n",
      "Epoch 30/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2582 - accuracy: 0.8857\n",
      "Epoch 30: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2347 - accuracy: 0.8973 - val_loss: 0.3709 - val_accuracy: 0.8539\n",
      "Epoch 31/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2231 - accuracy: 0.9098\n",
      "Epoch 31: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9001 - val_loss: 0.3718 - val_accuracy: 0.8596\n",
      "Epoch 32/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2270 - accuracy: 0.9070\n",
      "Epoch 32: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9030 - val_loss: 0.3708 - val_accuracy: 0.8596\n",
      "Epoch 33/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2232 - accuracy: 0.9095\n",
      "Epoch 33: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8973 - val_loss: 0.3831 - val_accuracy: 0.8315\n",
      "Epoch 34/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2136 - accuracy: 0.9098\n",
      "Epoch 34: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.8959 - val_loss: 0.3785 - val_accuracy: 0.8539\n",
      "Epoch 35/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2580 - accuracy: 0.8854\n",
      "Epoch 35: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.8945 - val_loss: 0.3821 - val_accuracy: 0.8427\n",
      "Epoch 36/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2463 - accuracy: 0.8966\n",
      "Epoch 36: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.8987 - val_loss: 0.3831 - val_accuracy: 0.8596\n",
      "Epoch 37/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2277 - accuracy: 0.9070\n",
      "Epoch 37: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.8987 - val_loss: 0.3837 - val_accuracy: 0.8596\n",
      "Epoch 38/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2125 - accuracy: 0.9190\n",
      "Epoch 38: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9086 - val_loss: 0.3808 - val_accuracy: 0.8539\n",
      "Epoch 39/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.8972\n",
      "Epoch 39: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.8973 - val_loss: 0.3892 - val_accuracy: 0.8596\n",
      "Epoch 40/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2225 - accuracy: 0.9163\n",
      "Epoch 40: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9044 - val_loss: 0.3788 - val_accuracy: 0.8539\n",
      "Epoch 41/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2325 - accuracy: 0.8976\n",
      "Epoch 41: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9015 - val_loss: 0.4084 - val_accuracy: 0.8483\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2113 - accuracy: 0.9167\n",
      "Epoch 42: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 0.8987 - val_loss: 0.3866 - val_accuracy: 0.8539\n",
      "Epoch 43/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2339 - accuracy: 0.9024\n",
      "Epoch 43: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.8987 - val_loss: 0.3690 - val_accuracy: 0.8483\n",
      "Epoch 44/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2168 - accuracy: 0.9098\n",
      "Epoch 44: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9030 - val_loss: 0.3781 - val_accuracy: 0.8483\n",
      "Epoch 45/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9071\n",
      "Epoch 45: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9072 - val_loss: 0.3824 - val_accuracy: 0.8315\n",
      "Epoch 46/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9122\n",
      "Epoch 46: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9030 - val_loss: 0.3950 - val_accuracy: 0.8539\n",
      "Epoch 47/100\n",
      "46/72 [==================>...........] - ETA: 0s - loss: 0.2201 - accuracy: 0.9022\n",
      "Epoch 47: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.8987 - val_loss: 0.3796 - val_accuracy: 0.8539\n",
      "Epoch 48/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2260 - accuracy: 0.9025\n",
      "Epoch 48: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9086 - val_loss: 0.3873 - val_accuracy: 0.8483\n",
      "Epoch 49/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2143 - accuracy: 0.9050\n",
      "Epoch 49: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.8959 - val_loss: 0.3889 - val_accuracy: 0.8596\n",
      "Epoch 50/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2803 - accuracy: 0.8738\n",
      "Epoch 50: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.8903 - val_loss: 0.3922 - val_accuracy: 0.8427\n",
      "Epoch 51/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2144 - accuracy: 0.9140\n",
      "Epoch 51: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9044 - val_loss: 0.3965 - val_accuracy: 0.8596\n",
      "Epoch 52/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2359 - accuracy: 0.9024\n",
      "Epoch 52: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.9072 - val_loss: 0.4154 - val_accuracy: 0.8371\n",
      "Epoch 53/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2201 - accuracy: 0.9122\n",
      "Epoch 53: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.9058 - val_loss: 0.4075 - val_accuracy: 0.8539\n",
      "Epoch 54/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2239 - accuracy: 0.8986\n",
      "Epoch 54: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.8973 - val_loss: 0.3932 - val_accuracy: 0.8483\n",
      "Epoch 55/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2306 - accuracy: 0.9047\n",
      "Epoch 55: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2351 - accuracy: 0.9030 - val_loss: 0.3963 - val_accuracy: 0.8483\n",
      "Epoch 56/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2348 - accuracy: 0.8976\n",
      "Epoch 56: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.8987 - val_loss: 0.3973 - val_accuracy: 0.8427\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9086\n",
      "Epoch 57: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9086 - val_loss: 0.3948 - val_accuracy: 0.8652\n",
      "Epoch 58/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2279 - accuracy: 0.9048\n",
      "Epoch 58: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.9001 - val_loss: 0.4040 - val_accuracy: 0.8539\n",
      "Epoch 59/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2160 - accuracy: 0.9071\n",
      "Epoch 59: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9044 - val_loss: 0.4085 - val_accuracy: 0.8371\n",
      "Epoch 60/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2367 - accuracy: 0.8929\n",
      "Epoch 60: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9015 - val_loss: 0.4119 - val_accuracy: 0.8596\n",
      "Epoch 61/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2343 - accuracy: 0.8951\n",
      "Epoch 61: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9001 - val_loss: 0.4053 - val_accuracy: 0.8427\n",
      "Epoch 62/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1856 - accuracy: 0.9317\n",
      "Epoch 62: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9058 - val_loss: 0.4102 - val_accuracy: 0.8315\n",
      "Epoch 63/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2191 - accuracy: 0.9095\n",
      "Epoch 63: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9044 - val_loss: 0.4116 - val_accuracy: 0.8539\n",
      "Epoch 64/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2140 - accuracy: 0.9143\n",
      "Epoch 64: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9058 - val_loss: 0.4009 - val_accuracy: 0.8596\n",
      "Epoch 65/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2229 - accuracy: 0.9167\n",
      "Epoch 65: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9086 - val_loss: 0.4021 - val_accuracy: 0.8371\n",
      "Epoch 66/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2506 - accuracy: 0.8810\n",
      "Epoch 66: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9015 - val_loss: 0.4207 - val_accuracy: 0.8371\n",
      "Epoch 67/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2146 - accuracy: 0.9086\n",
      "Epoch 67: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.8973 - val_loss: 0.4246 - val_accuracy: 0.8427\n",
      "Epoch 68/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1908 - accuracy: 0.9095\n",
      "Epoch 68: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9001 - val_loss: 0.3979 - val_accuracy: 0.8483\n",
      "Epoch 69/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2143 - accuracy: 0.8977\n",
      "Epoch 69: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.8973 - val_loss: 0.4122 - val_accuracy: 0.8539\n",
      "Epoch 70/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2068 - accuracy: 0.9122\n",
      "Epoch 70: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9058 - val_loss: 0.4196 - val_accuracy: 0.8539\n",
      "Epoch 71/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2215 - accuracy: 0.9024\n",
      "Epoch 71: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9001 - val_loss: 0.4245 - val_accuracy: 0.8539\n",
      "Epoch 72/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2091 - accuracy: 0.9000\n",
      "Epoch 72: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9044 - val_loss: 0.3994 - val_accuracy: 0.8539\n",
      "Epoch 73/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2231 - accuracy: 0.8976\n",
      "Epoch 73: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9044 - val_loss: 0.4214 - val_accuracy: 0.8427\n",
      "Epoch 74/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.2292 - accuracy: 0.9061\n",
      "Epoch 74: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9072 - val_loss: 0.4065 - val_accuracy: 0.8539\n",
      "Epoch 75/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2084 - accuracy: 0.9143\n",
      "Epoch 75: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9015 - val_loss: 0.4238 - val_accuracy: 0.8371\n",
      "Epoch 76/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2153 - accuracy: 0.9119\n",
      "Epoch 76: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9044 - val_loss: 0.4165 - val_accuracy: 0.8427\n",
      "Epoch 77/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2423 - accuracy: 0.8973\n",
      "Epoch 77: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9030 - val_loss: 0.4254 - val_accuracy: 0.8539\n",
      "Epoch 78/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2069 - accuracy: 0.9116\n",
      "Epoch 78: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2252 - accuracy: 0.9058 - val_loss: 0.4319 - val_accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2215 - accuracy: 0.9071\n",
      "Epoch 79: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.8987 - val_loss: 0.4155 - val_accuracy: 0.8596\n",
      "Epoch 80/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2070 - accuracy: 0.9195\n",
      "Epoch 80: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9086 - val_loss: 0.4203 - val_accuracy: 0.8596\n",
      "Epoch 81/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2138 - accuracy: 0.9209\n",
      "Epoch 81: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9058 - val_loss: 0.4309 - val_accuracy: 0.8539\n",
      "Epoch 82/100\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.2216 - accuracy: 0.9058\n",
      "Epoch 82: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9058 - val_loss: 0.4220 - val_accuracy: 0.8371\n",
      "Epoch 83/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2044 - accuracy: 0.9262\n",
      "Epoch 83: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9086 - val_loss: 0.4250 - val_accuracy: 0.8427\n",
      "Epoch 84/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2259 - accuracy: 0.9024\n",
      "Epoch 84: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9086 - val_loss: 0.4260 - val_accuracy: 0.8315\n",
      "Epoch 85/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1983 - accuracy: 0.9195\n",
      "Epoch 85: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9044 - val_loss: 0.4297 - val_accuracy: 0.8539\n",
      "Epoch 86/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2187 - accuracy: 0.9047\n",
      "Epoch 86: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9072 - val_loss: 0.4272 - val_accuracy: 0.8371\n",
      "Epoch 87/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2185 - accuracy: 0.9095\n",
      "Epoch 87: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9044 - val_loss: 0.4362 - val_accuracy: 0.8315\n",
      "Epoch 88/100\n",
      "44/72 [=================>............] - ETA: 0s - loss: 0.2005 - accuracy: 0.9136\n",
      "Epoch 88: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9015 - val_loss: 0.4217 - val_accuracy: 0.8483\n",
      "Epoch 89/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9029\n",
      "Epoch 89: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2230 - accuracy: 0.9044 - val_loss: 0.4479 - val_accuracy: 0.8483\n",
      "Epoch 90/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2192 - accuracy: 0.9150\n",
      "Epoch 90: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9100 - val_loss: 0.4369 - val_accuracy: 0.8427\n",
      "Epoch 91/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2199 - accuracy: 0.8930\n",
      "Epoch 91: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.8987 - val_loss: 0.4338 - val_accuracy: 0.8596\n",
      "Epoch 92/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2362 - accuracy: 0.8900\n",
      "Epoch 92: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9044 - val_loss: 0.4328 - val_accuracy: 0.8596\n",
      "Epoch 93/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2168 - accuracy: 0.9047\n",
      "Epoch 93: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.9058 - val_loss: 0.4376 - val_accuracy: 0.8315\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2229 - accuracy: 0.9000\n",
      "Epoch 94: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9001 - val_loss: 0.4460 - val_accuracy: 0.8483\n",
      "Epoch 95/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2088 - accuracy: 0.9195\n",
      "Epoch 95: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9058 - val_loss: 0.4381 - val_accuracy: 0.8258\n",
      "Epoch 96/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2274 - accuracy: 0.9024\n",
      "Epoch 96: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.8973 - val_loss: 0.4369 - val_accuracy: 0.8258\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9015\n",
      "Epoch 97: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9015 - val_loss: 0.4367 - val_accuracy: 0.8539\n",
      "Epoch 98/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1986 - accuracy: 0.9190\n",
      "Epoch 98: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9100 - val_loss: 0.4333 - val_accuracy: 0.8483\n",
      "Epoch 99/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2215 - accuracy: 0.9048\n",
      "Epoch 99: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9142 - val_loss: 0.4454 - val_accuracy: 0.8371\n",
      "Epoch 100/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2210 - accuracy: 0.9029\n",
      "Epoch 100: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.9030 - val_loss: 0.4588 - val_accuracy: 0.8539\n",
      "Data set number:  8\n",
      "Epoch 1/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3015 - accuracy: 0.8848\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87640, saving model to prediction_titanic\\best_model_batch_8.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.3022 - accuracy: 0.8819 - val_loss: 0.2755 - val_accuracy: 0.8764\n",
      "Epoch 2/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.2855 - accuracy: 0.8889\n",
      "Epoch 2: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2858 - accuracy: 0.8889 - val_loss: 0.2727 - val_accuracy: 0.8764\n",
      "Epoch 3/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2627 - accuracy: 0.9048\n",
      "Epoch 3: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8973 - val_loss: 0.2883 - val_accuracy: 0.8652\n",
      "Epoch 4/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2533 - accuracy: 0.8976\n",
      "Epoch 4: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.8931 - val_loss: 0.2837 - val_accuracy: 0.8708\n",
      "Epoch 5/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2452 - accuracy: 0.8975\n",
      "Epoch 5: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8889 - val_loss: 0.3093 - val_accuracy: 0.8652\n",
      "Epoch 6/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2646 - accuracy: 0.9000\n",
      "Epoch 6: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8903 - val_loss: 0.2991 - val_accuracy: 0.8596\n",
      "Epoch 7/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.3108 - accuracy: 0.8953\n",
      "Epoch 7: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8903 - val_loss: 0.2907 - val_accuracy: 0.8764\n",
      "Epoch 8/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2550 - accuracy: 0.9017\n",
      "Epoch 8: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.8973 - val_loss: 0.2987 - val_accuracy: 0.8596\n",
      "Epoch 9/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2450 - accuracy: 0.9028\n",
      "Epoch 9: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2586 - accuracy: 0.8945 - val_loss: 0.3036 - val_accuracy: 0.8652\n",
      "Epoch 10/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2480 - accuracy: 0.9048\n",
      "Epoch 10: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.9001 - val_loss: 0.3097 - val_accuracy: 0.8596\n",
      "Epoch 11/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2615 - accuracy: 0.8810\n",
      "Epoch 11: val_accuracy did not improve from 0.87640\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.8931 - val_loss: 0.3115 - val_accuracy: 0.8596\n",
      "Epoch 12/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2468 - accuracy: 0.8921\n",
      "Epoch 12: val_accuracy improved from 0.87640 to 0.88202, saving model to prediction_titanic\\best_model_batch_8.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.8861 - val_loss: 0.3015 - val_accuracy: 0.8820\n",
      "Epoch 13/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2389 - accuracy: 0.9000\n",
      "Epoch 13: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.8959 - val_loss: 0.3107 - val_accuracy: 0.8708\n",
      "Epoch 14/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2152 - accuracy: 0.9195\n",
      "Epoch 14: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9001 - val_loss: 0.3203 - val_accuracy: 0.8652\n",
      "Epoch 15/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2225 - accuracy: 0.9119\n",
      "Epoch 15: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.8959 - val_loss: 0.3145 - val_accuracy: 0.8708\n",
      "Epoch 16/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.2531 - accuracy: 0.8940\n",
      "Epoch 16: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.8959 - val_loss: 0.3180 - val_accuracy: 0.8652\n",
      "Epoch 17/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2434 - accuracy: 0.8976\n",
      "Epoch 17: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8903 - val_loss: 0.3214 - val_accuracy: 0.8596\n",
      "Epoch 18/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2535 - accuracy: 0.8878\n",
      "Epoch 18: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.8903 - val_loss: 0.3263 - val_accuracy: 0.8539\n",
      "Epoch 19/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.8944\n",
      "Epoch 19: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2411 - accuracy: 0.8931 - val_loss: 0.3235 - val_accuracy: 0.8652\n",
      "Epoch 20/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2796 - accuracy: 0.8810\n",
      "Epoch 20: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.8889 - val_loss: 0.3257 - val_accuracy: 0.8708\n",
      "Epoch 21/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2297 - accuracy: 0.9075\n",
      "Epoch 21: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.8917 - val_loss: 0.3307 - val_accuracy: 0.8708\n",
      "Epoch 22/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2171 - accuracy: 0.9122\n",
      "Epoch 22: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.8945 - val_loss: 0.3318 - val_accuracy: 0.8596\n",
      "Epoch 23/100\n",
      "46/72 [==================>...........] - ETA: 0s - loss: 0.2286 - accuracy: 0.9130\n",
      "Epoch 23: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.8945 - val_loss: 0.3275 - val_accuracy: 0.8652\n",
      "Epoch 24/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2171 - accuracy: 0.8976\n",
      "Epoch 24: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.8945 - val_loss: 0.3386 - val_accuracy: 0.8652\n",
      "Epoch 25/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2318 - accuracy: 0.8976\n",
      "Epoch 25: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.8973 - val_loss: 0.3330 - val_accuracy: 0.8708\n",
      "Epoch 26/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2156 - accuracy: 0.9054\n",
      "Epoch 26: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.8931 - val_loss: 0.3421 - val_accuracy: 0.8539\n",
      "Epoch 27/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2404 - accuracy: 0.9000\n",
      "Epoch 27: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9015 - val_loss: 0.3335 - val_accuracy: 0.8652\n",
      "Epoch 28/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2549 - accuracy: 0.8905\n",
      "Epoch 28: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9015 - val_loss: 0.3486 - val_accuracy: 0.8596\n",
      "Epoch 29/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2477 - accuracy: 0.8927\n",
      "Epoch 29: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.8945 - val_loss: 0.3329 - val_accuracy: 0.8596\n",
      "Epoch 30/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2501 - accuracy: 0.8902\n",
      "Epoch 30: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.8945 - val_loss: 0.3360 - val_accuracy: 0.8708\n",
      "Epoch 31/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2331 - accuracy: 0.9000\n",
      "Epoch 31: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.9001 - val_loss: 0.3399 - val_accuracy: 0.8596\n",
      "Epoch 32/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2551 - accuracy: 0.8930\n",
      "Epoch 32: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.9001 - val_loss: 0.3354 - val_accuracy: 0.8708\n",
      "Epoch 33/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2096 - accuracy: 0.9214\n",
      "Epoch 33: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.9058 - val_loss: 0.3369 - val_accuracy: 0.8652\n",
      "Epoch 34/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.8915\n",
      "Epoch 34: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.8917 - val_loss: 0.3459 - val_accuracy: 0.8652\n",
      "Epoch 35/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2553 - accuracy: 0.8952\n",
      "Epoch 35: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9030 - val_loss: 0.3432 - val_accuracy: 0.8708\n",
      "Epoch 36/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2426 - accuracy: 0.8850\n",
      "Epoch 36: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.8903 - val_loss: 0.3485 - val_accuracy: 0.8652\n",
      "Epoch 37/100\n",
      "53/72 [=====================>........] - ETA: 0s - loss: 0.2307 - accuracy: 0.9019\n",
      "Epoch 37: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9044 - val_loss: 0.3567 - val_accuracy: 0.8483\n",
      "Epoch 38/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2769 - accuracy: 0.8775\n",
      "Epoch 38: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.8945 - val_loss: 0.3458 - val_accuracy: 0.8764\n",
      "Epoch 39/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2164 - accuracy: 0.9171\n",
      "Epoch 39: val_accuracy improved from 0.88202 to 0.89326, saving model to prediction_titanic\\best_model_batch_8.h5\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9072 - val_loss: 0.3459 - val_accuracy: 0.8933\n",
      "Epoch 40/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2343 - accuracy: 0.9000\n",
      "Epoch 40: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.9030 - val_loss: 0.3470 - val_accuracy: 0.8596\n",
      "Epoch 41/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9029\n",
      "Epoch 41: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.9044 - val_loss: 0.3460 - val_accuracy: 0.8596\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1978 - accuracy: 0.9071\n",
      "Epoch 42: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.9001 - val_loss: 0.3434 - val_accuracy: 0.8708\n",
      "Epoch 43/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2384 - accuracy: 0.8952\n",
      "Epoch 43: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.8987 - val_loss: 0.3319 - val_accuracy: 0.8764\n",
      "Epoch 44/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2241 - accuracy: 0.9000\n",
      "Epoch 44: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9015 - val_loss: 0.3404 - val_accuracy: 0.8708\n",
      "Epoch 45/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2240 - accuracy: 0.9024\n",
      "Epoch 45: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.8973 - val_loss: 0.3392 - val_accuracy: 0.8708\n",
      "Epoch 46/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2414 - accuracy: 0.8975\n",
      "Epoch 46: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9086 - val_loss: 0.3457 - val_accuracy: 0.8708\n",
      "Epoch 47/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2218 - accuracy: 0.9048\n",
      "Epoch 47: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.8973 - val_loss: 0.3359 - val_accuracy: 0.8708\n",
      "Epoch 48/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2239 - accuracy: 0.9048\n",
      "Epoch 48: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8987 - val_loss: 0.3455 - val_accuracy: 0.8708\n",
      "Epoch 49/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2218 - accuracy: 0.9049\n",
      "Epoch 49: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.8987 - val_loss: 0.3443 - val_accuracy: 0.8708\n",
      "Epoch 50/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2283 - accuracy: 0.9027\n",
      "Epoch 50: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2289 - accuracy: 0.9058 - val_loss: 0.3543 - val_accuracy: 0.8708\n",
      "Epoch 51/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2321 - accuracy: 0.9000\n",
      "Epoch 51: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2318 - accuracy: 0.9001 - val_loss: 0.3449 - val_accuracy: 0.8652\n",
      "Epoch 52/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.1989 - accuracy: 0.9162\n",
      "Epoch 52: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.8973 - val_loss: 0.3424 - val_accuracy: 0.8708\n",
      "Epoch 53/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2612 - accuracy: 0.8905\n",
      "Epoch 53: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9015 - val_loss: 0.3538 - val_accuracy: 0.8652\n",
      "Epoch 54/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2499 - accuracy: 0.8857\n",
      "Epoch 54: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.8903 - val_loss: 0.3528 - val_accuracy: 0.8539\n",
      "Epoch 55/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2242 - accuracy: 0.9024\n",
      "Epoch 55: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9058 - val_loss: 0.3509 - val_accuracy: 0.8539\n",
      "Epoch 56/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2375 - accuracy: 0.8957\n",
      "Epoch 56: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.8959 - val_loss: 0.3520 - val_accuracy: 0.8483\n",
      "Epoch 57/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2343 - accuracy: 0.8976\n",
      "Epoch 57: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8987 - val_loss: 0.3424 - val_accuracy: 0.8708\n",
      "Epoch 58/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2423 - accuracy: 0.8902\n",
      "Epoch 58: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9030 - val_loss: 0.3661 - val_accuracy: 0.8427\n",
      "Epoch 59/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1959 - accuracy: 0.9171\n",
      "Epoch 59: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9030 - val_loss: 0.3408 - val_accuracy: 0.8764\n",
      "Epoch 60/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2361 - accuracy: 0.9095\n",
      "Epoch 60: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9044 - val_loss: 0.3498 - val_accuracy: 0.8596\n",
      "Epoch 61/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2118 - accuracy: 0.9085\n",
      "Epoch 61: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9001 - val_loss: 0.3529 - val_accuracy: 0.8652\n",
      "Epoch 62/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2248 - accuracy: 0.8975\n",
      "Epoch 62: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.9015 - val_loss: 0.3478 - val_accuracy: 0.8820\n",
      "Epoch 63/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2346 - accuracy: 0.8850\n",
      "Epoch 63: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.8875 - val_loss: 0.3563 - val_accuracy: 0.8652\n",
      "Epoch 64/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2413 - accuracy: 0.9049\n",
      "Epoch 64: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.9086 - val_loss: 0.3639 - val_accuracy: 0.8483\n",
      "Epoch 65/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2132 - accuracy: 0.9146\n",
      "Epoch 65: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9001 - val_loss: 0.3456 - val_accuracy: 0.8652\n",
      "Epoch 66/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2323 - accuracy: 0.9070\n",
      "Epoch 66: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9044 - val_loss: 0.3457 - val_accuracy: 0.8652\n",
      "Epoch 67/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2178 - accuracy: 0.9073\n",
      "Epoch 67: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.8973 - val_loss: 0.3570 - val_accuracy: 0.8652\n",
      "Epoch 68/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2278 - accuracy: 0.9100\n",
      "Epoch 68: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9086 - val_loss: 0.3878 - val_accuracy: 0.8427\n",
      "Epoch 69/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2401 - accuracy: 0.8912\n",
      "Epoch 69: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.8945 - val_loss: 0.3467 - val_accuracy: 0.8708\n",
      "Epoch 70/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2267 - accuracy: 0.8907\n",
      "Epoch 70: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9030 - val_loss: 0.3471 - val_accuracy: 0.8652\n",
      "Epoch 71/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2404 - accuracy: 0.8921\n",
      "Epoch 71: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.8973 - val_loss: 0.3648 - val_accuracy: 0.8596\n",
      "Epoch 72/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2269 - accuracy: 0.9078\n",
      "Epoch 72: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.9001 - val_loss: 0.3501 - val_accuracy: 0.8764\n",
      "Epoch 73/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2376 - accuracy: 0.8941\n",
      "Epoch 73: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.8917 - val_loss: 0.3555 - val_accuracy: 0.8596\n",
      "Epoch 74/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2292 - accuracy: 0.9015\n",
      "Epoch 74: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.9015 - val_loss: 0.3531 - val_accuracy: 0.8539\n",
      "Epoch 75/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2395 - accuracy: 0.8984\n",
      "Epoch 75: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.9001 - val_loss: 0.3588 - val_accuracy: 0.8596\n",
      "Epoch 76/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2358 - accuracy: 0.9073\n",
      "Epoch 76: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9030 - val_loss: 0.3585 - val_accuracy: 0.8539\n",
      "Epoch 77/100\n",
      "63/72 [=========================>....] - ETA: 0s - loss: 0.2248 - accuracy: 0.8921\n",
      "Epoch 77: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.8931 - val_loss: 0.3606 - val_accuracy: 0.8596\n",
      "Epoch 78/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2233 - accuracy: 0.9075\n",
      "Epoch 78: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9001 - val_loss: 0.3550 - val_accuracy: 0.8483\n",
      "Epoch 79/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2270 - accuracy: 0.9029\n",
      "Epoch 79: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9030 - val_loss: 0.3578 - val_accuracy: 0.8539\n",
      "Epoch 80/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2106 - accuracy: 0.9184\n",
      "Epoch 80: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9044 - val_loss: 0.3643 - val_accuracy: 0.8539\n",
      "Epoch 81/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2158 - accuracy: 0.9143\n",
      "Epoch 81: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9100 - val_loss: 0.3528 - val_accuracy: 0.8708\n",
      "Epoch 82/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2274 - accuracy: 0.9083\n",
      "Epoch 82: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9030 - val_loss: 0.3465 - val_accuracy: 0.8708\n",
      "Epoch 83/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2169 - accuracy: 0.9167\n",
      "Epoch 83: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9001 - val_loss: 0.3501 - val_accuracy: 0.8708\n",
      "Epoch 84/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2192 - accuracy: 0.9100\n",
      "Epoch 84: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9058 - val_loss: 0.3658 - val_accuracy: 0.8483\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.8987\n",
      "Epoch 85: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.8987 - val_loss: 0.3577 - val_accuracy: 0.8652\n",
      "Epoch 86/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2244 - accuracy: 0.8944\n",
      "Epoch 86: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.8987 - val_loss: 0.3613 - val_accuracy: 0.8315\n",
      "Epoch 87/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2273 - accuracy: 0.8881\n",
      "Epoch 87: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.8945 - val_loss: 0.3495 - val_accuracy: 0.8652\n",
      "Epoch 88/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2002 - accuracy: 0.9048\n",
      "Epoch 88: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2258 - accuracy: 0.9030 - val_loss: 0.3562 - val_accuracy: 0.8539\n",
      "Epoch 89/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2164 - accuracy: 0.9098\n",
      "Epoch 89: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9015 - val_loss: 0.3675 - val_accuracy: 0.8539\n",
      "Epoch 90/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2042 - accuracy: 0.9093\n",
      "Epoch 90: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.8959 - val_loss: 0.3654 - val_accuracy: 0.8708\n",
      "Epoch 91/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2385 - accuracy: 0.8900\n",
      "Epoch 91: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9015 - val_loss: 0.3577 - val_accuracy: 0.8652\n",
      "Epoch 92/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2256 - accuracy: 0.9049\n",
      "Epoch 92: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9100 - val_loss: 0.3534 - val_accuracy: 0.8652\n",
      "Epoch 93/100\n",
      "36/72 [==============>...............] - ETA: 0s - loss: 0.2100 - accuracy: 0.9306\n",
      "Epoch 93: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9086 - val_loss: 0.3721 - val_accuracy: 0.8539\n",
      "Epoch 94/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2242 - accuracy: 0.9077\n",
      "Epoch 94: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.8973 - val_loss: 0.3687 - val_accuracy: 0.8708\n",
      "Epoch 95/100\n",
      "56/72 [======================>.......] - ETA: 0s - loss: 0.2246 - accuracy: 0.9107\n",
      "Epoch 95: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9086 - val_loss: 0.3602 - val_accuracy: 0.8652\n",
      "Epoch 96/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2164 - accuracy: 0.9075\n",
      "Epoch 96: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9058 - val_loss: 0.3711 - val_accuracy: 0.8427\n",
      "Epoch 97/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2237 - accuracy: 0.9146\n",
      "Epoch 97: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9058 - val_loss: 0.3646 - val_accuracy: 0.8652\n",
      "Epoch 98/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2352 - accuracy: 0.9025\n",
      "Epoch 98: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9086 - val_loss: 0.3803 - val_accuracy: 0.8596\n",
      "Epoch 99/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2104 - accuracy: 0.9179\n",
      "Epoch 99: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9100 - val_loss: 0.3706 - val_accuracy: 0.8596\n",
      "Epoch 100/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2246 - accuracy: 0.9150\n",
      "Epoch 100: val_accuracy did not improve from 0.89326\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2253 - accuracy: 0.9058 - val_loss: 0.3789 - val_accuracy: 0.8483\n",
      "Data set number:  9\n",
      "Epoch 1/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2644 - accuracy: 0.8952\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88202, saving model to prediction_titanic\\best_model_batch_9.h5\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8945 - val_loss: 0.3749 - val_accuracy: 0.8820\n",
      "Epoch 2/100\n",
      "51/72 [====================>.........] - ETA: 0s - loss: 0.2699 - accuracy: 0.8843\n",
      "Epoch 2: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8903 - val_loss: 0.3568 - val_accuracy: 0.8764\n",
      "Epoch 3/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2128 - accuracy: 0.9163\n",
      "Epoch 3: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9044 - val_loss: 0.3612 - val_accuracy: 0.8652\n",
      "Epoch 4/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2347 - accuracy: 0.8976\n",
      "Epoch 4: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9001 - val_loss: 0.3724 - val_accuracy: 0.8708\n",
      "Epoch 5/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2090 - accuracy: 0.9116\n",
      "Epoch 5: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9001 - val_loss: 0.3775 - val_accuracy: 0.8708\n",
      "Epoch 6/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9042\n",
      "Epoch 6: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2190 - accuracy: 0.9044 - val_loss: 0.3850 - val_accuracy: 0.8764\n",
      "Epoch 7/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2417 - accuracy: 0.8977\n",
      "Epoch 7: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.8931 - val_loss: 0.3678 - val_accuracy: 0.8764\n",
      "Epoch 8/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2136 - accuracy: 0.9190\n",
      "Epoch 8: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9001 - val_loss: 0.3808 - val_accuracy: 0.8820\n",
      "Epoch 9/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2188 - accuracy: 0.9075\n",
      "Epoch 9: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9015 - val_loss: 0.3822 - val_accuracy: 0.8764\n",
      "Epoch 10/100\n",
      "61/72 [========================>.....] - ETA: 0s - loss: 0.1999 - accuracy: 0.9098\n",
      "Epoch 10: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9001 - val_loss: 0.3896 - val_accuracy: 0.8708\n",
      "Epoch 11/100\n",
      "57/72 [======================>.......] - ETA: 0s - loss: 0.2128 - accuracy: 0.9070\n",
      "Epoch 11: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9001 - val_loss: 0.3832 - val_accuracy: 0.8764\n",
      "Epoch 12/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1972 - accuracy: 0.9095\n",
      "Epoch 12: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.8987 - val_loss: 0.3861 - val_accuracy: 0.8764\n",
      "Epoch 13/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9014\n",
      "Epoch 13: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9015 - val_loss: 0.3946 - val_accuracy: 0.8820\n",
      "Epoch 14/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2055 - accuracy: 0.9024\n",
      "Epoch 14: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9015 - val_loss: 0.3867 - val_accuracy: 0.8652\n",
      "Epoch 15/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.1797 - accuracy: 0.9200\n",
      "Epoch 15: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.8959 - val_loss: 0.4026 - val_accuracy: 0.8764\n",
      "Epoch 16/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2210 - accuracy: 0.9143\n",
      "Epoch 16: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.9015 - val_loss: 0.4004 - val_accuracy: 0.8652\n",
      "Epoch 17/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2328 - accuracy: 0.9000\n",
      "Epoch 17: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.9058 - val_loss: 0.4118 - val_accuracy: 0.8820\n",
      "Epoch 18/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1929 - accuracy: 0.9238\n",
      "Epoch 18: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9072 - val_loss: 0.3960 - val_accuracy: 0.8820\n",
      "Epoch 19/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2235 - accuracy: 0.9000\n",
      "Epoch 19: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9044 - val_loss: 0.3976 - val_accuracy: 0.8820\n",
      "Epoch 20/100\n",
      "55/72 [=====================>........] - ETA: 0s - loss: 0.2198 - accuracy: 0.9055\n",
      "Epoch 20: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9100 - val_loss: 0.4176 - val_accuracy: 0.8652\n",
      "Epoch 21/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.2154 - accuracy: 0.9108\n",
      "Epoch 21: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9030 - val_loss: 0.4122 - val_accuracy: 0.8652\n",
      "Epoch 22/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2157 - accuracy: 0.9098\n",
      "Epoch 22: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2181 - accuracy: 0.9086 - val_loss: 0.4113 - val_accuracy: 0.8708\n",
      "Epoch 23/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2132 - accuracy: 0.9195\n",
      "Epoch 23: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9100 - val_loss: 0.4345 - val_accuracy: 0.8539\n",
      "Epoch 24/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2116 - accuracy: 0.9103\n",
      "Epoch 24: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9128 - val_loss: 0.4093 - val_accuracy: 0.8764\n",
      "Epoch 25/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1904 - accuracy: 0.9119\n",
      "Epoch 25: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9114 - val_loss: 0.4185 - val_accuracy: 0.8764\n",
      "Epoch 26/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2198 - accuracy: 0.9071\n",
      "Epoch 26: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9044 - val_loss: 0.4396 - val_accuracy: 0.8539\n",
      "Epoch 27/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2144 - accuracy: 0.9095\n",
      "Epoch 27: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9072 - val_loss: 0.4095 - val_accuracy: 0.8708\n",
      "Epoch 28/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2241 - accuracy: 0.9000\n",
      "Epoch 28: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9128 - val_loss: 0.4255 - val_accuracy: 0.8708\n",
      "Epoch 29/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.2156 - accuracy: 0.9068\n",
      "Epoch 29: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9058 - val_loss: 0.4184 - val_accuracy: 0.8764\n",
      "Epoch 30/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2136 - accuracy: 0.9024\n",
      "Epoch 30: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2090 - accuracy: 0.9001 - val_loss: 0.4434 - val_accuracy: 0.8708\n",
      "Epoch 31/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2222 - accuracy: 0.8905\n",
      "Epoch 31: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9001 - val_loss: 0.4404 - val_accuracy: 0.8708\n",
      "Epoch 32/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2002 - accuracy: 0.9119\n",
      "Epoch 32: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9030 - val_loss: 0.4184 - val_accuracy: 0.8652\n",
      "Epoch 33/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2006 - accuracy: 0.9293\n",
      "Epoch 33: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9114 - val_loss: 0.4241 - val_accuracy: 0.8539\n",
      "Epoch 34/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2179 - accuracy: 0.9023\n",
      "Epoch 34: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9044 - val_loss: 0.4502 - val_accuracy: 0.8539\n",
      "Epoch 35/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.1870 - accuracy: 0.9158\n",
      "Epoch 35: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9030 - val_loss: 0.4159 - val_accuracy: 0.8708\n",
      "Epoch 36/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2247 - accuracy: 0.8925\n",
      "Epoch 36: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9072 - val_loss: 0.4473 - val_accuracy: 0.8652\n",
      "Epoch 37/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.2009 - accuracy: 0.9077\n",
      "Epoch 37: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9001 - val_loss: 0.4219 - val_accuracy: 0.8708\n",
      "Epoch 38/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2389 - accuracy: 0.8860\n",
      "Epoch 38: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9044 - val_loss: 0.4527 - val_accuracy: 0.8596\n",
      "Epoch 39/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2141 - accuracy: 0.9099\n",
      "Epoch 39: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9100 - val_loss: 0.4309 - val_accuracy: 0.8596\n",
      "Epoch 40/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2037 - accuracy: 0.9098\n",
      "Epoch 40: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9072 - val_loss: 0.4250 - val_accuracy: 0.8652\n",
      "Epoch 41/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2236 - accuracy: 0.8902\n",
      "Epoch 41: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2086 - accuracy: 0.8959 - val_loss: 0.4688 - val_accuracy: 0.8483\n",
      "Epoch 42/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2147 - accuracy: 0.8905\n",
      "Epoch 42: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9030 - val_loss: 0.4402 - val_accuracy: 0.8596\n",
      "Epoch 43/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2203 - accuracy: 0.9023\n",
      "Epoch 43: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9044 - val_loss: 0.4308 - val_accuracy: 0.8708\n",
      "Epoch 44/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2063 - accuracy: 0.9088\n",
      "Epoch 44: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9058 - val_loss: 0.4592 - val_accuracy: 0.8539\n",
      "Epoch 45/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.2132 - accuracy: 0.9015\n",
      "Epoch 45: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2114 - accuracy: 0.9030 - val_loss: 0.4395 - val_accuracy: 0.8539\n",
      "Epoch 46/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9042\n",
      "Epoch 46: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9044 - val_loss: 0.4355 - val_accuracy: 0.8596\n",
      "Epoch 47/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.2027 - accuracy: 0.9074\n",
      "Epoch 47: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9072 - val_loss: 0.4606 - val_accuracy: 0.8596\n",
      "Epoch 48/100\n",
      "37/72 [==============>...............] - ETA: 0s - loss: 0.1961 - accuracy: 0.9135\n",
      "Epoch 48: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9086 - val_loss: 0.4376 - val_accuracy: 0.8596\n",
      "Epoch 49/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2154 - accuracy: 0.9024\n",
      "Epoch 49: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2057 - accuracy: 0.9128 - val_loss: 0.4562 - val_accuracy: 0.8539\n",
      "Epoch 50/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.9057\n",
      "Epoch 50: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9072 - val_loss: 0.4376 - val_accuracy: 0.8708\n",
      "Epoch 51/100\n",
      "59/72 [=======================>......] - ETA: 0s - loss: 0.1943 - accuracy: 0.9068\n",
      "Epoch 51: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9030 - val_loss: 0.4470 - val_accuracy: 0.8596\n",
      "Epoch 52/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.1887 - accuracy: 0.9158\n",
      "Epoch 52: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9086 - val_loss: 0.4398 - val_accuracy: 0.8652\n",
      "Epoch 53/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1989 - accuracy: 0.9024\n",
      "Epoch 53: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2191 - accuracy: 0.9001 - val_loss: 0.4433 - val_accuracy: 0.8483\n",
      "Epoch 54/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1860 - accuracy: 0.9190\n",
      "Epoch 54: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9100 - val_loss: 0.4431 - val_accuracy: 0.8539\n",
      "Epoch 55/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.2054 - accuracy: 0.9200\n",
      "Epoch 55: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9156 - val_loss: 0.4486 - val_accuracy: 0.8708\n",
      "Epoch 56/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2203 - accuracy: 0.8860\n",
      "Epoch 56: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.8945 - val_loss: 0.4456 - val_accuracy: 0.8596\n",
      "Epoch 57/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1962 - accuracy: 0.9146\n",
      "Epoch 57: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9114 - val_loss: 0.4496 - val_accuracy: 0.8539\n",
      "Epoch 58/100\n",
      "38/72 [==============>...............] - ETA: 0s - loss: 0.2022 - accuracy: 0.9184\n",
      "Epoch 58: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2062 - accuracy: 0.9100 - val_loss: 0.4478 - val_accuracy: 0.8652\n",
      "Epoch 59/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2131 - accuracy: 0.9000\n",
      "Epoch 59: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9086 - val_loss: 0.4758 - val_accuracy: 0.8483\n",
      "Epoch 60/100\n",
      "62/72 [========================>.....] - ETA: 0s - loss: 0.2093 - accuracy: 0.9065\n",
      "Epoch 60: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9058 - val_loss: 0.4496 - val_accuracy: 0.8596\n",
      "Epoch 61/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2160 - accuracy: 0.9195\n",
      "Epoch 61: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9100 - val_loss: 0.4684 - val_accuracy: 0.8483\n",
      "Epoch 62/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2574 - accuracy: 0.8854\n",
      "Epoch 62: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.8987 - val_loss: 0.4587 - val_accuracy: 0.8539\n",
      "Epoch 63/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2084 - accuracy: 0.9048\n",
      "Epoch 63: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2056 - accuracy: 0.9072 - val_loss: 0.4560 - val_accuracy: 0.8539\n",
      "Epoch 64/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1887 - accuracy: 0.9071\n",
      "Epoch 64: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9044 - val_loss: 0.4275 - val_accuracy: 0.8652\n",
      "Epoch 65/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.1881 - accuracy: 0.9154\n",
      "Epoch 65: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.9128 - val_loss: 0.4607 - val_accuracy: 0.8427\n",
      "Epoch 66/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.1936 - accuracy: 0.9088\n",
      "Epoch 66: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9044 - val_loss: 0.4573 - val_accuracy: 0.8596\n",
      "Epoch 67/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2012 - accuracy: 0.9140\n",
      "Epoch 67: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9058 - val_loss: 0.4574 - val_accuracy: 0.8652\n",
      "Epoch 68/100\n",
      "39/72 [===============>..............] - ETA: 0s - loss: 0.1879 - accuracy: 0.9154\n",
      "Epoch 68: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9072 - val_loss: 0.4545 - val_accuracy: 0.8652\n",
      "Epoch 69/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2075 - accuracy: 0.9143\n",
      "Epoch 69: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9114 - val_loss: 0.4610 - val_accuracy: 0.8708\n",
      "Epoch 70/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2010 - accuracy: 0.9146\n",
      "Epoch 70: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9100 - val_loss: 0.4744 - val_accuracy: 0.8483\n",
      "Epoch 71/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1950 - accuracy: 0.9214\n",
      "Epoch 71: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9100 - val_loss: 0.4586 - val_accuracy: 0.8596\n",
      "Epoch 72/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1943 - accuracy: 0.9093\n",
      "Epoch 72: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9072 - val_loss: 0.4843 - val_accuracy: 0.8596\n",
      "Epoch 73/100\n",
      "54/72 [=====================>........] - ETA: 0s - loss: 0.1938 - accuracy: 0.9074\n",
      "Epoch 73: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9044 - val_loss: 0.4620 - val_accuracy: 0.8652\n",
      "Epoch 74/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2131 - accuracy: 0.9093\n",
      "Epoch 74: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9086 - val_loss: 0.4616 - val_accuracy: 0.8764\n",
      "Epoch 75/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2058 - accuracy: 0.9000\n",
      "Epoch 75: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9072 - val_loss: 0.4720 - val_accuracy: 0.8539\n",
      "Epoch 76/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1828 - accuracy: 0.9238\n",
      "Epoch 76: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9086 - val_loss: 0.4639 - val_accuracy: 0.8483\n",
      "Epoch 77/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1928 - accuracy: 0.9209\n",
      "Epoch 77: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9114 - val_loss: 0.4651 - val_accuracy: 0.8483\n",
      "Epoch 78/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2133 - accuracy: 0.9048\n",
      "Epoch 78: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2032 - accuracy: 0.9100 - val_loss: 0.4809 - val_accuracy: 0.8596\n",
      "Epoch 79/100\n",
      "65/72 [==========================>...] - ETA: 0s - loss: 0.2019 - accuracy: 0.9077\n",
      "Epoch 79: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9100 - val_loss: 0.4583 - val_accuracy: 0.8596\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9100\n",
      "Epoch 80: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9100 - val_loss: 0.4704 - val_accuracy: 0.8596\n",
      "Epoch 81/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.1968 - accuracy: 0.9119\n",
      "Epoch 81: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9030 - val_loss: 0.4632 - val_accuracy: 0.8596\n",
      "Epoch 82/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.1879 - accuracy: 0.9256\n",
      "Epoch 82: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9212 - val_loss: 0.4644 - val_accuracy: 0.8539\n",
      "Epoch 83/100\n",
      "43/72 [================>.............] - ETA: 0s - loss: 0.2038 - accuracy: 0.9070\n",
      "Epoch 83: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9072 - val_loss: 0.4893 - val_accuracy: 0.8539\n",
      "Epoch 84/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.9121\n",
      "Epoch 84: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9100 - val_loss: 0.4585 - val_accuracy: 0.8596\n",
      "Epoch 85/100\n",
      "58/72 [=======================>......] - ETA: 0s - loss: 0.2148 - accuracy: 0.9000\n",
      "Epoch 85: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9072 - val_loss: 0.4766 - val_accuracy: 0.8596\n",
      "Epoch 86/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.1924 - accuracy: 0.9250\n",
      "Epoch 86: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9184 - val_loss: 0.4806 - val_accuracy: 0.8652\n",
      "Epoch 87/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.1800 - accuracy: 0.9150\n",
      "Epoch 87: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2026 - accuracy: 0.9086 - val_loss: 0.4749 - val_accuracy: 0.8539\n",
      "Epoch 88/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2136 - accuracy: 0.8952\n",
      "Epoch 88: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9086 - val_loss: 0.4814 - val_accuracy: 0.8596\n",
      "Epoch 89/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1868 - accuracy: 0.9195\n",
      "Epoch 89: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9058 - val_loss: 0.4655 - val_accuracy: 0.8483\n",
      "Epoch 90/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9127\n",
      "Epoch 90: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9128 - val_loss: 0.4864 - val_accuracy: 0.8652\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9156\n",
      "Epoch 91: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9156 - val_loss: 0.4614 - val_accuracy: 0.8427\n",
      "Epoch 92/100\n",
      "40/72 [===============>..............] - ETA: 0s - loss: 0.1994 - accuracy: 0.9225\n",
      "Epoch 92: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1989 - accuracy: 0.9156 - val_loss: 0.4771 - val_accuracy: 0.8652\n",
      "Epoch 93/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2124 - accuracy: 0.9190\n",
      "Epoch 93: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9114 - val_loss: 0.4713 - val_accuracy: 0.8596\n",
      "Epoch 94/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2061 - accuracy: 0.9095\n",
      "Epoch 94: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2018 - accuracy: 0.9114 - val_loss: 0.4774 - val_accuracy: 0.8596\n",
      "Epoch 95/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2070 - accuracy: 0.9049\n",
      "Epoch 95: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9072 - val_loss: 0.4769 - val_accuracy: 0.8708\n",
      "Epoch 96/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2155 - accuracy: 0.8878\n",
      "Epoch 96: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9015 - val_loss: 0.5071 - val_accuracy: 0.8539\n",
      "Epoch 97/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.2036 - accuracy: 0.9244\n",
      "Epoch 97: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9072 - val_loss: 0.4998 - val_accuracy: 0.8483\n",
      "Epoch 98/100\n",
      "41/72 [================>.............] - ETA: 0s - loss: 0.1851 - accuracy: 0.9049\n",
      "Epoch 98: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9114 - val_loss: 0.4660 - val_accuracy: 0.8483\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9114\n",
      "Epoch 99: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9114 - val_loss: 0.4902 - val_accuracy: 0.8652\n",
      "Epoch 100/100\n",
      "42/72 [================>.............] - ETA: 0s - loss: 0.2080 - accuracy: 0.9048\n",
      "Epoch 100: val_accuracy did not improve from 0.88202\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.9072 - val_loss: 0.4870 - val_accuracy: 0.8596\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"prediction_titanic/best_model_v2.h5\"\n",
    "\n",
    "# Creation of a callback to save the best model\n",
    "callback = keras.callbacks.ModelCheckpoint(filepath=save_directory, save_best_only=True, verbose=1, monitor='val_accuracy')\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(  X_train, Y_train,\n",
    "                      batch_size      = 10,\n",
    "                      epochs          = 100,\n",
    "                      verbose         = 1,\n",
    "                      callbacks       = [callback],\n",
    "                      validation_data = (X_validation, Y_validation))\n",
    "\n",
    "# Doing so for the list of 300 training data\n",
    "save_directory_list = [\"prediction_titanic/best_model_batch_{}.h5\".format(i) for i in range(batch_size)]\n",
    "history_list = []\n",
    "for i in range(batch_size):\n",
    "    keras.backend.clear_session()\n",
    "    print('Data set number: ', i)\n",
    "    callback = keras.callbacks.ModelCheckpoint(filepath=save_directory_list[i], save_best_only=True, verbose=1, monitor='val_accuracy')\n",
    "    history = model.fit(  X_train_list[i], Y_train_list[i],\n",
    "                      batch_size      = 10,\n",
    "                      epochs          = 100,\n",
    "                      verbose         = 1,\n",
    "                      callbacks       = [callback],\n",
    "                      validation_data = (X_validation_list[i], Y_validation_list[i]))\n",
    "    history_list.append(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model is performing during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting the evolution of the loss and the accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history['val_loss']\n",
    "train_acc  = np.array(history.history['accuracy'])*100\n",
    "val_acc    = np.array(history.history['val_accuracy'])*100\n",
    "\n",
    "# Plotting the evolution of the loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.plot(train_loss, label='loss on the training data')\n",
    "ax1.plot(val_loss, label='loss on the validation data')\n",
    "ax1.set_title('Evolution of the loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss [binary cross entropy]')\n",
    "\n",
    "ax2.plot(train_acc, label='accuracy on the training data')\n",
    "ax2.plot(val_acc, label='accuracy on the validation data')\n",
    "ax2.set_title('Evolution of the accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy [%]')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How perfomant is the best model we can get ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3928 - accuracy: 0.8483\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8495\n",
      "Best model accuracy on the validation data:  84.8 %\n",
      "Best model accuracy on the training data:  85.0 %\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8876\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8565\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8483\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8734\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8371\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8819\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8708\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8762\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.8876\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8734\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8876\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8847\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8652\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8959\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8764\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9001\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8933\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.8959\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8820\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9072\n",
      "Best accuracy of the 300 models on the validation data:  89.3 %\n",
      "Id of the best model:  8\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "best_model = keras.models.load_model(save_directory)\n",
    "\n",
    "# Best model evaluation\n",
    "acc_val = round(best_model.evaluate(X_validation, Y_validation)[1]*100,1)\n",
    "acc_train = round(best_model.evaluate(X_train, Y_train)[1]*100, 1)\n",
    "print(\"Best model accuracy on the validation data: \", acc_val,'%')\n",
    "print(\"Best model accuracy on the training data: \", acc_train,'%')\n",
    "\n",
    "# Doing so for the list of 300 training data\n",
    "acc_val_list = []\n",
    "acc_train_list = []\n",
    "for i in range(batch_size):\n",
    "    best_model = keras.models.load_model(save_directory_list[i])\n",
    "    acc_val = round(best_model.evaluate(X_validation_list[i], Y_validation_list[i])[1]*100,1)\n",
    "    acc_train = round(best_model.evaluate(X_train_list[i], Y_train_list[i])[1]*100, 1)\n",
    "    acc_val_list.append(acc_val)\n",
    "    acc_train_list.append(acc_train)\n",
    "print(\"Best accuracy of the 300 models on the validation data: \", max(acc_val_list),'%')\n",
    "print(\"Id of the best model: \", acc_val_list.index(max(acc_val_list)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 921us/step\n",
      "14/14 [==============================] - 0s 999us/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction of the test data\n",
    "survival_prediction = best_model.predict(X_test.drop(columns=['PassengerId'])).flatten()\n",
    "prediction = pd.DataFrame({'PassengerId': X_test['PassengerId'], 'Survived': [round(x) for x in survival_prediction]})\n",
    "\n",
    "# Show somme of the first predictions\n",
    "prediction.head()\n",
    "\n",
    "# Saving the prediction\n",
    "prediction.to_csv('prediction_titanic/prediction_v2.csv', index=False)\n",
    "\n",
    "# Prediction for the best model of the batch\n",
    "best_model = keras.models.load_model(save_directory_list[acc_val_list.index(max(acc_val_list))])\n",
    "survival_prediction = best_model.predict(X_test.drop(columns=['PassengerId'])).flatten()\n",
    "prediction = pd.DataFrame({'PassengerId': X_test['PassengerId'], 'Survived': [round(x) for x in survival_prediction]})\n",
    "prediction.to_csv('prediction_titanic/prediction_batch.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
